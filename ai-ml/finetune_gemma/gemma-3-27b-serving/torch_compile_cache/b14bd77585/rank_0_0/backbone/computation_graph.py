from __future__ import annotations
import torch
class GraphModule(torch.nn.Module):
    def forward(self, s59: "Sym(s59)", L_inputs_embeds_: "bf16[s59, 5376]", L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]", s80: "Sym(s59)", L_positions_: "i64[s59]", L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_0_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_0_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_1_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_1_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_1_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_1_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_2_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_2_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_2_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_2_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_3_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_3_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_4_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_4_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_4_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_4_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_5_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_5_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]", L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_5_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_5_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_6_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_6_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_6_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_6_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_7_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_7_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_7_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_7_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_8_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_8_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_8_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_8_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_9_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_9_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_9_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_9_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_10_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_10_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_10_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_10_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_11_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_11_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_11_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_11_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_12_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_12_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_12_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_12_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_13_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_13_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_13_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_13_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_14_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_14_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_14_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_14_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_15_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_15_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_15_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_15_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_16_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_16_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_16_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_16_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_17_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_17_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_17_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_17_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_18_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_18_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_18_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_18_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_19_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_19_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_19_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_19_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_20_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_20_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_20_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_20_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_21_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_21_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_21_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_21_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_22_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_22_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_22_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_22_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_23_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_23_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_23_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_23_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_24_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_24_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_24_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_24_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_25_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_25_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_25_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_25_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_26_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_26_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_26_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_26_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_27_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_27_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_27_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_27_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_28_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_28_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_28_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_28_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_29_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_29_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_29_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_29_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_30_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_30_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_30_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_30_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_31_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_31_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_31_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_31_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_32_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_32_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_32_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_32_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_32_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_32_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_32_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_33_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_33_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_33_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_33_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_33_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_33_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_33_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_34_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_34_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_34_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_34_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_34_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_34_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_34_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_35_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_35_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_35_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_35_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_35_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_35_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_35_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_36_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_36_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_36_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_36_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_36_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_36_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_36_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_36_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_36_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_36_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_37_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_37_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_37_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_37_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_37_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_37_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_37_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_37_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_37_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_37_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_38_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_38_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_38_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_38_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_38_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_38_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_38_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_38_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_38_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_38_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_39_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_39_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_39_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_39_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_39_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_39_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_39_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_39_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_39_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_39_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_40_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_40_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_40_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_40_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_40_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_40_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_40_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_40_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_40_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_40_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_41_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_41_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_41_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_41_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_41_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_41_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_41_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_41_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_41_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_41_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_42_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_42_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_42_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_42_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_42_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_42_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_42_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_42_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_42_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_42_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_43_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_43_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_43_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_43_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_43_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_43_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_43_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_43_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_43_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_43_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_44_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_44_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_44_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_44_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_44_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_44_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_44_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_44_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_44_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_44_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_45_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_45_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_45_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_45_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_45_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_45_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_45_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_45_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_45_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_45_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_46_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_46_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_46_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_46_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_46_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_46_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_46_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_46_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_46_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_46_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_47_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_47_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_47_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_47_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_47_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_47_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_47_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_47_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_47_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_47_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_48_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_48_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_48_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_48_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_48_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_48_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_48_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_48_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_48_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_48_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_49_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_49_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_49_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_49_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_49_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_49_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_49_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_49_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_49_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_49_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_50_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_50_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_50_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_50_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_50_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_50_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_50_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_50_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_50_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_50_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_51_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_51_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_51_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_51_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_51_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_51_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_51_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_51_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_51_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_51_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_52_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_52_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_52_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_52_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_52_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_52_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_52_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_52_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_52_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_52_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_53_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_53_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_53_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_53_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_53_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_53_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_53_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_53_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_53_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_53_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_54_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_54_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_54_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_54_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_54_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_54_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_54_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_54_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_54_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_54_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_55_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_55_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_55_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_55_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_55_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_55_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_55_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_55_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_55_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_55_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_56_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_56_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_56_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_56_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_56_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_56_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_56_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_56_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_56_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_56_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_57_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_57_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_57_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_57_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_57_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_57_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_57_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_57_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_57_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_57_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_58_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_58_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_58_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_58_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_58_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_58_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_58_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_58_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_58_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_58_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_59_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_59_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_59_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_59_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_59_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_59_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_59_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_59_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_59_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_59_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_60_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_60_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_60_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_60_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_60_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_60_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_60_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_60_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_60_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_60_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_61_modules_input_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_61_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", L_self_modules_layers_modules_61_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_61_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", L_self_modules_layers_modules_61_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", L_self_modules_layers_modules_61_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_61_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_layers_modules_61_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", L_self_modules_layers_modules_61_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", L_self_modules_layers_modules_61_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", L_self_modules_norm_parameters_weight_: "bf16[5376]"):
        l_inputs_embeds_ = L_inputs_embeds_
        l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = L_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_
        l_positions_ = L_positions_
        l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_1_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_1_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_1_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_1_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_1_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_1_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_1_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_2_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_2_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_2_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_2_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_2_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_2_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_2_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_3_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_3_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_3_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_3_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_4_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_4_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_4_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_4_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_4_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_4_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_4_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_5_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_5_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_5_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = L_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_
        l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_5_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_5_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_5_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_5_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_6_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_6_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_6_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_6_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_6_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_6_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_6_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_7_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_7_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_7_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_7_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_7_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_7_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_7_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_8_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_8_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_8_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_8_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_8_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_8_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_8_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_9_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_9_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_9_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_9_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_9_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_9_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_9_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_10_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_10_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_10_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_10_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_10_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_10_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_10_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_11_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_11_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_11_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_11_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_11_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_11_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_11_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_12_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_12_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_12_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_12_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_12_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_12_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_12_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_13_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_13_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_13_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_13_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_13_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_13_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_13_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_14_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_14_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_14_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_14_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_14_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_14_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_14_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_15_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_15_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_15_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_15_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_15_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_15_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_15_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_16_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_16_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_16_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_16_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_16_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_16_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_16_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_17_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_17_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_17_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_17_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_17_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_17_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_17_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_18_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_18_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_18_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_18_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_18_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_18_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_18_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_19_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_19_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_19_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_19_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_19_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_19_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_19_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_20_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_20_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_20_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_20_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_20_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_20_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_20_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_21_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_21_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_21_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_21_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_21_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_21_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_21_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_22_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_22_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_22_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_22_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_22_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_22_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_22_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_23_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_23_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_23_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_23_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_23_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_23_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_23_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_24_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_24_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_24_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_24_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_24_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_24_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_24_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_25_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_25_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_25_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_25_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_25_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_25_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_25_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_26_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_26_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_26_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_26_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_26_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_26_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_26_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_27_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_27_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_27_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_27_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_27_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_27_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_27_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_28_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_28_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_28_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_28_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_28_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_28_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_28_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_29_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_29_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_29_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_29_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_29_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_29_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_29_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_30_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_30_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_30_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_30_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_30_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_30_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_30_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_31_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_31_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_31_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_31_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_31_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_31_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_31_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_32_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_32_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_32_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_32_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_32_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_32_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_32_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_32_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_32_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_32_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_32_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_32_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_32_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_32_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_33_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_33_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_33_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_33_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_33_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_33_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_33_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_33_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_33_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_33_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_33_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_33_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_33_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_33_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_34_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_34_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_34_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_34_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_34_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_34_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_34_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_34_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_34_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_34_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_34_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_34_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_34_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_34_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_35_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_35_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_35_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_35_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_35_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_35_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_35_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_35_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_35_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_35_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_35_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_35_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_35_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_35_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_36_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_36_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_36_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_36_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_36_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_36_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_36_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_36_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_36_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_36_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_36_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_36_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_36_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_36_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_36_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_36_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_36_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_36_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_36_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_36_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_37_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_37_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_37_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_37_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_37_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_37_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_37_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_37_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_37_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_37_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_37_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_37_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_37_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_37_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_37_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_37_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_37_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_37_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_37_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_37_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_38_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_38_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_38_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_38_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_38_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_38_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_38_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_38_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_38_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_38_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_38_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_38_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_38_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_38_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_38_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_38_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_38_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_38_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_38_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_38_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_39_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_39_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_39_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_39_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_39_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_39_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_39_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_39_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_39_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_39_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_39_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_39_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_39_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_39_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_39_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_39_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_39_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_39_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_39_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_39_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_40_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_40_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_40_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_40_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_40_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_40_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_40_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_40_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_40_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_40_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_40_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_40_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_40_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_40_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_40_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_40_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_40_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_40_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_40_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_40_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_41_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_41_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_41_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_41_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_41_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_41_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_41_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_41_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_41_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_41_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_41_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_41_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_41_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_41_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_41_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_41_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_41_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_41_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_41_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_41_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_42_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_42_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_42_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_42_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_42_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_42_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_42_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_42_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_42_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_42_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_42_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_42_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_42_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_42_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_42_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_42_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_42_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_42_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_42_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_42_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_43_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_43_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_43_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_43_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_43_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_43_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_43_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_43_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_43_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_43_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_43_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_43_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_43_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_43_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_43_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_43_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_43_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_43_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_43_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_43_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_44_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_44_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_44_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_44_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_44_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_44_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_44_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_44_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_44_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_44_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_44_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_44_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_44_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_44_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_44_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_44_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_44_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_44_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_44_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_44_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_45_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_45_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_45_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_45_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_45_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_45_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_45_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_45_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_45_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_45_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_45_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_45_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_45_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_45_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_45_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_45_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_45_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_45_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_45_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_45_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_46_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_46_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_46_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_46_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_46_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_46_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_46_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_46_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_46_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_46_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_46_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_46_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_46_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_46_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_46_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_46_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_46_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_46_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_46_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_46_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_47_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_47_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_47_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_47_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_47_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_47_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_47_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_47_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_47_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_47_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_47_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_47_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_47_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_47_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_47_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_47_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_47_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_47_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_47_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_47_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_48_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_48_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_48_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_48_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_48_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_48_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_48_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_48_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_48_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_48_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_48_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_48_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_48_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_48_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_48_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_48_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_48_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_48_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_48_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_48_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_49_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_49_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_49_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_49_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_49_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_49_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_49_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_49_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_49_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_49_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_49_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_49_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_49_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_49_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_49_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_49_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_49_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_49_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_49_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_49_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_50_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_50_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_50_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_50_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_50_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_50_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_50_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_50_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_50_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_50_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_50_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_50_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_50_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_50_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_50_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_50_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_50_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_50_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_50_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_50_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_51_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_51_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_51_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_51_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_51_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_51_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_51_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_51_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_51_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_51_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_51_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_51_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_51_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_51_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_51_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_51_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_51_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_51_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_51_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_51_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_52_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_52_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_52_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_52_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_52_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_52_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_52_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_52_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_52_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_52_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_52_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_52_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_52_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_52_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_52_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_52_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_52_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_52_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_52_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_52_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_53_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_53_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_53_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_53_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_53_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_53_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_53_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_53_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_53_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_53_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_53_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_53_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_53_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_53_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_53_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_53_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_53_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_53_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_53_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_53_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_54_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_54_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_54_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_54_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_54_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_54_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_54_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_54_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_54_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_54_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_54_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_54_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_54_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_54_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_54_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_54_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_54_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_54_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_54_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_54_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_55_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_55_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_55_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_55_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_55_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_55_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_55_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_55_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_55_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_55_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_55_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_55_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_55_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_55_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_55_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_55_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_55_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_55_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_55_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_55_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_56_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_56_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_56_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_56_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_56_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_56_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_56_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_56_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_56_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_56_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_56_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_56_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_56_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_56_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_56_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_56_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_56_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_56_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_56_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_56_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_57_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_57_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_57_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_57_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_57_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_57_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_57_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_57_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_57_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_57_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_57_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_57_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_57_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_57_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_57_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_57_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_57_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_57_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_57_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_57_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_58_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_58_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_58_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_58_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_58_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_58_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_58_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_58_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_58_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_58_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_58_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_58_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_58_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_58_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_58_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_58_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_58_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_58_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_58_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_58_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_59_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_59_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_59_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_59_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_59_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_59_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_59_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_59_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_59_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_59_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_59_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_59_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_59_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_59_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_59_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_59_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_59_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_59_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_59_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_59_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_60_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_60_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_60_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_60_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_60_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_60_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_60_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_60_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_60_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_60_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_60_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_60_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_60_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_60_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_60_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_60_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_60_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_60_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_60_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_60_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_61_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_61_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_61_modules_self_attn_modules_qkv_proj_parameters_weight_ = L_self_modules_layers_modules_61_modules_self_attn_modules_qkv_proj_parameters_weight_
        l_self_modules_layers_modules_61_modules_self_attn_modules_q_norm_parameters_weight_ = L_self_modules_layers_modules_61_modules_self_attn_modules_q_norm_parameters_weight_
        l_self_modules_layers_modules_61_modules_self_attn_modules_k_norm_parameters_weight_ = L_self_modules_layers_modules_61_modules_self_attn_modules_k_norm_parameters_weight_
        l_self_modules_layers_modules_61_modules_self_attn_modules_o_proj_parameters_weight_ = L_self_modules_layers_modules_61_modules_self_attn_modules_o_proj_parameters_weight_
        l_self_modules_layers_modules_61_modules_post_attention_layernorm_parameters_weight_ = L_self_modules_layers_modules_61_modules_post_attention_layernorm_parameters_weight_
        l_self_modules_layers_modules_61_modules_pre_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_61_modules_pre_feedforward_layernorm_parameters_weight_
        l_self_modules_layers_modules_61_modules_mlp_modules_gate_up_proj_parameters_weight_ = L_self_modules_layers_modules_61_modules_mlp_modules_gate_up_proj_parameters_weight_
        l_self_modules_layers_modules_61_modules_mlp_modules_down_proj_parameters_weight_ = L_self_modules_layers_modules_61_modules_mlp_modules_down_proj_parameters_weight_
        l_self_modules_layers_modules_61_modules_post_feedforward_layernorm_parameters_weight_ = L_self_modules_layers_modules_61_modules_post_feedforward_layernorm_parameters_weight_
        l_self_modules_norm_parameters_weight_ = L_self_modules_norm_parameters_weight_
        
        # No stacktrace found for following nodes
        submod_0 = self.submod_0(l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_, l_inputs_embeds_, s59, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem = submod_0[0]
        getitem_1 = submod_0[1]
        getitem_2 = submod_0[2]
        getitem_3 = submod_0[3];  submod_0 = None
        submod_1 = self.submod_1(getitem, s59, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None
        submod_2 = self.submod_2(getitem_3, s59, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_0_modules_pre_feedforward_layernorm_parameters_weight_, l_inputs_embeds_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_0_modules_pre_feedforward_layernorm_parameters_weight_ = l_inputs_embeds_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_4 = submod_2[0]
        getitem_5 = submod_2[1]
        getitem_6 = submod_2[2]
        getitem_7 = submod_2[3]
        getitem_8 = submod_2[4];  submod_2 = None
        submod_3 = self.submod_3(getitem_4, s59, getitem_5, getitem_6, getitem_7);  getitem_4 = getitem_5 = getitem_6 = submod_3 = None
        submod_4 = self.submod_4(getitem_7, s59, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_pre_feedforward_layernorm_parameters_weight_, getitem_8, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_7 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_8 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_9 = submod_4[0]
        getitem_10 = submod_4[1]
        getitem_11 = submod_4[2]
        getitem_12 = submod_4[3]
        getitem_13 = submod_4[4];  submod_4 = None
        submod_5 = self.submod_5(getitem_9, s59, getitem_10, getitem_11, getitem_12);  getitem_9 = getitem_10 = getitem_11 = submod_5 = None
        submod_6 = self.submod_6(getitem_12, s59, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_pre_feedforward_layernorm_parameters_weight_, getitem_13, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_12 = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_13 = l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_14 = submod_6[0]
        getitem_15 = submod_6[1]
        getitem_16 = submod_6[2]
        getitem_17 = submod_6[3]
        getitem_18 = submod_6[4];  submod_6 = None
        submod_7 = self.submod_7(getitem_14, s59, getitem_15, getitem_16, getitem_17);  getitem_14 = getitem_15 = getitem_16 = submod_7 = None
        submod_8 = self.submod_8(getitem_17, s59, l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_3_modules_pre_feedforward_layernorm_parameters_weight_, getitem_18, l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_3_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_4_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_17 = l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_3_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_18 = l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_3_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_4_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_19 = submod_8[0]
        getitem_20 = submod_8[1]
        getitem_21 = submod_8[2]
        getitem_22 = submod_8[3]
        getitem_23 = submod_8[4];  submod_8 = None
        submod_9 = self.submod_9(getitem_19, s59, getitem_20, getitem_21, getitem_22);  getitem_19 = getitem_20 = getitem_21 = submod_9 = None
        submod_10 = self.submod_10(getitem_22, s59, l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_4_modules_pre_feedforward_layernorm_parameters_weight_, getitem_23, l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_4_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_5_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_22 = l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_4_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_23 = l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_4_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_5_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_24 = submod_10[0]
        getitem_25 = submod_10[1]
        getitem_26 = submod_10[2]
        getitem_27 = submod_10[3]
        getitem_28 = submod_10[4];  submod_10 = None
        submod_11 = self.submod_11(getitem_24, s59, getitem_25, getitem_26, getitem_27);  getitem_24 = getitem_25 = getitem_26 = submod_11 = None
        submod_12 = self.submod_12(getitem_27, s59, l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_5_modules_pre_feedforward_layernorm_parameters_weight_, getitem_28, l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_5_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_6_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_27 = l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_5_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_28 = l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_5_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_6_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_29 = submod_12[0]
        getitem_30 = submod_12[1]
        getitem_31 = submod_12[2]
        getitem_32 = submod_12[3]
        getitem_33 = submod_12[4];  submod_12 = None
        submod_13 = self.submod_13(getitem_29, s59, getitem_30, getitem_31, getitem_32);  getitem_29 = getitem_30 = getitem_31 = submod_13 = None
        submod_14 = self.submod_14(getitem_32, s59, l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_6_modules_pre_feedforward_layernorm_parameters_weight_, getitem_33, l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_6_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_7_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_32 = l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_6_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_33 = l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_6_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_7_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_34 = submod_14[0]
        getitem_35 = submod_14[1]
        getitem_36 = submod_14[2]
        getitem_37 = submod_14[3]
        getitem_38 = submod_14[4];  submod_14 = None
        submod_15 = self.submod_15(getitem_34, s59, getitem_35, getitem_36, getitem_37);  getitem_34 = getitem_35 = getitem_36 = submod_15 = None
        submod_16 = self.submod_16(getitem_37, s59, l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_7_modules_pre_feedforward_layernorm_parameters_weight_, getitem_38, l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_7_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_8_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_37 = l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_7_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_38 = l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_7_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_8_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_39 = submod_16[0]
        getitem_40 = submod_16[1]
        getitem_41 = submod_16[2]
        getitem_42 = submod_16[3]
        getitem_43 = submod_16[4];  submod_16 = None
        submod_17 = self.submod_17(getitem_39, s59, getitem_40, getitem_41, getitem_42);  getitem_39 = getitem_40 = getitem_41 = submod_17 = None
        submod_18 = self.submod_18(getitem_42, s59, l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_8_modules_pre_feedforward_layernorm_parameters_weight_, getitem_43, l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_8_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_9_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_42 = l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_8_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_43 = l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_8_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_9_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_44 = submod_18[0]
        getitem_45 = submod_18[1]
        getitem_46 = submod_18[2]
        getitem_47 = submod_18[3]
        getitem_48 = submod_18[4];  submod_18 = None
        submod_19 = self.submod_19(getitem_44, s59, getitem_45, getitem_46, getitem_47);  getitem_44 = getitem_45 = getitem_46 = submod_19 = None
        submod_20 = self.submod_20(getitem_47, s59, l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_9_modules_pre_feedforward_layernorm_parameters_weight_, getitem_48, l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_9_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_10_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_47 = l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_9_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_48 = l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_9_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_10_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_49 = submod_20[0]
        getitem_50 = submod_20[1]
        getitem_51 = submod_20[2]
        getitem_52 = submod_20[3]
        getitem_53 = submod_20[4];  submod_20 = None
        submod_21 = self.submod_21(getitem_49, s59, getitem_50, getitem_51, getitem_52);  getitem_49 = getitem_50 = getitem_51 = submod_21 = None
        submod_22 = self.submod_22(getitem_52, s59, l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_10_modules_pre_feedforward_layernorm_parameters_weight_, getitem_53, l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_10_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_11_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_52 = l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_10_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_53 = l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_10_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_11_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_54 = submod_22[0]
        getitem_55 = submod_22[1]
        getitem_56 = submod_22[2]
        getitem_57 = submod_22[3]
        getitem_58 = submod_22[4];  submod_22 = None
        submod_23 = self.submod_23(getitem_54, s59, getitem_55, getitem_56, getitem_57);  getitem_54 = getitem_55 = getitem_56 = submod_23 = None
        submod_24 = self.submod_24(getitem_57, s59, l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_11_modules_pre_feedforward_layernorm_parameters_weight_, getitem_58, l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_11_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_12_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_57 = l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_11_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_58 = l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_11_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_12_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_59 = submod_24[0]
        getitem_60 = submod_24[1]
        getitem_61 = submod_24[2]
        getitem_62 = submod_24[3]
        getitem_63 = submod_24[4];  submod_24 = None
        submod_25 = self.submod_25(getitem_59, s59, getitem_60, getitem_61, getitem_62);  getitem_59 = getitem_60 = getitem_61 = submod_25 = None
        submod_26 = self.submod_26(getitem_62, s59, l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_12_modules_pre_feedforward_layernorm_parameters_weight_, getitem_63, l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_12_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_13_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_62 = l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_12_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_63 = l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_12_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_13_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_64 = submod_26[0]
        getitem_65 = submod_26[1]
        getitem_66 = submod_26[2]
        getitem_67 = submod_26[3]
        getitem_68 = submod_26[4];  submod_26 = None
        submod_27 = self.submod_27(getitem_64, s59, getitem_65, getitem_66, getitem_67);  getitem_64 = getitem_65 = getitem_66 = submod_27 = None
        submod_28 = self.submod_28(getitem_67, s59, l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_13_modules_pre_feedforward_layernorm_parameters_weight_, getitem_68, l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_13_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_14_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_67 = l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_13_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_68 = l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_13_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_14_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_69 = submod_28[0]
        getitem_70 = submod_28[1]
        getitem_71 = submod_28[2]
        getitem_72 = submod_28[3]
        getitem_73 = submod_28[4];  submod_28 = None
        submod_29 = self.submod_29(getitem_69, s59, getitem_70, getitem_71, getitem_72);  getitem_69 = getitem_70 = getitem_71 = submod_29 = None
        submod_30 = self.submod_30(getitem_72, s59, l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_14_modules_pre_feedforward_layernorm_parameters_weight_, getitem_73, l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_14_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_15_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_72 = l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_14_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_73 = l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_14_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_15_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_74 = submod_30[0]
        getitem_75 = submod_30[1]
        getitem_76 = submod_30[2]
        getitem_77 = submod_30[3]
        getitem_78 = submod_30[4];  submod_30 = None
        submod_31 = self.submod_31(getitem_74, s59, getitem_75, getitem_76, getitem_77);  getitem_74 = getitem_75 = getitem_76 = submod_31 = None
        submod_32 = self.submod_32(getitem_77, s59, l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_15_modules_pre_feedforward_layernorm_parameters_weight_, getitem_78, l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_15_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_16_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_77 = l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_15_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_78 = l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_15_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_16_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_79 = submod_32[0]
        getitem_80 = submod_32[1]
        getitem_81 = submod_32[2]
        getitem_82 = submod_32[3]
        getitem_83 = submod_32[4];  submod_32 = None
        submod_33 = self.submod_33(getitem_79, s59, getitem_80, getitem_81, getitem_82);  getitem_79 = getitem_80 = getitem_81 = submod_33 = None
        submod_34 = self.submod_34(getitem_82, s59, l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_16_modules_pre_feedforward_layernorm_parameters_weight_, getitem_83, l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_16_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_17_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_82 = l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_16_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_83 = l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_16_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_17_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_84 = submod_34[0]
        getitem_85 = submod_34[1]
        getitem_86 = submod_34[2]
        getitem_87 = submod_34[3]
        getitem_88 = submod_34[4];  submod_34 = None
        submod_35 = self.submod_35(getitem_84, s59, getitem_85, getitem_86, getitem_87);  getitem_84 = getitem_85 = getitem_86 = submod_35 = None
        submod_36 = self.submod_36(getitem_87, s59, l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_17_modules_pre_feedforward_layernorm_parameters_weight_, getitem_88, l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_17_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_18_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_87 = l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_17_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_88 = l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_17_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_18_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_89 = submod_36[0]
        getitem_90 = submod_36[1]
        getitem_91 = submod_36[2]
        getitem_92 = submod_36[3]
        getitem_93 = submod_36[4];  submod_36 = None
        submod_37 = self.submod_37(getitem_89, s59, getitem_90, getitem_91, getitem_92);  getitem_89 = getitem_90 = getitem_91 = submod_37 = None
        submod_38 = self.submod_38(getitem_92, s59, l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_18_modules_pre_feedforward_layernorm_parameters_weight_, getitem_93, l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_18_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_19_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_92 = l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_18_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_93 = l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_18_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_19_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_94 = submod_38[0]
        getitem_95 = submod_38[1]
        getitem_96 = submod_38[2]
        getitem_97 = submod_38[3]
        getitem_98 = submod_38[4];  submod_38 = None
        submod_39 = self.submod_39(getitem_94, s59, getitem_95, getitem_96, getitem_97);  getitem_94 = getitem_95 = getitem_96 = submod_39 = None
        submod_40 = self.submod_40(getitem_97, s59, l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_19_modules_pre_feedforward_layernorm_parameters_weight_, getitem_98, l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_19_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_20_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_97 = l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_19_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_98 = l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_19_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_20_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_99 = submod_40[0]
        getitem_100 = submod_40[1]
        getitem_101 = submod_40[2]
        getitem_102 = submod_40[3]
        getitem_103 = submod_40[4];  submod_40 = None
        submod_41 = self.submod_41(getitem_99, s59, getitem_100, getitem_101, getitem_102);  getitem_99 = getitem_100 = getitem_101 = submod_41 = None
        submod_42 = self.submod_42(getitem_102, s59, l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_20_modules_pre_feedforward_layernorm_parameters_weight_, getitem_103, l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_20_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_21_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_102 = l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_20_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_103 = l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_20_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_21_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_104 = submod_42[0]
        getitem_105 = submod_42[1]
        getitem_106 = submod_42[2]
        getitem_107 = submod_42[3]
        getitem_108 = submod_42[4];  submod_42 = None
        submod_43 = self.submod_43(getitem_104, s59, getitem_105, getitem_106, getitem_107);  getitem_104 = getitem_105 = getitem_106 = submod_43 = None
        submod_44 = self.submod_44(getitem_107, s59, l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_21_modules_pre_feedforward_layernorm_parameters_weight_, getitem_108, l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_21_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_22_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_107 = l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_21_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_108 = l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_21_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_22_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_109 = submod_44[0]
        getitem_110 = submod_44[1]
        getitem_111 = submod_44[2]
        getitem_112 = submod_44[3]
        getitem_113 = submod_44[4];  submod_44 = None
        submod_45 = self.submod_45(getitem_109, s59, getitem_110, getitem_111, getitem_112);  getitem_109 = getitem_110 = getitem_111 = submod_45 = None
        submod_46 = self.submod_46(getitem_112, s59, l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_22_modules_pre_feedforward_layernorm_parameters_weight_, getitem_113, l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_22_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_23_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_112 = l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_22_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_113 = l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_22_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_23_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_114 = submod_46[0]
        getitem_115 = submod_46[1]
        getitem_116 = submod_46[2]
        getitem_117 = submod_46[3]
        getitem_118 = submod_46[4];  submod_46 = None
        submod_47 = self.submod_47(getitem_114, s59, getitem_115, getitem_116, getitem_117);  getitem_114 = getitem_115 = getitem_116 = submod_47 = None
        submod_48 = self.submod_48(getitem_117, s59, l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_23_modules_pre_feedforward_layernorm_parameters_weight_, getitem_118, l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_23_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_24_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_117 = l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_23_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_118 = l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_23_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_24_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_119 = submod_48[0]
        getitem_120 = submod_48[1]
        getitem_121 = submod_48[2]
        getitem_122 = submod_48[3]
        getitem_123 = submod_48[4];  submod_48 = None
        submod_49 = self.submod_49(getitem_119, s59, getitem_120, getitem_121, getitem_122);  getitem_119 = getitem_120 = getitem_121 = submod_49 = None
        submod_50 = self.submod_50(getitem_122, s59, l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_24_modules_pre_feedforward_layernorm_parameters_weight_, getitem_123, l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_24_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_25_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_122 = l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_24_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_123 = l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_24_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_25_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_124 = submod_50[0]
        getitem_125 = submod_50[1]
        getitem_126 = submod_50[2]
        getitem_127 = submod_50[3]
        getitem_128 = submod_50[4];  submod_50 = None
        submod_51 = self.submod_51(getitem_124, s59, getitem_125, getitem_126, getitem_127);  getitem_124 = getitem_125 = getitem_126 = submod_51 = None
        submod_52 = self.submod_52(getitem_127, s59, l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_25_modules_pre_feedforward_layernorm_parameters_weight_, getitem_128, l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_25_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_26_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_127 = l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_25_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_128 = l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_25_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_26_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_129 = submod_52[0]
        getitem_130 = submod_52[1]
        getitem_131 = submod_52[2]
        getitem_132 = submod_52[3]
        getitem_133 = submod_52[4];  submod_52 = None
        submod_53 = self.submod_53(getitem_129, s59, getitem_130, getitem_131, getitem_132);  getitem_129 = getitem_130 = getitem_131 = submod_53 = None
        submod_54 = self.submod_54(getitem_132, s59, l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_26_modules_pre_feedforward_layernorm_parameters_weight_, getitem_133, l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_26_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_27_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_132 = l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_26_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_133 = l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_26_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_27_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_134 = submod_54[0]
        getitem_135 = submod_54[1]
        getitem_136 = submod_54[2]
        getitem_137 = submod_54[3]
        getitem_138 = submod_54[4];  submod_54 = None
        submod_55 = self.submod_55(getitem_134, s59, getitem_135, getitem_136, getitem_137);  getitem_134 = getitem_135 = getitem_136 = submod_55 = None
        submod_56 = self.submod_56(getitem_137, s59, l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_27_modules_pre_feedforward_layernorm_parameters_weight_, getitem_138, l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_27_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_28_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_137 = l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_27_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_138 = l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_27_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_28_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_139 = submod_56[0]
        getitem_140 = submod_56[1]
        getitem_141 = submod_56[2]
        getitem_142 = submod_56[3]
        getitem_143 = submod_56[4];  submod_56 = None
        submod_57 = self.submod_57(getitem_139, s59, getitem_140, getitem_141, getitem_142);  getitem_139 = getitem_140 = getitem_141 = submod_57 = None
        submod_58 = self.submod_58(getitem_142, s59, l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_28_modules_pre_feedforward_layernorm_parameters_weight_, getitem_143, l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_28_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_29_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_142 = l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_28_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_143 = l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_28_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_29_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_144 = submod_58[0]
        getitem_145 = submod_58[1]
        getitem_146 = submod_58[2]
        getitem_147 = submod_58[3]
        getitem_148 = submod_58[4];  submod_58 = None
        submod_59 = self.submod_59(getitem_144, s59, getitem_145, getitem_146, getitem_147);  getitem_144 = getitem_145 = getitem_146 = submod_59 = None
        submod_60 = self.submod_60(getitem_147, s59, l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_29_modules_pre_feedforward_layernorm_parameters_weight_, getitem_148, l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_29_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_30_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_147 = l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_29_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_148 = l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_29_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_30_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_149 = submod_60[0]
        getitem_150 = submod_60[1]
        getitem_151 = submod_60[2]
        getitem_152 = submod_60[3]
        getitem_153 = submod_60[4];  submod_60 = None
        submod_61 = self.submod_61(getitem_149, s59, getitem_150, getitem_151, getitem_152);  getitem_149 = getitem_150 = getitem_151 = submod_61 = None
        submod_62 = self.submod_62(getitem_152, s59, l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_30_modules_pre_feedforward_layernorm_parameters_weight_, getitem_153, l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_30_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_31_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_152 = l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_30_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_153 = l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_30_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_31_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_154 = submod_62[0]
        getitem_155 = submod_62[1]
        getitem_156 = submod_62[2]
        getitem_157 = submod_62[3]
        getitem_158 = submod_62[4];  submod_62 = None
        submod_63 = self.submod_63(getitem_154, s59, getitem_155, getitem_156, getitem_157);  getitem_154 = getitem_155 = getitem_156 = submod_63 = None
        submod_64 = self.submod_64(getitem_157, s59, l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_31_modules_pre_feedforward_layernorm_parameters_weight_, getitem_158, l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_31_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_32_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_32_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_32_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_157 = l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_31_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_158 = l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_31_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_32_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_32_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_32_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_159 = submod_64[0]
        getitem_160 = submod_64[1]
        getitem_161 = submod_64[2]
        getitem_162 = submod_64[3]
        getitem_163 = submod_64[4];  submod_64 = None
        submod_65 = self.submod_65(getitem_159, s59, getitem_160, getitem_161, getitem_162);  getitem_159 = getitem_160 = getitem_161 = submod_65 = None
        submod_66 = self.submod_66(getitem_162, s59, l_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_32_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_32_modules_pre_feedforward_layernorm_parameters_weight_, getitem_163, l_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_32_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_32_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_33_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_33_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_33_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_162 = l_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_32_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_32_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_163 = l_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_32_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_32_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_33_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_33_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_33_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_164 = submod_66[0]
        getitem_165 = submod_66[1]
        getitem_166 = submod_66[2]
        getitem_167 = submod_66[3]
        getitem_168 = submod_66[4];  submod_66 = None
        submod_67 = self.submod_67(getitem_164, s59, getitem_165, getitem_166, getitem_167);  getitem_164 = getitem_165 = getitem_166 = submod_67 = None
        submod_68 = self.submod_68(getitem_167, s59, l_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_33_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_33_modules_pre_feedforward_layernorm_parameters_weight_, getitem_168, l_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_33_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_33_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_34_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_34_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_34_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_167 = l_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_33_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_33_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_168 = l_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_33_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_33_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_34_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_34_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_34_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_169 = submod_68[0]
        getitem_170 = submod_68[1]
        getitem_171 = submod_68[2]
        getitem_172 = submod_68[3]
        getitem_173 = submod_68[4];  submod_68 = None
        submod_69 = self.submod_69(getitem_169, s59, getitem_170, getitem_171, getitem_172);  getitem_169 = getitem_170 = getitem_171 = submod_69 = None
        submod_70 = self.submod_70(getitem_172, s59, l_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_34_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_34_modules_pre_feedforward_layernorm_parameters_weight_, getitem_173, l_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_34_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_34_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_35_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_35_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_35_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_172 = l_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_34_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_34_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_173 = l_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_34_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_34_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_35_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_35_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_35_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_174 = submod_70[0]
        getitem_175 = submod_70[1]
        getitem_176 = submod_70[2]
        getitem_177 = submod_70[3]
        getitem_178 = submod_70[4];  submod_70 = None
        submod_71 = self.submod_71(getitem_174, s59, getitem_175, getitem_176, getitem_177);  getitem_174 = getitem_175 = getitem_176 = submod_71 = None
        submod_72 = self.submod_72(getitem_177, s59, l_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_35_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_35_modules_pre_feedforward_layernorm_parameters_weight_, getitem_178, l_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_35_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_35_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_36_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_36_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_36_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_36_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_177 = l_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_35_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_35_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_178 = l_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_35_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_35_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_36_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_36_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_36_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_36_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_179 = submod_72[0]
        getitem_180 = submod_72[1]
        getitem_181 = submod_72[2]
        getitem_182 = submod_72[3]
        getitem_183 = submod_72[4];  submod_72 = None
        submod_73 = self.submod_73(getitem_179, s59, getitem_180, getitem_181, getitem_182);  getitem_179 = getitem_180 = getitem_181 = submod_73 = None
        submod_74 = self.submod_74(getitem_182, s59, l_self_modules_layers_modules_36_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_36_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_36_modules_pre_feedforward_layernorm_parameters_weight_, getitem_183, l_self_modules_layers_modules_36_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_36_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_36_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_37_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_37_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_37_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_37_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_182 = l_self_modules_layers_modules_36_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_36_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_36_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_183 = l_self_modules_layers_modules_36_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_36_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_36_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_37_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_37_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_37_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_37_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_184 = submod_74[0]
        getitem_185 = submod_74[1]
        getitem_186 = submod_74[2]
        getitem_187 = submod_74[3]
        getitem_188 = submod_74[4];  submod_74 = None
        submod_75 = self.submod_75(getitem_184, s59, getitem_185, getitem_186, getitem_187);  getitem_184 = getitem_185 = getitem_186 = submod_75 = None
        submod_76 = self.submod_76(getitem_187, s59, l_self_modules_layers_modules_37_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_37_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_37_modules_pre_feedforward_layernorm_parameters_weight_, getitem_188, l_self_modules_layers_modules_37_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_37_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_37_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_38_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_38_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_38_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_38_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_187 = l_self_modules_layers_modules_37_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_37_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_37_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_188 = l_self_modules_layers_modules_37_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_37_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_37_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_38_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_38_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_38_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_38_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_189 = submod_76[0]
        getitem_190 = submod_76[1]
        getitem_191 = submod_76[2]
        getitem_192 = submod_76[3]
        getitem_193 = submod_76[4];  submod_76 = None
        submod_77 = self.submod_77(getitem_189, s59, getitem_190, getitem_191, getitem_192);  getitem_189 = getitem_190 = getitem_191 = submod_77 = None
        submod_78 = self.submod_78(getitem_192, s59, l_self_modules_layers_modules_38_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_38_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_38_modules_pre_feedforward_layernorm_parameters_weight_, getitem_193, l_self_modules_layers_modules_38_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_38_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_38_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_39_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_39_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_39_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_39_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_192 = l_self_modules_layers_modules_38_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_38_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_38_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_193 = l_self_modules_layers_modules_38_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_38_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_38_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_39_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_39_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_39_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_39_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_194 = submod_78[0]
        getitem_195 = submod_78[1]
        getitem_196 = submod_78[2]
        getitem_197 = submod_78[3]
        getitem_198 = submod_78[4];  submod_78 = None
        submod_79 = self.submod_79(getitem_194, s59, getitem_195, getitem_196, getitem_197);  getitem_194 = getitem_195 = getitem_196 = submod_79 = None
        submod_80 = self.submod_80(getitem_197, s59, l_self_modules_layers_modules_39_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_39_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_39_modules_pre_feedforward_layernorm_parameters_weight_, getitem_198, l_self_modules_layers_modules_39_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_39_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_39_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_40_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_40_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_40_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_40_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_197 = l_self_modules_layers_modules_39_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_39_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_39_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_198 = l_self_modules_layers_modules_39_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_39_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_39_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_40_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_40_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_40_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_40_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_199 = submod_80[0]
        getitem_200 = submod_80[1]
        getitem_201 = submod_80[2]
        getitem_202 = submod_80[3]
        getitem_203 = submod_80[4];  submod_80 = None
        submod_81 = self.submod_81(getitem_199, s59, getitem_200, getitem_201, getitem_202);  getitem_199 = getitem_200 = getitem_201 = submod_81 = None
        submod_82 = self.submod_82(getitem_202, s59, l_self_modules_layers_modules_40_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_40_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_40_modules_pre_feedforward_layernorm_parameters_weight_, getitem_203, l_self_modules_layers_modules_40_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_40_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_40_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_41_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_41_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_41_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_41_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_202 = l_self_modules_layers_modules_40_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_40_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_40_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_203 = l_self_modules_layers_modules_40_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_40_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_40_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_41_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_41_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_41_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_41_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_204 = submod_82[0]
        getitem_205 = submod_82[1]
        getitem_206 = submod_82[2]
        getitem_207 = submod_82[3]
        getitem_208 = submod_82[4];  submod_82 = None
        submod_83 = self.submod_83(getitem_204, s59, getitem_205, getitem_206, getitem_207);  getitem_204 = getitem_205 = getitem_206 = submod_83 = None
        submod_84 = self.submod_84(getitem_207, s59, l_self_modules_layers_modules_41_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_41_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_41_modules_pre_feedforward_layernorm_parameters_weight_, getitem_208, l_self_modules_layers_modules_41_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_41_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_41_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_42_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_42_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_42_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_42_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_207 = l_self_modules_layers_modules_41_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_41_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_41_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_208 = l_self_modules_layers_modules_41_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_41_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_41_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_42_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_42_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_42_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_42_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_209 = submod_84[0]
        getitem_210 = submod_84[1]
        getitem_211 = submod_84[2]
        getitem_212 = submod_84[3]
        getitem_213 = submod_84[4];  submod_84 = None
        submod_85 = self.submod_85(getitem_209, s59, getitem_210, getitem_211, getitem_212);  getitem_209 = getitem_210 = getitem_211 = submod_85 = None
        submod_86 = self.submod_86(getitem_212, s59, l_self_modules_layers_modules_42_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_42_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_42_modules_pre_feedforward_layernorm_parameters_weight_, getitem_213, l_self_modules_layers_modules_42_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_42_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_42_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_43_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_43_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_43_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_43_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_212 = l_self_modules_layers_modules_42_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_42_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_42_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_213 = l_self_modules_layers_modules_42_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_42_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_42_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_43_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_43_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_43_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_43_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_214 = submod_86[0]
        getitem_215 = submod_86[1]
        getitem_216 = submod_86[2]
        getitem_217 = submod_86[3]
        getitem_218 = submod_86[4];  submod_86 = None
        submod_87 = self.submod_87(getitem_214, s59, getitem_215, getitem_216, getitem_217);  getitem_214 = getitem_215 = getitem_216 = submod_87 = None
        submod_88 = self.submod_88(getitem_217, s59, l_self_modules_layers_modules_43_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_43_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_43_modules_pre_feedforward_layernorm_parameters_weight_, getitem_218, l_self_modules_layers_modules_43_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_43_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_43_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_44_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_44_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_44_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_44_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_217 = l_self_modules_layers_modules_43_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_43_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_43_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_218 = l_self_modules_layers_modules_43_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_43_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_43_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_44_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_44_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_44_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_44_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_219 = submod_88[0]
        getitem_220 = submod_88[1]
        getitem_221 = submod_88[2]
        getitem_222 = submod_88[3]
        getitem_223 = submod_88[4];  submod_88 = None
        submod_89 = self.submod_89(getitem_219, s59, getitem_220, getitem_221, getitem_222);  getitem_219 = getitem_220 = getitem_221 = submod_89 = None
        submod_90 = self.submod_90(getitem_222, s59, l_self_modules_layers_modules_44_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_44_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_44_modules_pre_feedforward_layernorm_parameters_weight_, getitem_223, l_self_modules_layers_modules_44_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_44_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_44_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_45_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_45_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_45_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_45_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_222 = l_self_modules_layers_modules_44_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_44_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_44_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_223 = l_self_modules_layers_modules_44_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_44_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_44_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_45_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_45_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_45_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_45_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_224 = submod_90[0]
        getitem_225 = submod_90[1]
        getitem_226 = submod_90[2]
        getitem_227 = submod_90[3]
        getitem_228 = submod_90[4];  submod_90 = None
        submod_91 = self.submod_91(getitem_224, s59, getitem_225, getitem_226, getitem_227);  getitem_224 = getitem_225 = getitem_226 = submod_91 = None
        submod_92 = self.submod_92(getitem_227, s59, l_self_modules_layers_modules_45_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_45_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_45_modules_pre_feedforward_layernorm_parameters_weight_, getitem_228, l_self_modules_layers_modules_45_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_45_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_45_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_46_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_46_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_46_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_46_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_227 = l_self_modules_layers_modules_45_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_45_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_45_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_228 = l_self_modules_layers_modules_45_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_45_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_45_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_46_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_46_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_46_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_46_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_229 = submod_92[0]
        getitem_230 = submod_92[1]
        getitem_231 = submod_92[2]
        getitem_232 = submod_92[3]
        getitem_233 = submod_92[4];  submod_92 = None
        submod_93 = self.submod_93(getitem_229, s59, getitem_230, getitem_231, getitem_232);  getitem_229 = getitem_230 = getitem_231 = submod_93 = None
        submod_94 = self.submod_94(getitem_232, s59, l_self_modules_layers_modules_46_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_46_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_46_modules_pre_feedforward_layernorm_parameters_weight_, getitem_233, l_self_modules_layers_modules_46_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_46_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_46_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_47_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_47_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_47_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_47_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_232 = l_self_modules_layers_modules_46_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_46_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_46_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_233 = l_self_modules_layers_modules_46_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_46_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_46_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_47_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_47_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_47_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_47_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_234 = submod_94[0]
        getitem_235 = submod_94[1]
        getitem_236 = submod_94[2]
        getitem_237 = submod_94[3]
        getitem_238 = submod_94[4];  submod_94 = None
        submod_95 = self.submod_95(getitem_234, s59, getitem_235, getitem_236, getitem_237);  getitem_234 = getitem_235 = getitem_236 = submod_95 = None
        submod_96 = self.submod_96(getitem_237, s59, l_self_modules_layers_modules_47_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_47_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_47_modules_pre_feedforward_layernorm_parameters_weight_, getitem_238, l_self_modules_layers_modules_47_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_47_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_47_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_48_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_48_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_48_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_48_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_237 = l_self_modules_layers_modules_47_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_47_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_47_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_238 = l_self_modules_layers_modules_47_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_47_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_47_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_48_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_48_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_48_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_48_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_239 = submod_96[0]
        getitem_240 = submod_96[1]
        getitem_241 = submod_96[2]
        getitem_242 = submod_96[3]
        getitem_243 = submod_96[4];  submod_96 = None
        submod_97 = self.submod_97(getitem_239, s59, getitem_240, getitem_241, getitem_242);  getitem_239 = getitem_240 = getitem_241 = submod_97 = None
        submod_98 = self.submod_98(getitem_242, s59, l_self_modules_layers_modules_48_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_48_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_48_modules_pre_feedforward_layernorm_parameters_weight_, getitem_243, l_self_modules_layers_modules_48_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_48_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_48_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_49_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_49_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_49_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_49_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_242 = l_self_modules_layers_modules_48_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_48_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_48_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_243 = l_self_modules_layers_modules_48_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_48_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_48_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_49_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_49_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_49_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_49_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_244 = submod_98[0]
        getitem_245 = submod_98[1]
        getitem_246 = submod_98[2]
        getitem_247 = submod_98[3]
        getitem_248 = submod_98[4];  submod_98 = None
        submod_99 = self.submod_99(getitem_244, s59, getitem_245, getitem_246, getitem_247);  getitem_244 = getitem_245 = getitem_246 = submod_99 = None
        submod_100 = self.submod_100(getitem_247, s59, l_self_modules_layers_modules_49_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_49_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_49_modules_pre_feedforward_layernorm_parameters_weight_, getitem_248, l_self_modules_layers_modules_49_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_49_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_49_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_50_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_50_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_50_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_50_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_247 = l_self_modules_layers_modules_49_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_49_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_49_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_248 = l_self_modules_layers_modules_49_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_49_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_49_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_50_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_50_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_50_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_50_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_249 = submod_100[0]
        getitem_250 = submod_100[1]
        getitem_251 = submod_100[2]
        getitem_252 = submod_100[3]
        getitem_253 = submod_100[4];  submod_100 = None
        submod_101 = self.submod_101(getitem_249, s59, getitem_250, getitem_251, getitem_252);  getitem_249 = getitem_250 = getitem_251 = submod_101 = None
        submod_102 = self.submod_102(getitem_252, s59, l_self_modules_layers_modules_50_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_50_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_50_modules_pre_feedforward_layernorm_parameters_weight_, getitem_253, l_self_modules_layers_modules_50_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_50_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_50_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_51_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_51_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_51_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_51_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_252 = l_self_modules_layers_modules_50_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_50_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_50_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_253 = l_self_modules_layers_modules_50_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_50_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_50_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_51_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_51_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_51_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_51_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_254 = submod_102[0]
        getitem_255 = submod_102[1]
        getitem_256 = submod_102[2]
        getitem_257 = submod_102[3]
        getitem_258 = submod_102[4];  submod_102 = None
        submod_103 = self.submod_103(getitem_254, s59, getitem_255, getitem_256, getitem_257);  getitem_254 = getitem_255 = getitem_256 = submod_103 = None
        submod_104 = self.submod_104(getitem_257, s59, l_self_modules_layers_modules_51_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_51_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_51_modules_pre_feedforward_layernorm_parameters_weight_, getitem_258, l_self_modules_layers_modules_51_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_51_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_51_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_52_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_52_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_52_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_52_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_257 = l_self_modules_layers_modules_51_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_51_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_51_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_258 = l_self_modules_layers_modules_51_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_51_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_51_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_52_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_52_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_52_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_52_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_259 = submod_104[0]
        getitem_260 = submod_104[1]
        getitem_261 = submod_104[2]
        getitem_262 = submod_104[3]
        getitem_263 = submod_104[4];  submod_104 = None
        submod_105 = self.submod_105(getitem_259, s59, getitem_260, getitem_261, getitem_262);  getitem_259 = getitem_260 = getitem_261 = submod_105 = None
        submod_106 = self.submod_106(getitem_262, s59, l_self_modules_layers_modules_52_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_52_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_52_modules_pre_feedforward_layernorm_parameters_weight_, getitem_263, l_self_modules_layers_modules_52_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_52_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_52_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_53_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_53_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_53_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_53_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_262 = l_self_modules_layers_modules_52_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_52_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_52_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_263 = l_self_modules_layers_modules_52_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_52_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_52_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_53_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_53_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_53_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_53_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_264 = submod_106[0]
        getitem_265 = submod_106[1]
        getitem_266 = submod_106[2]
        getitem_267 = submod_106[3]
        getitem_268 = submod_106[4];  submod_106 = None
        submod_107 = self.submod_107(getitem_264, s59, getitem_265, getitem_266, getitem_267);  getitem_264 = getitem_265 = getitem_266 = submod_107 = None
        submod_108 = self.submod_108(getitem_267, s59, l_self_modules_layers_modules_53_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_53_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_53_modules_pre_feedforward_layernorm_parameters_weight_, getitem_268, l_self_modules_layers_modules_53_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_53_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_53_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_54_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_54_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_54_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_54_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_267 = l_self_modules_layers_modules_53_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_53_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_53_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_268 = l_self_modules_layers_modules_53_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_53_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_53_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_54_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_54_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_54_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_54_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_269 = submod_108[0]
        getitem_270 = submod_108[1]
        getitem_271 = submod_108[2]
        getitem_272 = submod_108[3]
        getitem_273 = submod_108[4];  submod_108 = None
        submod_109 = self.submod_109(getitem_269, s59, getitem_270, getitem_271, getitem_272);  getitem_269 = getitem_270 = getitem_271 = submod_109 = None
        submod_110 = self.submod_110(getitem_272, s59, l_self_modules_layers_modules_54_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_54_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_54_modules_pre_feedforward_layernorm_parameters_weight_, getitem_273, l_self_modules_layers_modules_54_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_54_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_54_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_55_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_55_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_55_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_55_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_272 = l_self_modules_layers_modules_54_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_54_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_54_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_273 = l_self_modules_layers_modules_54_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_54_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_54_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_55_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_55_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_55_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_55_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_274 = submod_110[0]
        getitem_275 = submod_110[1]
        getitem_276 = submod_110[2]
        getitem_277 = submod_110[3]
        getitem_278 = submod_110[4];  submod_110 = None
        submod_111 = self.submod_111(getitem_274, s59, getitem_275, getitem_276, getitem_277);  getitem_274 = getitem_275 = getitem_276 = submod_111 = None
        submod_112 = self.submod_112(getitem_277, s59, l_self_modules_layers_modules_55_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_55_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_55_modules_pre_feedforward_layernorm_parameters_weight_, getitem_278, l_self_modules_layers_modules_55_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_55_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_55_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_56_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_56_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_56_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_56_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_277 = l_self_modules_layers_modules_55_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_55_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_55_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_278 = l_self_modules_layers_modules_55_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_55_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_55_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_56_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_56_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_56_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_56_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_279 = submod_112[0]
        getitem_280 = submod_112[1]
        getitem_281 = submod_112[2]
        getitem_282 = submod_112[3]
        getitem_283 = submod_112[4];  submod_112 = None
        submod_113 = self.submod_113(getitem_279, s59, getitem_280, getitem_281, getitem_282);  getitem_279 = getitem_280 = getitem_281 = submod_113 = None
        submod_114 = self.submod_114(getitem_282, s59, l_self_modules_layers_modules_56_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_56_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_56_modules_pre_feedforward_layernorm_parameters_weight_, getitem_283, l_self_modules_layers_modules_56_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_56_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_56_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_57_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_57_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_57_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_57_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_282 = l_self_modules_layers_modules_56_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_56_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_56_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_283 = l_self_modules_layers_modules_56_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_56_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_56_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_57_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_57_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_57_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_57_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_284 = submod_114[0]
        getitem_285 = submod_114[1]
        getitem_286 = submod_114[2]
        getitem_287 = submod_114[3]
        getitem_288 = submod_114[4];  submod_114 = None
        submod_115 = self.submod_115(getitem_284, s59, getitem_285, getitem_286, getitem_287);  getitem_284 = getitem_285 = getitem_286 = submod_115 = None
        submod_116 = self.submod_116(getitem_287, s59, l_self_modules_layers_modules_57_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_57_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_57_modules_pre_feedforward_layernorm_parameters_weight_, getitem_288, l_self_modules_layers_modules_57_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_57_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_57_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_58_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_58_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_58_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_58_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_287 = l_self_modules_layers_modules_57_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_57_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_57_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_288 = l_self_modules_layers_modules_57_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_57_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_57_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_58_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_58_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_58_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_58_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_289 = submod_116[0]
        getitem_290 = submod_116[1]
        getitem_291 = submod_116[2]
        getitem_292 = submod_116[3]
        getitem_293 = submod_116[4];  submod_116 = None
        submod_117 = self.submod_117(getitem_289, s59, getitem_290, getitem_291, getitem_292);  getitem_289 = getitem_290 = getitem_291 = submod_117 = None
        submod_118 = self.submod_118(getitem_292, s59, l_self_modules_layers_modules_58_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_58_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_58_modules_pre_feedforward_layernorm_parameters_weight_, getitem_293, l_self_modules_layers_modules_58_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_58_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_58_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_59_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_59_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_59_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_59_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_292 = l_self_modules_layers_modules_58_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_58_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_58_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_293 = l_self_modules_layers_modules_58_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_58_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_58_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_59_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_59_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_59_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_59_modules_self_attn_modules_k_norm_parameters_weight_ = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = None
        getitem_294 = submod_118[0]
        getitem_295 = submod_118[1]
        getitem_296 = submod_118[2]
        getitem_297 = submod_118[3]
        getitem_298 = submod_118[4];  submod_118 = None
        submod_119 = self.submod_119(getitem_294, s59, getitem_295, getitem_296, getitem_297);  getitem_294 = getitem_295 = getitem_296 = submod_119 = None
        submod_120 = self.submod_120(getitem_297, s59, l_self_modules_layers_modules_59_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_59_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_59_modules_pre_feedforward_layernorm_parameters_weight_, getitem_298, l_self_modules_layers_modules_59_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_59_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_59_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_60_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_60_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_60_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_60_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_297 = l_self_modules_layers_modules_59_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_59_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_59_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_298 = l_self_modules_layers_modules_59_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_59_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_59_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_60_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_60_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_60_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_60_modules_self_attn_modules_k_norm_parameters_weight_ = None
        getitem_299 = submod_120[0]
        getitem_300 = submod_120[1]
        getitem_301 = submod_120[2]
        getitem_302 = submod_120[3]
        getitem_303 = submod_120[4];  submod_120 = None
        submod_121 = self.submod_121(getitem_299, s59, getitem_300, getitem_301, getitem_302);  getitem_299 = getitem_300 = getitem_301 = submod_121 = None
        submod_122 = self.submod_122(getitem_302, s59, l_self_modules_layers_modules_60_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_60_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_60_modules_pre_feedforward_layernorm_parameters_weight_, getitem_303, l_self_modules_layers_modules_60_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_60_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_60_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_layers_modules_61_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_61_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_61_modules_self_attn_modules_q_norm_parameters_weight_, l_self_modules_layers_modules_61_modules_self_attn_modules_k_norm_parameters_weight_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_302 = l_self_modules_layers_modules_60_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_60_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_60_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_303 = l_self_modules_layers_modules_60_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_60_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_60_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_layers_modules_61_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_61_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_61_modules_self_attn_modules_q_norm_parameters_weight_ = l_self_modules_layers_modules_61_modules_self_attn_modules_k_norm_parameters_weight_ = l_positions_ = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = None
        getitem_304 = submod_122[0]
        getitem_305 = submod_122[1]
        getitem_306 = submod_122[2]
        getitem_307 = submod_122[3]
        getitem_308 = submod_122[4];  submod_122 = None
        submod_123 = self.submod_123(getitem_304, s59, getitem_305, getitem_306, getitem_307);  getitem_304 = getitem_305 = getitem_306 = submod_123 = None
        submod_124 = self.submod_124(getitem_307, s59, l_self_modules_layers_modules_61_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_61_modules_post_attention_layernorm_parameters_weight_, l_self_modules_layers_modules_61_modules_pre_feedforward_layernorm_parameters_weight_, getitem_308, l_self_modules_layers_modules_61_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_61_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_61_modules_post_feedforward_layernorm_parameters_weight_, l_self_modules_norm_parameters_weight_);  getitem_307 = s59 = l_self_modules_layers_modules_61_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_61_modules_post_attention_layernorm_parameters_weight_ = l_self_modules_layers_modules_61_modules_pre_feedforward_layernorm_parameters_weight_ = getitem_308 = l_self_modules_layers_modules_61_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_61_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_61_modules_post_feedforward_layernorm_parameters_weight_ = l_self_modules_norm_parameters_weight_ = None
        return (submod_124,)
        
    class submod_0(torch.nn.Module):
        def forward(self, l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_inputs_embeds_: "bf16[s59, 5376]", s59: "Sym(s59)", l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = l_inputs_embeds_.float();  l_inputs_embeds_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 8192]" = torch._C._nn.linear(to, l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to = l_self_modules_layers_modules_0_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear.split([4096, 2048, 2048], dim = -1);  linear = None
            getitem: "bf16[s59, 4096]" = split[0]
            getitem_1: "bf16[s59, 2048]" = split[1]
            getitem_2: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem.unflatten(-1, (32, 128));  getitem = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_1: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_0_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_3: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 32, 128]" = float_3.pow(2)
            mean_1: "f32[s59, 32, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_2: "f32[s59, 32, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 32, 1]" = torch.rsqrt(add_2);  add_2 = None
            mul_2: "f32[s59, 32, 128]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[128]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_3: "f32[128]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 32, 128]" = mul_2 * add_3;  mul_2 = add_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 32, 128]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_1.flatten(-2, -1);  to_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_1.unflatten(-1, (16, 128));  getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_0_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 16, 128]" = float_5.pow(2)
            mean_2: "f32[s59, 16, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_4: "f32[s59, 16, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 16, 1]" = torch.rsqrt(add_4);  add_4 = None
            mul_4: "f32[s59, 16, 128]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[128]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_5: "f32[128]" = 1.0 + float_6;  float_6 = None
            mul_5: "f32[s59, 16, 128]" = mul_4 * add_5;  mul_4 = add_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 16, 128]" = mul_5.to(torch.bfloat16);  mul_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_2.flatten(-2, -1);  to_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_3: "bf16[s59, 64]" = chunk[0]
            getitem_4: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_5: "bf16[s59, 32, 128]" = view[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_6: "bf16[s59, 32, 0]" = view[(Ellipsis, slice(128, None, None))];  view = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_3.unsqueeze(-2)
            to_3: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_4.unsqueeze(-2)
            to_4: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_5, 2, dim = -1);  getitem_5 = None
            getitem_7: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_8: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_6: "bf16[s59, 32, 64]" = getitem_7 * to_3
            mul_7: "bf16[s59, 32, 64]" = getitem_8 * to_4
            sub: "bf16[s59, 32, 64]" = mul_6 - mul_7;  mul_6 = mul_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_8: "bf16[s59, 32, 64]" = getitem_8 * to_3;  getitem_8 = to_3 = None
            mul_9: "bf16[s59, 32, 64]" = getitem_7 * to_4;  getitem_7 = to_4 = None
            add_6: "bf16[s59, 32, 64]" = mul_8 + mul_9;  mul_8 = mul_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_6), dim = -1);  sub = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_6), dim = -1);  cat = getitem_6 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_9: "bf16[s59, 16, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_10: "bf16[s59, 16, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_3.unsqueeze(-2);  getitem_3 = None
            to_5: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_4.unsqueeze(-2);  getitem_4 = None
            to_6: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_9, 2, dim = -1);  getitem_9 = None
            getitem_11: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_12: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_10: "bf16[s59, 16, 64]" = getitem_11 * to_5
            mul_11: "bf16[s59, 16, 64]" = getitem_12 * to_6
            sub_1: "bf16[s59, 16, 64]" = mul_10 - mul_11;  mul_10 = mul_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_12: "bf16[s59, 16, 64]" = getitem_12 * to_5;  getitem_12 = to_5 = None
            mul_13: "bf16[s59, 16, 64]" = getitem_11 * to_6;  getitem_11 = to_6 = None
            add_7: "bf16[s59, 16, 64]" = mul_12 + mul_13;  mul_12 = mul_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_7), dim = -1);  sub_1 = add_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_10), dim = -1);  cat_2 = getitem_10 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_2: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_3: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_4: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_5: "bf16[s59, 16, 128]" = getitem_2.view(-1, 16, 128);  getitem_2 = None
            return (view_2, view_4, view_5, view_3)
            
    class submod_1(torch.nn.Module):
        def forward(self, query_2: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_2: "bf16[s59, 16, 128]", value: "bf16[s59, 16, 128]", output_3: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_3, 'language_model.model.layers.0.self_attn.attn', kv_cache_dummy_dep = None);  query_2 = key_2 = value = output_3 = unified_attention_with_output = None
            return ()
            
    class submod_2(torch.nn.Module):
        def forward(self, output_3: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_0_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_inputs_embeds_: "bf16[s59, 5376]", l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_0_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_1_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_1_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_3.view(-1, 4096);  output_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_0_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_0_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + l_inputs_embeds_;  to = l_inputs_embeds_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_0_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_0_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_1_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_1_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_1_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_1_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_3(torch.nn.Module):
        def forward(self, query_5: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_5: "bf16[s59, 16, 128]", value_1: "bf16[s59, 16, 128]", output_7: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_5, key_5, value_1, output_7, 'language_model.model.layers.1.self_attn.attn', kv_cache_dummy_dep = None);  query_5 = key_5 = value_1 = output_7 = unified_attention_with_output = None
            return ()
            
    class submod_4(torch.nn.Module):
        def forward(self, output_7: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_1_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_26: "bf16[s59, 5376]", l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_1_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_2_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_2_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_7.view(-1, 4096);  output_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_1_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_1_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_26;  to = x_26 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_1_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_1_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_2_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_2_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_2_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_2_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_5(torch.nn.Module):
        def forward(self, query_8: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_8: "bf16[s59, 16, 128]", value_2: "bf16[s59, 16, 128]", output_11: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_8, key_8, value_2, output_11, 'language_model.model.layers.2.self_attn.attn', kv_cache_dummy_dep = None);  query_8 = key_8 = value_2 = output_11 = unified_attention_with_output = None
            return ()
            
    class submod_6(torch.nn.Module):
        def forward(self, output_11: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_2_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_53: "bf16[s59, 5376]", l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_2_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_11.view(-1, 4096);  output_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_2_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_2_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_2_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_2_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_53;  to = x_53 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_2_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_2_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_2_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_2_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_3_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_3_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_3_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_3_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_7(torch.nn.Module):
        def forward(self, query_11: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_11: "bf16[s59, 16, 128]", value_3: "bf16[s59, 16, 128]", output_15: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_11, key_11, value_3, output_15, 'language_model.model.layers.3.self_attn.attn', kv_cache_dummy_dep = None);  query_11 = key_11 = value_3 = output_15 = unified_attention_with_output = None
            return ()
            
    class submod_8(torch.nn.Module):
        def forward(self, output_15: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_3_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_80: "bf16[s59, 5376]", l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_3_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_4_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_4_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_15.view(-1, 4096);  output_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_3_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_3_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_3_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_3_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_80;  to = x_80 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_3_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_3_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_3_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_3_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_4_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_4_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_4_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_4_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_4_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_4_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_9(torch.nn.Module):
        def forward(self, query_14: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_14: "bf16[s59, 16, 128]", value_4: "bf16[s59, 16, 128]", output_19: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_14, key_14, value_4, output_19, 'language_model.model.layers.4.self_attn.attn', kv_cache_dummy_dep = None);  query_14 = key_14 = value_4 = output_19 = unified_attention_with_output = None
            return ()
            
    class submod_10(torch.nn.Module):
        def forward(self, output_19: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_4_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_107: "bf16[s59, 5376]", l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_4_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_5_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_5_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_19.view(-1, 4096);  output_19 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_4_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_4_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_4_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_4_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_107;  to = x_107 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_4_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_4_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_4_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_4_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_5_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_5_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_5_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_5_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_5_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_5_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_11(torch.nn.Module):
        def forward(self, query_17: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_17: "bf16[s59, 16, 128]", value_5: "bf16[s59, 16, 128]", output_23: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_17, key_17, value_5, output_23, 'language_model.model.layers.5.self_attn.attn', kv_cache_dummy_dep = None);  query_17 = key_17 = value_5 = output_23 = unified_attention_with_output = None
            return ()
            
    class submod_12(torch.nn.Module):
        def forward(self, output_23: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_5_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_134: "bf16[s59, 5376]", l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_5_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_6_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_6_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_23.view(-1, 4096);  output_23 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_5_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_5_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_5_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_5_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_134;  to = x_134 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_5_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_5_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_5_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_5_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_6_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_6_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_6_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_6_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_6_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_6_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_13(torch.nn.Module):
        def forward(self, query_20: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_20: "bf16[s59, 16, 128]", value_6: "bf16[s59, 16, 128]", output_27: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_20, key_20, value_6, output_27, 'language_model.model.layers.6.self_attn.attn', kv_cache_dummy_dep = None);  query_20 = key_20 = value_6 = output_27 = unified_attention_with_output = None
            return ()
            
    class submod_14(torch.nn.Module):
        def forward(self, output_27: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_6_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_161: "bf16[s59, 5376]", l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_6_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_7_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_7_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_27.view(-1, 4096);  output_27 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_6_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_6_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_6_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_6_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_161;  to = x_161 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_6_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_6_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_6_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_6_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_7_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_7_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_7_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_7_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_7_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_7_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_15(torch.nn.Module):
        def forward(self, query_23: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_23: "bf16[s59, 16, 128]", value_7: "bf16[s59, 16, 128]", output_31: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_23, key_23, value_7, output_31, 'language_model.model.layers.7.self_attn.attn', kv_cache_dummy_dep = None);  query_23 = key_23 = value_7 = output_31 = unified_attention_with_output = None
            return ()
            
    class submod_16(torch.nn.Module):
        def forward(self, output_31: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_7_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_188: "bf16[s59, 5376]", l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_7_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_8_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_8_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_31.view(-1, 4096);  output_31 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_7_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_7_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_7_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_7_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_188;  to = x_188 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_7_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_7_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_7_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_7_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_8_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_8_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_8_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_8_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_8_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_8_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_17(torch.nn.Module):
        def forward(self, query_26: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_26: "bf16[s59, 16, 128]", value_8: "bf16[s59, 16, 128]", output_35: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_26, key_26, value_8, output_35, 'language_model.model.layers.8.self_attn.attn', kv_cache_dummy_dep = None);  query_26 = key_26 = value_8 = output_35 = unified_attention_with_output = None
            return ()
            
    class submod_18(torch.nn.Module):
        def forward(self, output_35: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_8_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_215: "bf16[s59, 5376]", l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_8_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_9_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_9_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_35.view(-1, 4096);  output_35 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_8_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_8_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_8_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_8_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_215;  to = x_215 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_8_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_8_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_8_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_8_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_9_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_9_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_9_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_9_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_9_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_9_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_19(torch.nn.Module):
        def forward(self, query_29: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_29: "bf16[s59, 16, 128]", value_9: "bf16[s59, 16, 128]", output_39: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_29, key_29, value_9, output_39, 'language_model.model.layers.9.self_attn.attn', kv_cache_dummy_dep = None);  query_29 = key_29 = value_9 = output_39 = unified_attention_with_output = None
            return ()
            
    class submod_20(torch.nn.Module):
        def forward(self, output_39: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_9_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_242: "bf16[s59, 5376]", l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_9_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_10_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_10_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_39.view(-1, 4096);  output_39 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_9_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_9_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_9_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_9_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_242;  to = x_242 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_9_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_9_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_9_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_9_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_10_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_10_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_10_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_10_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_10_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_10_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_21(torch.nn.Module):
        def forward(self, query_32: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_32: "bf16[s59, 16, 128]", value_10: "bf16[s59, 16, 128]", output_43: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_32, key_32, value_10, output_43, 'language_model.model.layers.10.self_attn.attn', kv_cache_dummy_dep = None);  query_32 = key_32 = value_10 = output_43 = unified_attention_with_output = None
            return ()
            
    class submod_22(torch.nn.Module):
        def forward(self, output_43: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_10_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_269: "bf16[s59, 5376]", l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_10_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_11_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_11_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_43.view(-1, 4096);  output_43 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_10_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_10_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_10_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_10_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_269;  to = x_269 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_10_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_10_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_10_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_10_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_11_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_11_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_11_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_11_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_11_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_11_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_23(torch.nn.Module):
        def forward(self, query_35: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_35: "bf16[s59, 16, 128]", value_11: "bf16[s59, 16, 128]", output_47: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_35, key_35, value_11, output_47, 'language_model.model.layers.11.self_attn.attn', kv_cache_dummy_dep = None);  query_35 = key_35 = value_11 = output_47 = unified_attention_with_output = None
            return ()
            
    class submod_24(torch.nn.Module):
        def forward(self, output_47: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_11_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_296: "bf16[s59, 5376]", l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_11_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_12_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_12_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_47.view(-1, 4096);  output_47 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_11_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_11_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_11_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_11_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_296;  to = x_296 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_11_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_11_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_11_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_11_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_12_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_12_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_12_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_12_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_12_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_12_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_25(torch.nn.Module):
        def forward(self, query_38: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_38: "bf16[s59, 16, 128]", value_12: "bf16[s59, 16, 128]", output_51: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_38, key_38, value_12, output_51, 'language_model.model.layers.12.self_attn.attn', kv_cache_dummy_dep = None);  query_38 = key_38 = value_12 = output_51 = unified_attention_with_output = None
            return ()
            
    class submod_26(torch.nn.Module):
        def forward(self, output_51: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_12_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_323: "bf16[s59, 5376]", l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_12_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_13_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_13_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_51.view(-1, 4096);  output_51 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_12_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_12_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_12_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_12_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_323;  to = x_323 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_12_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_12_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_12_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_12_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_13_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_13_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_13_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_13_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_13_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_13_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_27(torch.nn.Module):
        def forward(self, query_41: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_41: "bf16[s59, 16, 128]", value_13: "bf16[s59, 16, 128]", output_55: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_41, key_41, value_13, output_55, 'language_model.model.layers.13.self_attn.attn', kv_cache_dummy_dep = None);  query_41 = key_41 = value_13 = output_55 = unified_attention_with_output = None
            return ()
            
    class submod_28(torch.nn.Module):
        def forward(self, output_55: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_13_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_350: "bf16[s59, 5376]", l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_13_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_14_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_14_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_55.view(-1, 4096);  output_55 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_13_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_13_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_13_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_13_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_350;  to = x_350 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_13_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_13_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_13_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_13_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_14_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_14_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_14_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_14_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_14_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_14_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_29(torch.nn.Module):
        def forward(self, query_44: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_44: "bf16[s59, 16, 128]", value_14: "bf16[s59, 16, 128]", output_59: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_44, key_44, value_14, output_59, 'language_model.model.layers.14.self_attn.attn', kv_cache_dummy_dep = None);  query_44 = key_44 = value_14 = output_59 = unified_attention_with_output = None
            return ()
            
    class submod_30(torch.nn.Module):
        def forward(self, output_59: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_14_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_377: "bf16[s59, 5376]", l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_14_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_15_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_15_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_59.view(-1, 4096);  output_59 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_14_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_14_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_14_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_14_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_377;  to = x_377 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_14_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_14_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_14_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_14_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_15_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_15_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_15_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_15_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_15_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_15_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_31(torch.nn.Module):
        def forward(self, query_47: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_47: "bf16[s59, 16, 128]", value_15: "bf16[s59, 16, 128]", output_63: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_47, key_47, value_15, output_63, 'language_model.model.layers.15.self_attn.attn', kv_cache_dummy_dep = None);  query_47 = key_47 = value_15 = output_63 = unified_attention_with_output = None
            return ()
            
    class submod_32(torch.nn.Module):
        def forward(self, output_63: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_15_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_404: "bf16[s59, 5376]", l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_15_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_16_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_16_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_63.view(-1, 4096);  output_63 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_15_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_15_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_15_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_15_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_404;  to = x_404 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_15_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_15_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_15_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_15_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_16_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_16_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_16_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_16_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_16_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_16_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_33(torch.nn.Module):
        def forward(self, query_50: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_50: "bf16[s59, 16, 128]", value_16: "bf16[s59, 16, 128]", output_67: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_50, key_50, value_16, output_67, 'language_model.model.layers.16.self_attn.attn', kv_cache_dummy_dep = None);  query_50 = key_50 = value_16 = output_67 = unified_attention_with_output = None
            return ()
            
    class submod_34(torch.nn.Module):
        def forward(self, output_67: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_16_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_431: "bf16[s59, 5376]", l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_16_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_17_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_17_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_67.view(-1, 4096);  output_67 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_16_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_16_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_16_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_16_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_431;  to = x_431 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_16_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_16_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_16_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_16_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_17_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_17_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_17_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_17_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_17_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_17_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_35(torch.nn.Module):
        def forward(self, query_53: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_53: "bf16[s59, 16, 128]", value_17: "bf16[s59, 16, 128]", output_71: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_53, key_53, value_17, output_71, 'language_model.model.layers.17.self_attn.attn', kv_cache_dummy_dep = None);  query_53 = key_53 = value_17 = output_71 = unified_attention_with_output = None
            return ()
            
    class submod_36(torch.nn.Module):
        def forward(self, output_71: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_17_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_458: "bf16[s59, 5376]", l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_17_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_18_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_18_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_71.view(-1, 4096);  output_71 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_17_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_17_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_17_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_17_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_458;  to = x_458 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_17_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_17_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_17_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_17_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_18_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_18_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_18_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_18_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_18_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_18_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_37(torch.nn.Module):
        def forward(self, query_56: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_56: "bf16[s59, 16, 128]", value_18: "bf16[s59, 16, 128]", output_75: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_56, key_56, value_18, output_75, 'language_model.model.layers.18.self_attn.attn', kv_cache_dummy_dep = None);  query_56 = key_56 = value_18 = output_75 = unified_attention_with_output = None
            return ()
            
    class submod_38(torch.nn.Module):
        def forward(self, output_75: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_18_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_485: "bf16[s59, 5376]", l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_18_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_19_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_19_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_75.view(-1, 4096);  output_75 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_18_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_18_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_18_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_18_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_485;  to = x_485 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_18_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_18_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_18_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_18_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_19_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_19_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_19_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_19_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_19_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_19_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_39(torch.nn.Module):
        def forward(self, query_59: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_59: "bf16[s59, 16, 128]", value_19: "bf16[s59, 16, 128]", output_79: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_59, key_59, value_19, output_79, 'language_model.model.layers.19.self_attn.attn', kv_cache_dummy_dep = None);  query_59 = key_59 = value_19 = output_79 = unified_attention_with_output = None
            return ()
            
    class submod_40(torch.nn.Module):
        def forward(self, output_79: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_19_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_512: "bf16[s59, 5376]", l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_19_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_20_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_20_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_79.view(-1, 4096);  output_79 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_19_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_19_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_19_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_19_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_512;  to = x_512 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_19_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_19_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_19_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_19_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_20_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_20_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_20_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_20_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_20_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_20_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_41(torch.nn.Module):
        def forward(self, query_62: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_62: "bf16[s59, 16, 128]", value_20: "bf16[s59, 16, 128]", output_83: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_62, key_62, value_20, output_83, 'language_model.model.layers.20.self_attn.attn', kv_cache_dummy_dep = None);  query_62 = key_62 = value_20 = output_83 = unified_attention_with_output = None
            return ()
            
    class submod_42(torch.nn.Module):
        def forward(self, output_83: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_20_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_539: "bf16[s59, 5376]", l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_20_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_21_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_21_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_83.view(-1, 4096);  output_83 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_20_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_20_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_20_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_20_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_539;  to = x_539 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_20_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_20_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_20_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_20_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_21_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_21_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_21_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_21_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_21_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_21_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_43(torch.nn.Module):
        def forward(self, query_65: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_65: "bf16[s59, 16, 128]", value_21: "bf16[s59, 16, 128]", output_87: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_65, key_65, value_21, output_87, 'language_model.model.layers.21.self_attn.attn', kv_cache_dummy_dep = None);  query_65 = key_65 = value_21 = output_87 = unified_attention_with_output = None
            return ()
            
    class submod_44(torch.nn.Module):
        def forward(self, output_87: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_21_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_566: "bf16[s59, 5376]", l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_21_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_22_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_22_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_87.view(-1, 4096);  output_87 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_21_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_21_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_21_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_21_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_566;  to = x_566 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_21_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_21_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_21_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_21_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_22_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_22_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_22_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_22_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_22_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_22_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_45(torch.nn.Module):
        def forward(self, query_68: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_68: "bf16[s59, 16, 128]", value_22: "bf16[s59, 16, 128]", output_91: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_68, key_68, value_22, output_91, 'language_model.model.layers.22.self_attn.attn', kv_cache_dummy_dep = None);  query_68 = key_68 = value_22 = output_91 = unified_attention_with_output = None
            return ()
            
    class submod_46(torch.nn.Module):
        def forward(self, output_91: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_22_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_593: "bf16[s59, 5376]", l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_22_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_23_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_23_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_91.view(-1, 4096);  output_91 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_22_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_22_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_22_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_22_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_593;  to = x_593 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_22_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_22_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_22_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_22_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_23_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_23_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_23_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_23_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_23_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_23_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_47(torch.nn.Module):
        def forward(self, query_71: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_71: "bf16[s59, 16, 128]", value_23: "bf16[s59, 16, 128]", output_95: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_71, key_71, value_23, output_95, 'language_model.model.layers.23.self_attn.attn', kv_cache_dummy_dep = None);  query_71 = key_71 = value_23 = output_95 = unified_attention_with_output = None
            return ()
            
    class submod_48(torch.nn.Module):
        def forward(self, output_95: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_23_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_620: "bf16[s59, 5376]", l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_23_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_24_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_24_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_95.view(-1, 4096);  output_95 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_23_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_23_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_23_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_23_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_620;  to = x_620 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_23_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_23_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_23_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_23_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_24_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_24_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_24_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_24_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_24_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_24_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_49(torch.nn.Module):
        def forward(self, query_74: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_74: "bf16[s59, 16, 128]", value_24: "bf16[s59, 16, 128]", output_99: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_74, key_74, value_24, output_99, 'language_model.model.layers.24.self_attn.attn', kv_cache_dummy_dep = None);  query_74 = key_74 = value_24 = output_99 = unified_attention_with_output = None
            return ()
            
    class submod_50(torch.nn.Module):
        def forward(self, output_99: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_24_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_647: "bf16[s59, 5376]", l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_24_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_25_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_25_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_99.view(-1, 4096);  output_99 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_24_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_24_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_24_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_24_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_647;  to = x_647 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_24_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_24_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_24_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_24_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_25_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_25_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_25_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_25_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_25_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_25_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_51(torch.nn.Module):
        def forward(self, query_77: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_77: "bf16[s59, 16, 128]", value_25: "bf16[s59, 16, 128]", output_103: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_77, key_77, value_25, output_103, 'language_model.model.layers.25.self_attn.attn', kv_cache_dummy_dep = None);  query_77 = key_77 = value_25 = output_103 = unified_attention_with_output = None
            return ()
            
    class submod_52(torch.nn.Module):
        def forward(self, output_103: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_25_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_674: "bf16[s59, 5376]", l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_25_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_26_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_26_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_103.view(-1, 4096);  output_103 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_25_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_25_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_25_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_25_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_674;  to = x_674 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_25_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_25_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_25_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_25_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_26_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_26_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_26_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_26_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_26_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_26_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_53(torch.nn.Module):
        def forward(self, query_80: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_80: "bf16[s59, 16, 128]", value_26: "bf16[s59, 16, 128]", output_107: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_80, key_80, value_26, output_107, 'language_model.model.layers.26.self_attn.attn', kv_cache_dummy_dep = None);  query_80 = key_80 = value_26 = output_107 = unified_attention_with_output = None
            return ()
            
    class submod_54(torch.nn.Module):
        def forward(self, output_107: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_26_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_701: "bf16[s59, 5376]", l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_26_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_27_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_27_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_107.view(-1, 4096);  output_107 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_26_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_26_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_26_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_26_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_701;  to = x_701 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_26_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_26_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_26_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_26_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_27_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_27_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_27_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_27_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_27_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_27_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_55(torch.nn.Module):
        def forward(self, query_83: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_83: "bf16[s59, 16, 128]", value_27: "bf16[s59, 16, 128]", output_111: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_83, key_83, value_27, output_111, 'language_model.model.layers.27.self_attn.attn', kv_cache_dummy_dep = None);  query_83 = key_83 = value_27 = output_111 = unified_attention_with_output = None
            return ()
            
    class submod_56(torch.nn.Module):
        def forward(self, output_111: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_27_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_728: "bf16[s59, 5376]", l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_27_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_28_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_28_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_111.view(-1, 4096);  output_111 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_27_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_27_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_27_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_27_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_728;  to = x_728 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_27_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_27_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_27_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_27_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_28_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_28_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_28_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_28_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_28_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_28_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_57(torch.nn.Module):
        def forward(self, query_86: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_86: "bf16[s59, 16, 128]", value_28: "bf16[s59, 16, 128]", output_115: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_86, key_86, value_28, output_115, 'language_model.model.layers.28.self_attn.attn', kv_cache_dummy_dep = None);  query_86 = key_86 = value_28 = output_115 = unified_attention_with_output = None
            return ()
            
    class submod_58(torch.nn.Module):
        def forward(self, output_115: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_28_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_755: "bf16[s59, 5376]", l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_28_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_29_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_29_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_115.view(-1, 4096);  output_115 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_28_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_28_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_28_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_28_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_755;  to = x_755 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_28_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_28_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_28_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_28_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_29_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_29_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_29_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_29_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_29_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_29_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_59(torch.nn.Module):
        def forward(self, query_89: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_89: "bf16[s59, 16, 128]", value_29: "bf16[s59, 16, 128]", output_119: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_89, key_89, value_29, output_119, 'language_model.model.layers.29.self_attn.attn', kv_cache_dummy_dep = None);  query_89 = key_89 = value_29 = output_119 = unified_attention_with_output = None
            return ()
            
    class submod_60(torch.nn.Module):
        def forward(self, output_119: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_29_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_782: "bf16[s59, 5376]", l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_29_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_30_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_30_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_119.view(-1, 4096);  output_119 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_29_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_29_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_29_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_29_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_782;  to = x_782 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_29_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_29_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_29_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_29_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_30_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_30_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_30_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_30_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_30_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_30_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_61(torch.nn.Module):
        def forward(self, query_92: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_92: "bf16[s59, 16, 128]", value_30: "bf16[s59, 16, 128]", output_123: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_92, key_92, value_30, output_123, 'language_model.model.layers.30.self_attn.attn', kv_cache_dummy_dep = None);  query_92 = key_92 = value_30 = output_123 = unified_attention_with_output = None
            return ()
            
    class submod_62(torch.nn.Module):
        def forward(self, output_123: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_30_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_809: "bf16[s59, 5376]", l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_30_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_31_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_31_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_123.view(-1, 4096);  output_123 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_30_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_30_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_30_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_30_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_809;  to = x_809 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_30_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_30_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_30_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_30_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_31_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_31_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_31_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_31_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_31_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_31_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_63(torch.nn.Module):
        def forward(self, query_95: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_95: "bf16[s59, 16, 128]", value_31: "bf16[s59, 16, 128]", output_127: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_95, key_95, value_31, output_127, 'language_model.model.layers.31.self_attn.attn', kv_cache_dummy_dep = None);  query_95 = key_95 = value_31 = output_127 = unified_attention_with_output = None
            return ()
            
    class submod_64(torch.nn.Module):
        def forward(self, output_127: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_31_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_836: "bf16[s59, 5376]", l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_31_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_32_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_32_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_32_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_127.view(-1, 4096);  output_127 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_31_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_31_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_31_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_31_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_836;  to = x_836 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_31_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_31_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_31_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_31_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_32_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_32_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_32_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_32_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_32_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_32_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_32_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_65(torch.nn.Module):
        def forward(self, query_98: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_98: "bf16[s59, 16, 128]", value_32: "bf16[s59, 16, 128]", output_131: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_98, key_98, value_32, output_131, 'language_model.model.layers.32.self_attn.attn', kv_cache_dummy_dep = None);  query_98 = key_98 = value_32 = output_131 = unified_attention_with_output = None
            return ()
            
    class submod_66(torch.nn.Module):
        def forward(self, output_131: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_32_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_32_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_863: "bf16[s59, 5376]", l_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_32_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_32_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_33_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_33_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_33_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_131.view(-1, 4096);  output_131 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_32_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_32_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_32_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_32_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_32_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_863;  to = x_863 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_32_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_32_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_32_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_32_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_32_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_33_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_33_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_33_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_33_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_33_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_33_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_33_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_67(torch.nn.Module):
        def forward(self, query_101: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_101: "bf16[s59, 16, 128]", value_33: "bf16[s59, 16, 128]", output_135: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_101, key_101, value_33, output_135, 'language_model.model.layers.33.self_attn.attn', kv_cache_dummy_dep = None);  query_101 = key_101 = value_33 = output_135 = unified_attention_with_output = None
            return ()
            
    class submod_68(torch.nn.Module):
        def forward(self, output_135: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_33_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_33_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_890: "bf16[s59, 5376]", l_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_33_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_33_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_34_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_34_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_34_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_135.view(-1, 4096);  output_135 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_33_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_33_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_33_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_33_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_33_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_890;  to = x_890 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_33_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_33_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_33_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_33_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_33_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_34_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_34_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_34_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_34_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_34_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_34_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_34_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_69(torch.nn.Module):
        def forward(self, query_104: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_104: "bf16[s59, 16, 128]", value_34: "bf16[s59, 16, 128]", output_139: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_104, key_104, value_34, output_139, 'language_model.model.layers.34.self_attn.attn', kv_cache_dummy_dep = None);  query_104 = key_104 = value_34 = output_139 = unified_attention_with_output = None
            return ()
            
    class submod_70(torch.nn.Module):
        def forward(self, output_139: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_34_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_34_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_917: "bf16[s59, 5376]", l_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_34_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_34_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_35_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_35_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_35_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_139.view(-1, 4096);  output_139 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_34_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_34_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_34_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_34_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_34_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_917;  to = x_917 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_34_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_34_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_34_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_34_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_34_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_35_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_35_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_35_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_35_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_35_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_35_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_35_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_71(torch.nn.Module):
        def forward(self, query_107: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_107: "bf16[s59, 16, 128]", value_35: "bf16[s59, 16, 128]", output_143: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_107, key_107, value_35, output_143, 'language_model.model.layers.35.self_attn.attn', kv_cache_dummy_dep = None);  query_107 = key_107 = value_35 = output_143 = unified_attention_with_output = None
            return ()
            
    class submod_72(torch.nn.Module):
        def forward(self, output_143: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_35_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_35_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_944: "bf16[s59, 5376]", l_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_35_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_35_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_36_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_36_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_36_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_36_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_143.view(-1, 4096);  output_143 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_35_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_35_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_35_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_35_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_35_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_944;  to = x_944 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_35_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_35_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_35_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_35_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_35_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_36_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_36_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_36_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_36_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_36_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_36_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_36_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_36_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_73(torch.nn.Module):
        def forward(self, query_110: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_110: "bf16[s59, 16, 128]", value_36: "bf16[s59, 16, 128]", output_147: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_110, key_110, value_36, output_147, 'language_model.model.layers.36.self_attn.attn', kv_cache_dummy_dep = None);  query_110 = key_110 = value_36 = output_147 = unified_attention_with_output = None
            return ()
            
    class submod_74(torch.nn.Module):
        def forward(self, output_147: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_36_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_36_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_36_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_971: "bf16[s59, 5376]", l_self_modules_layers_modules_36_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_36_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_36_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_37_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_37_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_37_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_37_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_147.view(-1, 4096);  output_147 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_36_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_36_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_36_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_36_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_36_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_36_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_971;  to = x_971 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_36_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_36_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_36_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_36_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_36_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_36_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_37_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_37_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_37_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_37_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_37_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_37_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_37_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_37_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_75(torch.nn.Module):
        def forward(self, query_113: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_113: "bf16[s59, 16, 128]", value_37: "bf16[s59, 16, 128]", output_151: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_113, key_113, value_37, output_151, 'language_model.model.layers.37.self_attn.attn', kv_cache_dummy_dep = None);  query_113 = key_113 = value_37 = output_151 = unified_attention_with_output = None
            return ()
            
    class submod_76(torch.nn.Module):
        def forward(self, output_151: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_37_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_37_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_37_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_998: "bf16[s59, 5376]", l_self_modules_layers_modules_37_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_37_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_37_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_38_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_38_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_38_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_38_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_151.view(-1, 4096);  output_151 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_37_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_37_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_37_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_37_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_37_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_37_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_998;  to = x_998 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_37_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_37_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_37_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_37_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_37_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_37_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_38_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_38_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_38_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_38_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_38_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_38_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_38_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_38_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_77(torch.nn.Module):
        def forward(self, query_116: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_116: "bf16[s59, 16, 128]", value_38: "bf16[s59, 16, 128]", output_155: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_116, key_116, value_38, output_155, 'language_model.model.layers.38.self_attn.attn', kv_cache_dummy_dep = None);  query_116 = key_116 = value_38 = output_155 = unified_attention_with_output = None
            return ()
            
    class submod_78(torch.nn.Module):
        def forward(self, output_155: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_38_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_38_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_38_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1025: "bf16[s59, 5376]", l_self_modules_layers_modules_38_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_38_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_38_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_39_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_39_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_39_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_39_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_155.view(-1, 4096);  output_155 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_38_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_38_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_38_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_38_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_38_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_38_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1025;  to = x_1025 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_38_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_38_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_38_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_38_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_38_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_38_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_39_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_39_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_39_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_39_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_39_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_39_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_39_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_39_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_79(torch.nn.Module):
        def forward(self, query_119: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_119: "bf16[s59, 16, 128]", value_39: "bf16[s59, 16, 128]", output_159: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_119, key_119, value_39, output_159, 'language_model.model.layers.39.self_attn.attn', kv_cache_dummy_dep = None);  query_119 = key_119 = value_39 = output_159 = unified_attention_with_output = None
            return ()
            
    class submod_80(torch.nn.Module):
        def forward(self, output_159: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_39_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_39_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_39_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1052: "bf16[s59, 5376]", l_self_modules_layers_modules_39_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_39_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_39_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_40_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_40_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_40_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_40_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_159.view(-1, 4096);  output_159 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_39_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_39_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_39_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_39_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_39_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_39_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1052;  to = x_1052 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_39_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_39_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_39_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_39_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_39_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_39_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_40_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_40_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_40_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_40_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_40_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_40_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_40_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_40_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_81(torch.nn.Module):
        def forward(self, query_122: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_122: "bf16[s59, 16, 128]", value_40: "bf16[s59, 16, 128]", output_163: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_122, key_122, value_40, output_163, 'language_model.model.layers.40.self_attn.attn', kv_cache_dummy_dep = None);  query_122 = key_122 = value_40 = output_163 = unified_attention_with_output = None
            return ()
            
    class submod_82(torch.nn.Module):
        def forward(self, output_163: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_40_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_40_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_40_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1079: "bf16[s59, 5376]", l_self_modules_layers_modules_40_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_40_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_40_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_41_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_41_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_41_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_41_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_163.view(-1, 4096);  output_163 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_40_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_40_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_40_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_40_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_40_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_40_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1079;  to = x_1079 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_40_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_40_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_40_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_40_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_40_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_40_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_41_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_41_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_41_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_41_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_41_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_41_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_41_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_41_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_83(torch.nn.Module):
        def forward(self, query_125: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_125: "bf16[s59, 16, 128]", value_41: "bf16[s59, 16, 128]", output_167: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_125, key_125, value_41, output_167, 'language_model.model.layers.41.self_attn.attn', kv_cache_dummy_dep = None);  query_125 = key_125 = value_41 = output_167 = unified_attention_with_output = None
            return ()
            
    class submod_84(torch.nn.Module):
        def forward(self, output_167: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_41_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_41_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_41_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1106: "bf16[s59, 5376]", l_self_modules_layers_modules_41_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_41_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_41_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_42_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_42_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_42_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_42_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_167.view(-1, 4096);  output_167 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_41_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_41_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_41_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_41_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_41_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_41_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1106;  to = x_1106 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_41_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_41_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_41_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_41_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_41_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_41_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_42_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_42_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_42_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_42_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_42_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_42_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_42_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_42_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_85(torch.nn.Module):
        def forward(self, query_128: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_128: "bf16[s59, 16, 128]", value_42: "bf16[s59, 16, 128]", output_171: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_128, key_128, value_42, output_171, 'language_model.model.layers.42.self_attn.attn', kv_cache_dummy_dep = None);  query_128 = key_128 = value_42 = output_171 = unified_attention_with_output = None
            return ()
            
    class submod_86(torch.nn.Module):
        def forward(self, output_171: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_42_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_42_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_42_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1133: "bf16[s59, 5376]", l_self_modules_layers_modules_42_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_42_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_42_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_43_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_43_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_43_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_43_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_171.view(-1, 4096);  output_171 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_42_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_42_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_42_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_42_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_42_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_42_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1133;  to = x_1133 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_42_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_42_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_42_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_42_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_42_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_42_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_43_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_43_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_43_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_43_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_43_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_43_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_43_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_43_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_87(torch.nn.Module):
        def forward(self, query_131: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_131: "bf16[s59, 16, 128]", value_43: "bf16[s59, 16, 128]", output_175: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_131, key_131, value_43, output_175, 'language_model.model.layers.43.self_attn.attn', kv_cache_dummy_dep = None);  query_131 = key_131 = value_43 = output_175 = unified_attention_with_output = None
            return ()
            
    class submod_88(torch.nn.Module):
        def forward(self, output_175: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_43_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_43_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_43_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1160: "bf16[s59, 5376]", l_self_modules_layers_modules_43_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_43_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_43_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_44_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_44_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_44_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_44_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_175.view(-1, 4096);  output_175 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_43_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_43_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_43_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_43_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_43_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_43_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1160;  to = x_1160 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_43_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_43_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_43_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_43_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_43_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_43_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_44_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_44_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_44_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_44_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_44_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_44_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_44_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_44_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_89(torch.nn.Module):
        def forward(self, query_134: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_134: "bf16[s59, 16, 128]", value_44: "bf16[s59, 16, 128]", output_179: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_134, key_134, value_44, output_179, 'language_model.model.layers.44.self_attn.attn', kv_cache_dummy_dep = None);  query_134 = key_134 = value_44 = output_179 = unified_attention_with_output = None
            return ()
            
    class submod_90(torch.nn.Module):
        def forward(self, output_179: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_44_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_44_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_44_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1187: "bf16[s59, 5376]", l_self_modules_layers_modules_44_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_44_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_44_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_45_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_45_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_45_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_45_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_179.view(-1, 4096);  output_179 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_44_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_44_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_44_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_44_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_44_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_44_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1187;  to = x_1187 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_44_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_44_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_44_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_44_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_44_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_44_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_45_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_45_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_45_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_45_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_45_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_45_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_45_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_45_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_91(torch.nn.Module):
        def forward(self, query_137: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_137: "bf16[s59, 16, 128]", value_45: "bf16[s59, 16, 128]", output_183: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_137, key_137, value_45, output_183, 'language_model.model.layers.45.self_attn.attn', kv_cache_dummy_dep = None);  query_137 = key_137 = value_45 = output_183 = unified_attention_with_output = None
            return ()
            
    class submod_92(torch.nn.Module):
        def forward(self, output_183: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_45_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_45_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_45_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1214: "bf16[s59, 5376]", l_self_modules_layers_modules_45_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_45_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_45_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_46_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_46_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_46_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_46_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_183.view(-1, 4096);  output_183 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_45_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_45_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_45_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_45_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_45_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_45_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1214;  to = x_1214 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_45_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_45_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_45_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_45_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_45_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_45_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_46_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_46_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_46_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_46_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_46_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_46_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_46_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_46_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_93(torch.nn.Module):
        def forward(self, query_140: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_140: "bf16[s59, 16, 128]", value_46: "bf16[s59, 16, 128]", output_187: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_140, key_140, value_46, output_187, 'language_model.model.layers.46.self_attn.attn', kv_cache_dummy_dep = None);  query_140 = key_140 = value_46 = output_187 = unified_attention_with_output = None
            return ()
            
    class submod_94(torch.nn.Module):
        def forward(self, output_187: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_46_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_46_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_46_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1241: "bf16[s59, 5376]", l_self_modules_layers_modules_46_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_46_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_46_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_47_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_47_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_47_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_47_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_187.view(-1, 4096);  output_187 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_46_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_46_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_46_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_46_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_46_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_46_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1241;  to = x_1241 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_46_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_46_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_46_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_46_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_46_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_46_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_47_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_47_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_47_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_47_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_47_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_47_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_47_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_47_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_95(torch.nn.Module):
        def forward(self, query_143: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_143: "bf16[s59, 16, 128]", value_47: "bf16[s59, 16, 128]", output_191: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_143, key_143, value_47, output_191, 'language_model.model.layers.47.self_attn.attn', kv_cache_dummy_dep = None);  query_143 = key_143 = value_47 = output_191 = unified_attention_with_output = None
            return ()
            
    class submod_96(torch.nn.Module):
        def forward(self, output_191: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_47_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_47_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_47_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1268: "bf16[s59, 5376]", l_self_modules_layers_modules_47_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_47_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_47_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_48_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_48_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_48_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_48_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_191.view(-1, 4096);  output_191 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_47_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_47_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_47_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_47_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_47_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_47_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1268;  to = x_1268 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_47_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_47_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_47_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_47_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_47_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_47_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_48_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_48_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_48_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_48_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_48_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_48_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_48_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_48_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_97(torch.nn.Module):
        def forward(self, query_146: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_146: "bf16[s59, 16, 128]", value_48: "bf16[s59, 16, 128]", output_195: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_146, key_146, value_48, output_195, 'language_model.model.layers.48.self_attn.attn', kv_cache_dummy_dep = None);  query_146 = key_146 = value_48 = output_195 = unified_attention_with_output = None
            return ()
            
    class submod_98(torch.nn.Module):
        def forward(self, output_195: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_48_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_48_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_48_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1295: "bf16[s59, 5376]", l_self_modules_layers_modules_48_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_48_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_48_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_49_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_49_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_49_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_49_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_195.view(-1, 4096);  output_195 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_48_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_48_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_48_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_48_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_48_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_48_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1295;  to = x_1295 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_48_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_48_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_48_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_48_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_48_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_48_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_49_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_49_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_49_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_49_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_49_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_49_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_49_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_49_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_99(torch.nn.Module):
        def forward(self, query_149: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_149: "bf16[s59, 16, 128]", value_49: "bf16[s59, 16, 128]", output_199: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_149, key_149, value_49, output_199, 'language_model.model.layers.49.self_attn.attn', kv_cache_dummy_dep = None);  query_149 = key_149 = value_49 = output_199 = unified_attention_with_output = None
            return ()
            
    class submod_100(torch.nn.Module):
        def forward(self, output_199: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_49_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_49_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_49_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1322: "bf16[s59, 5376]", l_self_modules_layers_modules_49_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_49_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_49_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_50_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_50_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_50_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_50_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_199.view(-1, 4096);  output_199 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_49_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_49_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_49_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_49_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_49_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_49_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1322;  to = x_1322 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_49_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_49_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_49_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_49_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_49_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_49_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_50_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_50_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_50_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_50_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_50_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_50_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_50_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_50_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_101(torch.nn.Module):
        def forward(self, query_152: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_152: "bf16[s59, 16, 128]", value_50: "bf16[s59, 16, 128]", output_203: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_152, key_152, value_50, output_203, 'language_model.model.layers.50.self_attn.attn', kv_cache_dummy_dep = None);  query_152 = key_152 = value_50 = output_203 = unified_attention_with_output = None
            return ()
            
    class submod_102(torch.nn.Module):
        def forward(self, output_203: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_50_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_50_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_50_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1349: "bf16[s59, 5376]", l_self_modules_layers_modules_50_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_50_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_50_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_51_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_51_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_51_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_51_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_203.view(-1, 4096);  output_203 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_50_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_50_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_50_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_50_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_50_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_50_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1349;  to = x_1349 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_50_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_50_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_50_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_50_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_50_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_50_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_51_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_51_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_51_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_51_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_51_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_51_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_51_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_51_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_103(torch.nn.Module):
        def forward(self, query_155: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_155: "bf16[s59, 16, 128]", value_51: "bf16[s59, 16, 128]", output_207: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_155, key_155, value_51, output_207, 'language_model.model.layers.51.self_attn.attn', kv_cache_dummy_dep = None);  query_155 = key_155 = value_51 = output_207 = unified_attention_with_output = None
            return ()
            
    class submod_104(torch.nn.Module):
        def forward(self, output_207: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_51_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_51_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_51_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1376: "bf16[s59, 5376]", l_self_modules_layers_modules_51_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_51_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_51_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_52_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_52_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_52_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_52_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_207.view(-1, 4096);  output_207 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_51_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_51_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_51_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_51_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_51_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_51_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1376;  to = x_1376 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_51_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_51_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_51_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_51_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_51_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_51_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_52_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_52_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_52_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_52_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_52_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_52_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_52_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_52_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_105(torch.nn.Module):
        def forward(self, query_158: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_158: "bf16[s59, 16, 128]", value_52: "bf16[s59, 16, 128]", output_211: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_158, key_158, value_52, output_211, 'language_model.model.layers.52.self_attn.attn', kv_cache_dummy_dep = None);  query_158 = key_158 = value_52 = output_211 = unified_attention_with_output = None
            return ()
            
    class submod_106(torch.nn.Module):
        def forward(self, output_211: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_52_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_52_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_52_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1403: "bf16[s59, 5376]", l_self_modules_layers_modules_52_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_52_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_52_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_53_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_53_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_53_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_53_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_211.view(-1, 4096);  output_211 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_52_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_52_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_52_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_52_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_52_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_52_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1403;  to = x_1403 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_52_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_52_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_52_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_52_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_52_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_52_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_53_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_53_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_53_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_53_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_53_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_53_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_53_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_53_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_107(torch.nn.Module):
        def forward(self, query_161: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_161: "bf16[s59, 16, 128]", value_53: "bf16[s59, 16, 128]", output_215: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_161, key_161, value_53, output_215, 'language_model.model.layers.53.self_attn.attn', kv_cache_dummy_dep = None);  query_161 = key_161 = value_53 = output_215 = unified_attention_with_output = None
            return ()
            
    class submod_108(torch.nn.Module):
        def forward(self, output_215: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_53_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_53_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_53_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1430: "bf16[s59, 5376]", l_self_modules_layers_modules_53_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_53_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_53_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_54_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_54_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_54_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_54_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_215.view(-1, 4096);  output_215 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_53_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_53_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_53_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_53_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_53_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_53_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1430;  to = x_1430 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_53_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_53_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_53_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_53_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_53_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_53_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_54_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_54_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_54_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_54_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_54_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_54_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_54_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_54_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_109(torch.nn.Module):
        def forward(self, query_164: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_164: "bf16[s59, 16, 128]", value_54: "bf16[s59, 16, 128]", output_219: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_164, key_164, value_54, output_219, 'language_model.model.layers.54.self_attn.attn', kv_cache_dummy_dep = None);  query_164 = key_164 = value_54 = output_219 = unified_attention_with_output = None
            return ()
            
    class submod_110(torch.nn.Module):
        def forward(self, output_219: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_54_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_54_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_54_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1457: "bf16[s59, 5376]", l_self_modules_layers_modules_54_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_54_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_54_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_55_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_55_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_55_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_55_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_219.view(-1, 4096);  output_219 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_54_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_54_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_54_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_54_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_54_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_54_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1457;  to = x_1457 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_54_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_54_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_54_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_54_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_54_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_54_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_55_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_55_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_55_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_55_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_55_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_55_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_55_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_55_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_111(torch.nn.Module):
        def forward(self, query_167: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_167: "bf16[s59, 16, 128]", value_55: "bf16[s59, 16, 128]", output_223: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_167, key_167, value_55, output_223, 'language_model.model.layers.55.self_attn.attn', kv_cache_dummy_dep = None);  query_167 = key_167 = value_55 = output_223 = unified_attention_with_output = None
            return ()
            
    class submod_112(torch.nn.Module):
        def forward(self, output_223: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_55_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_55_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_55_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1484: "bf16[s59, 5376]", l_self_modules_layers_modules_55_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_55_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_55_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_56_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_56_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_56_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_56_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_223.view(-1, 4096);  output_223 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_55_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_55_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_55_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_55_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_55_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_55_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1484;  to = x_1484 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_55_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_55_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_55_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_55_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_55_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_55_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_56_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_56_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_56_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_56_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_56_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_56_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_56_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_56_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_113(torch.nn.Module):
        def forward(self, query_170: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_170: "bf16[s59, 16, 128]", value_56: "bf16[s59, 16, 128]", output_227: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_170, key_170, value_56, output_227, 'language_model.model.layers.56.self_attn.attn', kv_cache_dummy_dep = None);  query_170 = key_170 = value_56 = output_227 = unified_attention_with_output = None
            return ()
            
    class submod_114(torch.nn.Module):
        def forward(self, output_227: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_56_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_56_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_56_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1511: "bf16[s59, 5376]", l_self_modules_layers_modules_56_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_56_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_56_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_57_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_57_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_57_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_57_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_227.view(-1, 4096);  output_227 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_56_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_56_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_56_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_56_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_56_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_56_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1511;  to = x_1511 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_56_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_56_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_56_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_56_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_56_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_56_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_57_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_57_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_57_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_57_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_57_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_57_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_57_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_57_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_115(torch.nn.Module):
        def forward(self, query_173: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_173: "bf16[s59, 16, 128]", value_57: "bf16[s59, 16, 128]", output_231: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_173, key_173, value_57, output_231, 'language_model.model.layers.57.self_attn.attn', kv_cache_dummy_dep = None);  query_173 = key_173 = value_57 = output_231 = unified_attention_with_output = None
            return ()
            
    class submod_116(torch.nn.Module):
        def forward(self, output_231: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_57_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_57_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_57_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1538: "bf16[s59, 5376]", l_self_modules_layers_modules_57_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_57_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_57_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_58_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_58_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_58_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_58_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_231.view(-1, 4096);  output_231 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_57_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_57_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_57_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_57_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_57_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_57_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1538;  to = x_1538 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_57_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_57_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_57_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_57_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_57_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_57_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_58_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_58_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_58_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_58_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_58_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_58_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_58_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_58_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_117(torch.nn.Module):
        def forward(self, query_176: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_176: "bf16[s59, 16, 128]", value_58: "bf16[s59, 16, 128]", output_235: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_176, key_176, value_58, output_235, 'language_model.model.layers.58.self_attn.attn', kv_cache_dummy_dep = None);  query_176 = key_176 = value_58 = output_235 = unified_attention_with_output = None
            return ()
            
    class submod_118(torch.nn.Module):
        def forward(self, output_235: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_58_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_58_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_58_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1565: "bf16[s59, 5376]", l_self_modules_layers_modules_58_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_58_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_58_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_59_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_59_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_59_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_59_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[1048576, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_235.view(-1, 4096);  output_235 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_58_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_58_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_58_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_58_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_58_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_58_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1565;  to = x_1565 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_58_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_58_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_58_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_58_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_58_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_58_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_59_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_59_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_59_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_59_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_59_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_59_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_59_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_59_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_5_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_119(torch.nn.Module):
        def forward(self, query_179: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_179: "bf16[s59, 16, 128]", value_59: "bf16[s59, 16, 128]", output_239: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_179, key_179, value_59, output_239, 'language_model.model.layers.59.self_attn.attn', kv_cache_dummy_dep = None);  query_179 = key_179 = value_59 = output_239 = unified_attention_with_output = None
            return ()
            
    class submod_120(torch.nn.Module):
        def forward(self, output_239: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_59_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_59_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_59_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1592: "bf16[s59, 5376]", l_self_modules_layers_modules_59_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_59_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_59_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_60_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_60_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_60_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_60_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_239.view(-1, 4096);  output_239 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_59_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_59_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_59_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_59_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_59_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_59_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1592;  to = x_1592 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_59_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_59_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_59_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_59_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_59_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_59_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_60_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_60_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_60_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_60_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_60_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_60_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_60_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_60_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_121(torch.nn.Module):
        def forward(self, query_182: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_182: "bf16[s59, 16, 128]", value_60: "bf16[s59, 16, 128]", output_243: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_182, key_182, value_60, output_243, 'language_model.model.layers.60.self_attn.attn', kv_cache_dummy_dep = None);  query_182 = key_182 = value_60 = output_243 = unified_attention_with_output = None
            return ()
            
    class submod_122(torch.nn.Module):
        def forward(self, output_243: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_60_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_60_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_60_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1619: "bf16[s59, 5376]", l_self_modules_layers_modules_60_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_60_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_60_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_61_modules_input_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_61_modules_self_attn_modules_qkv_proj_parameters_weight_: "bf16[8192, 5376]", l_self_modules_layers_modules_61_modules_self_attn_modules_q_norm_parameters_weight_: "bf16[128]", l_self_modules_layers_modules_61_modules_self_attn_modules_k_norm_parameters_weight_: "bf16[128]", l_positions_: "i64[s59]", l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_: "bf16[131072, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_243.view(-1, 4096);  output_243 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_60_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_60_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_60_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_60_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_60_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_60_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1619;  to = x_1619 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_60_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_60_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_60_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_60_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_60_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_60_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_61_modules_input_layernorm_parameters_weight_);  l_self_modules_layers_modules_61_modules_input_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_3: "bf16[s59, 8192]" = torch._C._nn.linear(to_3, l_self_modules_layers_modules_61_modules_self_attn_modules_qkv_proj_parameters_weight_, None);  to_3 = l_self_modules_layers_modules_61_modules_self_attn_modules_qkv_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:219 in forward, code: q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
            split = linear_3.split([4096, 2048, 2048], dim = -1);  linear_3 = None
            getitem_2: "bf16[s59, 4096]" = split[0]
            getitem_3: "bf16[s59, 2048]" = split[1]
            getitem_4: "bf16[s59, 2048]" = split[2];  split = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:221 in forward, code: q = q.unflatten(-1, (self.num_heads, self.head_dim))
            unflatten: "bf16[s59, 32, 128]" = getitem_2.unflatten(-1, (32, 128));  getitem_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_4: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_61_modules_self_attn_modules_q_norm_parameters_weight_);  l_self_modules_layers_modules_61_modules_self_attn_modules_q_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_9: "f32[s59, 32, 128]" = unflatten.float();  unflatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_5: "f32[s59, 32, 128]" = float_9.pow(2)
            mean_4: "f32[s59, 32, 1]" = pow_5.mean(dim = -1, keepdim = True);  pow_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_10: "f32[s59, 32, 1]" = mean_4 + 1e-06;  mean_4 = None
            rsqrt_4: "f32[s59, 32, 1]" = torch.rsqrt(add_10);  add_10 = None
            mul_9: "f32[s59, 32, 128]" = float_9 * rsqrt_4;  float_9 = rsqrt_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_10: "f32[128]" = _get_data_attr_4.float();  _get_data_attr_4 = None
            add_11: "f32[128]" = 1.0 + float_10;  float_10 = None
            mul_10: "f32[s59, 32, 128]" = mul_9 * add_11;  mul_9 = add_11 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_4: "bf16[s59, 32, 128]" = mul_10.to(torch.bfloat16);  mul_10 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:223 in forward, code: q = q.flatten(-2, -1)
            flatten: "bf16[s59, 4096]" = to_4.flatten(-2, -1);  to_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:224 in forward, code: k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            unflatten_1: "bf16[s59, 16, 128]" = getitem_3.unflatten(-1, (16, 128));  getitem_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_5: "bf16[128]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_61_modules_self_attn_modules_k_norm_parameters_weight_);  l_self_modules_layers_modules_61_modules_self_attn_modules_k_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_11: "f32[s59, 16, 128]" = unflatten_1.float();  unflatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_6: "f32[s59, 16, 128]" = float_11.pow(2)
            mean_5: "f32[s59, 16, 1]" = pow_6.mean(dim = -1, keepdim = True);  pow_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_12: "f32[s59, 16, 1]" = mean_5 + 1e-06;  mean_5 = None
            rsqrt_5: "f32[s59, 16, 1]" = torch.rsqrt(add_12);  add_12 = None
            mul_11: "f32[s59, 16, 128]" = float_11 * rsqrt_5;  float_11 = rsqrt_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_12: "f32[128]" = _get_data_attr_5.float();  _get_data_attr_5 = None
            add_13: "f32[128]" = 1.0 + float_12;  float_12 = None
            mul_12: "f32[s59, 16, 128]" = mul_11 * add_13;  mul_11 = add_13 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_5: "bf16[s59, 16, 128]" = mul_12.to(torch.bfloat16);  mul_12 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/gemma3.py:226 in forward, code: k = k.flatten(-2, -1)
            flatten_1: "bf16[s59, 2048]" = to_5.flatten(-2, -1);  to_5 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:127 in forward_static, code: positions = positions.flatten()
            flatten_2: "i64[s59]" = l_positions_.flatten();  l_positions_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:129 in forward_static, code: cos_sin = cos_sin_cache.index_select(0, positions)
            index_select: "bf16[s59, 128]" = l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_.index_select(0, flatten_2);  l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_ = flatten_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:130 in forward_static, code: cos, sin = cos_sin.chunk(2, dim=-1)
            chunk = index_select.chunk(2, dim = -1);  index_select = None
            getitem_5: "bf16[s59, 64]" = chunk[0]
            getitem_6: "bf16[s59, 64]" = chunk[1];  chunk = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:132 in forward_static, code: query_shape = query.shape
            size = flatten.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:133 in forward_static, code: query = query.view(num_tokens, -1, head_size)
            view_1: "bf16[s59, 32, 128]" = flatten.view(s59, -1, 128);  flatten = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:134 in forward_static, code: query_rot = query[..., :rotary_dim]
            getitem_7: "bf16[s59, 32, 128]" = view_1[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:135 in forward_static, code: query_pass = query[..., rotary_dim:]
            getitem_8: "bf16[s59, 32, 0]" = view_1[(Ellipsis, slice(128, None, None))];  view_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2)
            to_6: "bf16[s59, 1, 64]" = unsqueeze.to(torch.bfloat16);  unsqueeze = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_1: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2)
            to_7: "bf16[s59, 1, 64]" = unsqueeze_1.to(torch.bfloat16);  unsqueeze_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_1 = torch.chunk(getitem_7, 2, dim = -1);  getitem_7 = None
            getitem_9: "bf16[s59, 32, 64]" = chunk_1[0]
            getitem_10: "bf16[s59, 32, 64]" = chunk_1[1];  chunk_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_13: "bf16[s59, 32, 64]" = getitem_9 * to_6
            mul_14: "bf16[s59, 32, 64]" = getitem_10 * to_7
            sub: "bf16[s59, 32, 64]" = mul_13 - mul_14;  mul_13 = mul_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_15: "bf16[s59, 32, 64]" = getitem_10 * to_6;  getitem_10 = to_6 = None
            mul_16: "bf16[s59, 32, 64]" = getitem_9 * to_7;  getitem_9 = to_7 = None
            add_14: "bf16[s59, 32, 64]" = mul_15 + mul_16;  mul_15 = mul_16 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat: "bf16[s59, 32, 128]" = torch.cat((sub, add_14), dim = -1);  sub = add_14 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:142 in forward_static, code: query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
            cat_1: "bf16[s59, 32, 128]" = torch.cat((cat, getitem_8), dim = -1);  cat = getitem_8 = None
            reshape: "bf16[s59, 4096]" = cat_1.reshape(size);  cat_1 = size = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:146 in forward_static, code: key_shape = key.shape
            size_1 = flatten_1.size()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:147 in forward_static, code: key = key.view(num_tokens, -1, head_size)
            view_2: "bf16[s59, 16, 128]" = flatten_1.view(s59, -1, 128);  flatten_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:148 in forward_static, code: key_rot = key[..., :rotary_dim]
            getitem_11: "bf16[s59, 16, 128]" = view_2[(Ellipsis, slice(None, 128, None))]
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:149 in forward_static, code: key_pass = key[..., rotary_dim:]
            getitem_12: "bf16[s59, 16, 0]" = view_2[(Ellipsis, slice(128, None, None))];  view_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:163 in forward_static, code: cos = cos.unsqueeze(-2).to(x.dtype)
            unsqueeze_2: "bf16[s59, 1, 64]" = getitem_5.unsqueeze(-2);  getitem_5 = None
            to_8: "bf16[s59, 1, 64]" = unsqueeze_2.to(torch.bfloat16);  unsqueeze_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:164 in forward_static, code: sin = sin.unsqueeze(-2).to(x.dtype)
            unsqueeze_3: "bf16[s59, 1, 64]" = getitem_6.unsqueeze(-2);  getitem_6 = None
            to_9: "bf16[s59, 1, 64]" = unsqueeze_3.to(torch.bfloat16);  unsqueeze_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:167 in forward_static, code: x1, x2 = torch.chunk(x, 2, dim=-1)
            chunk_2 = torch.chunk(getitem_11, 2, dim = -1);  getitem_11 = None
            getitem_13: "bf16[s59, 16, 64]" = chunk_2[0]
            getitem_14: "bf16[s59, 16, 64]" = chunk_2[1];  chunk_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:172 in forward_static, code: o1 = x1 * cos - x2 * sin
            mul_17: "bf16[s59, 16, 64]" = getitem_13 * to_8
            mul_18: "bf16[s59, 16, 64]" = getitem_14 * to_9
            sub_1: "bf16[s59, 16, 64]" = mul_17 - mul_18;  mul_17 = mul_18 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:173 in forward_static, code: o2 = x2 * cos + x1 * sin
            mul_19: "bf16[s59, 16, 64]" = getitem_14 * to_8;  getitem_14 = to_8 = None
            mul_20: "bf16[s59, 16, 64]" = getitem_13 * to_9;  getitem_13 = to_9 = None
            add_15: "bf16[s59, 16, 64]" = mul_19 + mul_20;  mul_19 = mul_20 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/common.py:176 in forward_static, code: output = torch.cat((o1, o2), dim=-1)
            cat_2: "bf16[s59, 16, 128]" = torch.cat((sub_1, add_15), dim = -1);  sub_1 = add_15 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding/base.py:156 in forward_static, code: key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
            cat_3: "bf16[s59, 16, 128]" = torch.cat((cat_2, getitem_12), dim = -1);  cat_2 = getitem_12 = None
            reshape_1: "bf16[s59, 2048]" = cat_3.reshape(size_1);  cat_3 = size_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:381 in forward, code: output = torch.empty(output_shape, dtype=output_dtype, device=query.device)
            size_2 = torch.Size([s59, 4096]);  s59 = None
            empty: "bf16[s59, 4096]" = torch.empty(size_2, dtype = torch.bfloat16, device = device(type='cuda', index=0));  size_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:386 in forward, code: query = query.view(-1, self.num_heads, self.head_size)
            view_3: "bf16[s59, 32, 128]" = reshape.view(-1, 32, 128);  reshape = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:387 in forward, code: output = output.view(-1, self.num_heads, self.head_size_v)
            view_4: "bf16[s59, 32, 128]" = empty.view(-1, 32, 128);  empty = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:389 in forward, code: key = key.view(-1, self.num_kv_heads, self.head_size)
            view_5: "bf16[s59, 16, 128]" = reshape_1.view(-1, 16, 128);  reshape_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:391 in forward, code: value = value.view(-1, self.num_kv_heads, self.head_size_v)
            view_6: "bf16[s59, 16, 128]" = getitem_4.view(-1, 16, 128);  getitem_4 = None
            return (view_3, view_5, view_6, view_4, add_7)
            
    class submod_123(torch.nn.Module):
        def forward(self, query_185: "bf16[s59, 32, 128]", s59: "Sym(s59)", key_185: "bf16[s59, 16, 128]", value_61: "bf16[s59, 16, 128]", output_247: "bf16[s59, 32, 128]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:415 in forward, code: torch.ops.vllm.unified_attention_with_output(
            unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_185, key_185, value_61, output_247, 'language_model.model.layers.61.self_attn.attn', kv_cache_dummy_dep = None);  query_185 = key_185 = value_61 = output_247 = unified_attention_with_output = None
            return ()
            
    class submod_124(torch.nn.Module):
        def forward(self, output_247: "bf16[s59, 32, 128]", s59: "Sym(s59)", l_self_modules_layers_modules_61_modules_self_attn_modules_o_proj_parameters_weight_: "bf16[5376, 4096]", l_self_modules_layers_modules_61_modules_post_attention_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_layers_modules_61_modules_pre_feedforward_layernorm_parameters_weight_: "bf16[5376]", x_1646: "bf16[s59, 5376]", l_self_modules_layers_modules_61_modules_mlp_modules_gate_up_proj_parameters_weight_: "bf16[43008, 5376]", l_self_modules_layers_modules_61_modules_mlp_modules_down_proj_parameters_weight_: "bf16[5376, 21504]", l_self_modules_layers_modules_61_modules_post_feedforward_layernorm_parameters_weight_: "bf16[5376]", l_self_modules_norm_parameters_weight_: "bf16[5376]"):
             # File: /usr/local/lib/python3.12/dist-packages/vllm/attention/layer.py:423 in forward, code: return output.view(-1, hidden_size)
            view: "bf16[s59, 4096]" = output_247.view(-1, 4096);  output_247 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear: "bf16[s59, 5376]" = torch._C._nn.linear(view, l_self_modules_layers_modules_61_modules_self_attn_modules_o_proj_parameters_weight_, None);  view = l_self_modules_layers_modules_61_modules_self_attn_modules_o_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_61_modules_post_attention_layernorm_parameters_weight_);  l_self_modules_layers_modules_61_modules_post_attention_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_1: "f32[s59, 5376]" = linear.float();  linear = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_1: "f32[s59, 5376]" = float_1.pow(2)
            mean: "f32[s59, 1]" = pow_1.mean(dim = -1, keepdim = True);  pow_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add: "f32[s59, 1]" = mean + 1e-06;  mean = None
            rsqrt: "f32[s59, 1]" = torch.rsqrt(add);  add = None
            mul: "f32[s59, 5376]" = float_1 * rsqrt;  float_1 = rsqrt = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_2: "f32[5376]" = _get_data_attr.float();  _get_data_attr = None
            add_1: "f32[5376]" = 1.0 + float_2;  float_2 = None
            mul_1: "f32[s59, 5376]" = mul * add_1;  mul = add_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to: "bf16[s59, 5376]" = mul_1.to(torch.bfloat16);  mul_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_1: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_61_modules_pre_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_61_modules_pre_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_2: "bf16[s59, 5376]" = to + x_1646;  to = x_1646 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_3: "f32[s59, 5376]" = add_2.float()
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_2: "f32[s59, 5376]" = float_3.pow(2)
            mean_1: "f32[s59, 1]" = pow_2.mean(dim = -1, keepdim = True);  pow_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_3: "f32[s59, 1]" = mean_1 + 1e-06;  mean_1 = None
            rsqrt_1: "f32[s59, 1]" = torch.rsqrt(add_3);  add_3 = None
            mul_2: "f32[s59, 5376]" = float_3 * rsqrt_1;  float_3 = rsqrt_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_4: "f32[5376]" = _get_data_attr_1.float();  _get_data_attr_1 = None
            add_4: "f32[5376]" = 1.0 + float_4;  float_4 = None
            mul_3: "f32[s59, 5376]" = mul_2 * add_4;  mul_2 = add_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_1: "bf16[s59, 5376]" = mul_3.to(torch.bfloat16);  mul_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_1: "bf16[s59, 43008]" = torch._C._nn.linear(to_1, l_self_modules_layers_modules_61_modules_mlp_modules_gate_up_proj_parameters_weight_, None);  to_1 = l_self_modules_layers_modules_61_modules_mlp_modules_gate_up_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/activation.py:307 in forward_native, code: return F.gelu(x[..., :d], approximate=approximate) * x[..., d:]
            getitem: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(None, 21504, None))]
            gelu: "bf16[s59, 21504]" = torch._C._nn.gelu(getitem, approximate = 'tanh');  getitem = None
            getitem_1: "bf16[s59, 21504]" = linear_1[(Ellipsis, slice(21504, None, None))];  linear_1 = None
            mul_4: "bf16[s59, 21504]" = gelu * getitem_1;  gelu = getitem_1 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/utils.py:105 in default_unquantized_gemm, code: return torch.nn.functional.linear(x, weight, bias)
            linear_2: "bf16[s59, 5376]" = torch._C._nn.linear(mul_4, l_self_modules_layers_modules_61_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_4 = l_self_modules_layers_modules_61_modules_mlp_modules_down_proj_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:328 in forward_native, code: self.weight.data, self.variance_epsilon, x
            _get_data_attr_2: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_layers_modules_61_modules_post_feedforward_layernorm_parameters_weight_);  l_self_modules_layers_modules_61_modules_post_feedforward_layernorm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:288 in _forward_static_no_residual, code: x = x.float()
            float_5: "f32[s59, 5376]" = linear_2.float();  linear_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:289 in _forward_static_no_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_3: "f32[s59, 5376]" = float_5.pow(2)
            mean_2: "f32[s59, 1]" = pow_3.mean(dim = -1, keepdim = True);  pow_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:290 in _forward_static_no_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_5: "f32[s59, 1]" = mean_2 + 1e-06;  mean_2 = None
            rsqrt_2: "f32[s59, 1]" = torch.rsqrt(add_5);  add_5 = None
            mul_5: "f32[s59, 5376]" = float_5 * rsqrt_2;  float_5 = rsqrt_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:291 in _forward_static_no_residual, code: x = x * (1.0 + weight.float())
            float_6: "f32[5376]" = _get_data_attr_2.float();  _get_data_attr_2 = None
            add_6: "f32[5376]" = 1.0 + float_6;  float_6 = None
            mul_6: "f32[s59, 5376]" = mul_5 * add_6;  mul_5 = add_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:292 in _forward_static_no_residual, code: x = x.to(orig_dtype)
            to_2: "bf16[s59, 5376]" = mul_6.to(torch.bfloat16);  mul_6 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:332 in forward_native, code: self.weight.data, self.variance_epsilon, x, residual
            _get_data_attr_3: "bf16[5376]" = torch._C._autograd._get_data_attr(l_self_modules_norm_parameters_weight_);  l_self_modules_norm_parameters_weight_ = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:307 in _forward_static_with_residual, code: else x + residual
            add_7: "bf16[s59, 5376]" = to_2 + add_2;  to_2 = add_2 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:311 in _forward_static_with_residual, code: x = x.float()
            float_7: "f32[s59, 5376]" = add_7.float();  add_7 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:312 in _forward_static_with_residual, code: variance = x.pow(2).mean(dim=-1, keepdim=True)
            pow_4: "f32[s59, 5376]" = float_7.pow(2)
            mean_3: "f32[s59, 1]" = pow_4.mean(dim = -1, keepdim = True);  pow_4 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:313 in _forward_static_with_residual, code: x = x * torch.rsqrt(variance + variance_epsilon)
            add_8: "f32[s59, 1]" = mean_3 + 1e-06;  mean_3 = None
            rsqrt_3: "f32[s59, 1]" = torch.rsqrt(add_8);  add_8 = None
            mul_7: "f32[s59, 5376]" = float_7 * rsqrt_3;  float_7 = rsqrt_3 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:316 in _forward_static_with_residual, code: x = x * (1.0 + weight.float())
            float_8: "f32[5376]" = _get_data_attr_3.float();  _get_data_attr_3 = None
            add_9: "f32[5376]" = 1.0 + float_8;  float_8 = None
            mul_8: "f32[s59, 5376]" = mul_7 * add_9;  mul_7 = add_9 = None
            
             # File: /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/layernorm.py:317 in _forward_static_with_residual, code: x = x.to(orig_dtype)
            to_3: "bf16[s59, 5376]" = mul_8.to(torch.bfloat16);  mul_8 = None
            return to_3
            