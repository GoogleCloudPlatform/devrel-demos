{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "942914b4-b30c-4b2a-ab33-059dcf8e8a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1136e863f42f42cca20b3b1a4e4f8276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fine-tuning MedGemma-4B for Breast Cancer Histopathology Classification\n",
    "# This notebook demonstrates how to fine-tune Google's MedGemma vision-language model\n",
    "# on the BreakHis breast cancer dataset using LoRA (Low-Rank Adaptation)\n",
    "\n",
    "# ============================================================================\n",
    "# 0. SETUP AND INSTALLATIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages\n",
    "!pip install --upgrade --quiet transformers datasets evaluate peft trl scikit-learn \n",
    "# Then, reinstall it, forcing a build from source\n",
    "# This will take a few minutes as it compiles the code\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "# Hugging face authentication\n",
    "\n",
    "# Important Security Note: You should never hardcode secrets like API keys or tokens directly into your code or notebooks, especially in a production environment. This practice is insecure and creates a significant security risk.\n",
    "\n",
    "# The most secure and \"enterprise-grade\" way to handle secrets (like  Hugging Face token) in Vertex AI Workbench is to use Google Cloud Secret Manager. \n",
    "\n",
    "#To do that you will need to:\n",
    "# 1. In the Google cloud console, go to Google Cloud Secret Manager and create your secret. \n",
    "# 2. Grant Permission to your Workbench Instance \"Service Account\" to read the secret - Find your Workbench Service Account email (click on the instance name), Go to the IAM & Admin page, find that service account and add the role: Secret Manager Secret Accessor.\n",
    "# 3. Access the Secret in Your Notebook (see https://docs.cloud.google.com/secret-manager/docs/reference/libraries#client-libraries-usage-python)\n",
    "\n",
    "# If you are just experimenting and don't want to set up Secret Manager yet, you can use the interactive login widget. This saves the token temporarily in the instance's file system.\n",
    "\n",
    "\n",
    "# Hugging Face authentication using interactive login widget\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "\n",
    "# Hugging Face authentication using Google cloud secret manager\n",
    "\n",
    "# from google.cloud import secretmanager\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# def get_secret(secret_id, version_id=\"latest\"):\n",
    "#     # Create the Secret Manager client.\n",
    "#     client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "#     # Build the resource name of the secret version.\n",
    "#     # Replace 'YOUR_PROJECT_ID' with your actual project ID\n",
    "#     project_id = \"YOUR_PROJECT_ID\" \n",
    "#     name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "\n",
    "#     # Access the secret version.\n",
    "#     response = client.access_secret_version(request={\"name\": name})\n",
    "#     return response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "# # Retrieve the token\n",
    "# hf_token = get_secret(\"hugging-face-token\")\n",
    "\n",
    "# # Login to Hugging Face\n",
    "# login(token=hf_token)\n",
    "\n",
    "# print(\"Successfully logged in to Hugging Face!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f33bdec-577c-46fe-a8c6-23817b637e16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Path to dataset files: /home/jupyter/.cache/kagglehub/datasets/ambarish/breakhis/versions/4\n",
      "\n",
      "For 100X magnificantion found: 1321 train files:  29% benign, 71% malignant, 760 test files  with 34% benign and 66% malignant.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. LOAD AND PREPARE DATA FROM KAGGLE\n",
    "# ============================================================================\n",
    "\n",
    "! pip install -q kagglehub\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd  # <-- This import was missing\n",
    "from PIL import Image\n",
    "\n",
    "# Download the dataset metadata\n",
    "path = kagglehub.dataset_download(\"ambarish/breakhis\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "folds = pd.read_csv('{}/Folds.csv'.format(path))\n",
    "\n",
    "# Filter for 100X magnification from the first fold\n",
    "folds_100x = folds[folds['mag']==100]\n",
    "folds_100x = folds_100x[folds_100x['fold']==1]\n",
    "\n",
    "\n",
    "# Get the train/test splits\n",
    "folds_100x_test = folds_100x[folds_100x.grp=='test']\n",
    "folds_100x_train = folds_100x[folds_100x.grp=='train']\n",
    "\n",
    "# Get the lists of relative filenames\n",
    "test_filenames = folds_100x_test.filename.values\n",
    "train_filenames = folds_100x_train.filename.values\n",
    "\n",
    "# Define the base path for images\n",
    "BASE_PATH = \"/home/jupyter/.cache/kagglehub/datasets/ambarish/breakhis/versions/4/BreaKHis_v1\"\n",
    "\n",
    "rate_benign_in_test = round(100* sum(folds_100x_test.filename.str.contains('benign')==1)/len(test_filenames))\n",
    "rate_malignant_in_test = round(100*sum(folds_100x_test.filename.str.contains('malignant')==1)/len(test_filenames))\n",
    "rate_benign_in_train = round(100*sum(folds_100x_train.filename.str.contains('benign')==1)/len(train_filenames))\n",
    "rate_malignant_in_train = round(100*sum(folds_100x_train.filename.str.contains('malignant')==1)/len(train_filenames))\n",
    "\n",
    "print(f\"\\nFor 100X magnificantion found: {len(train_filenames)} train files:  {rate_benign_in_train}% benign, {rate_malignant_in_train}% malignant, {len(test_filenames)} test files  with {rate_benign_in_test}% benign and {rate_malignant_in_test}% malignant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e420ba0a-a9d0-433f-9795-4e596d0fc5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train files: 1321\n",
      "Original test files: 760\n",
      "\n",
      "--- Balanced Sets Created (50/50) ---\n",
      "Balanced Train: 766 files (383 benign + 383 malignant)\n",
      "Balanced Test:  522 files (261 benign + 261 malignant)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 1.1 UNDERSAMPLE DATA TO GET BALANCED TRAIN AND TEST SETS\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Create Balanced TRAIN Set ---\n",
    "\n",
    "# Separate benign and malignant training files\n",
    "train_benign_df = folds_100x_train[folds_100x_train['filename'].str.contains('benign')]\n",
    "train_malignant_df = folds_100x_train[folds_100x_train['filename'].str.contains('malignant')]\n",
    "\n",
    "# Find the size of the smaller class\n",
    "min_train_count = min(len(train_benign_df), len(train_malignant_df))\n",
    "\n",
    "# Sample the smaller amount from both classes\n",
    "balanced_train_benign = train_benign_df.sample(n=min_train_count, random_state=42)\n",
    "balanced_train_malignant = train_malignant_df.sample(n=min_train_count, random_state=42)\n",
    "\n",
    "# Combine them into a new balanced DataFrame\n",
    "balanced_train_df = pd.concat([balanced_train_benign, balanced_train_malignant])\n",
    "\n",
    "# --- 2. Create Balanced TEST Set ---\n",
    "\n",
    "# Separate benign and malignant test files\n",
    "test_benign_df = folds_100x_test[folds_100x_test['filename'].str.contains('benign')]\n",
    "test_malignant_df = folds_100x_test[folds_100x_test['filename'].str.contains('malignant')]\n",
    "\n",
    "# Find the size of the smaller class\n",
    "min_test_count = min(len(test_benign_df), len(test_malignant_df))\n",
    "\n",
    "# Sample the smaller amount from both classes\n",
    "balanced_test_benign = test_benign_df.sample(n=min_test_count, random_state=42)\n",
    "balanced_test_malignant = test_malignant_df.sample(n=min_test_count, random_state=42)\n",
    "\n",
    "# Combine them into a new balanced DataFrame\n",
    "balanced_test_df = pd.concat([balanced_test_benign, balanced_test_malignant])\n",
    "\n",
    "# --- 3. Get the Final Filename Lists ---\n",
    "\n",
    "# These are the variables you requested\n",
    "balanced_train_filenames = balanced_train_df['filename'].values\n",
    "balanced_test_filenames = balanced_test_df['filename'].values\n",
    "\n",
    "\n",
    "# --- 4. Print Summary ---\n",
    "print(\"Original train files:\", len(folds_100x_train))\n",
    "print(\"Original test files:\", len(folds_100x_test))\n",
    "print(\"\\n--- Balanced Sets Created (50/50) ---\")\n",
    "print(f\"Balanced Train: {len(balanced_train_filenames)} files ({min_train_count} benign + {min_train_count} malignant)\")\n",
    "print(f\"Balanced Test:  {len(balanced_test_filenames)} files ({min_test_count} benign + {min_test_count} malignant)\")\n",
    "\n",
    "test_filenames = balanced_test_filenames\n",
    "train_filenames = balanced_train_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e0dc88-9ac3-40c2-876a-907c0cc47c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Created new Hugging Face Datasets ---\n",
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 766\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 522\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5251fe5d2f43a29c25fd65780f09d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e07853645a4f9a8a924dd8add76d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/522 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applied formatting ---\n",
      "Dataset({\n",
      "    features: ['image', 'label', 'messages'],\n",
      "    num_rows: 766\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'label', 'messages'],\n",
      "    num_rows: 522\n",
      "})\n",
      "\n",
      "--- Example from new formatted_train ---\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=700x460 at 0x7F220BCB6B60>, 'label': 2, 'messages': [{'content': [{'text': None, 'type': 'image'}, {'text': 'Analyze this breast tissue histopathology image and classify it.\\n\\nClasses (0-7):\\n0: benign_adenosis\\n1: benign_fibroadenoma\\n2: benign_phyllodes_tumor\\n3: benign_tubular_adenoma\\n4: malignant_ductal_carcinoma\\n5: malignant_lobular_carcinoma\\n6: malignant_mucinous_carcinoma\\n7: malignant_papillary_carcinoma\\n\\nAnswer with only the number (0-7):', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': '2', 'type': 'text'}], 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 2. CREATE HUGGING FACE DATASETS\n",
    "# ============================================================================\n",
    "from datasets import Dataset, Image as HFImage, Features, ClassLabel\n",
    "\n",
    "# These class names match your original notebook (cell [40])\n",
    "CLASS_NAMES = [\n",
    "    'benign_adenosis', 'benign_fibroadenoma', 'benign_phyllodes_tumor', \n",
    "    'benign_tubular_adenoma', 'malignant_ductal_carcinoma', \n",
    "    'malignant_lobular_carcinoma', 'malignant_mucinous_carcinoma', \n",
    "    'malignant_papillary_carcinoma'\n",
    "]\n",
    "\n",
    "\n",
    "# This function maps the filename path to the correct class label (0-7)\n",
    "def get_label_from_filename(filename):\n",
    "    \"\"\"Extract label from BreakHis filename path.\"\"\"\n",
    "    filename = filename.replace('\\\\', '/').lower()\n",
    "    \n",
    "    # Map folder names to labels\n",
    "    if '/adenosis/' in filename: return 0\n",
    "    if '/fibroadenoma/' in filename: return 1\n",
    "    if '/phyllodes_tumor/' in filename: return 2\n",
    "    if '/tubular_adenoma/' in filename: return 3\n",
    "    if '/ductal_carcinoma/' in filename: return 4\n",
    "    if '/lobular_carcinoma/' in filename: return 5\n",
    "    if '/mucinous_carcinoma/' in filename: return 6\n",
    "    if '/papillary_carcinoma/' in filename: return 7\n",
    "    \n",
    "    return -1\n",
    "\n",
    "# Create dictionaries with full paths and labels\n",
    "train_data_dict = {\n",
    "    'image': [os.path.join(BASE_PATH, f) for f in train_filenames],\n",
    "    'label': [get_label_from_filename(f) for f in train_filenames]\n",
    "}\n",
    "\n",
    "test_data_dict = {\n",
    "    'image': [os.path.join(BASE_PATH, f) for f in test_filenames],\n",
    "    'label': [get_label_from_filename(f) for f in test_filenames]\n",
    "}\n",
    "\n",
    "# Define the dataset features, just like your original dataset\n",
    "features = Features({\n",
    "    'image': HFImage(),\n",
    "    'label': ClassLabel(names=CLASS_NAMES)\n",
    "})\n",
    "\n",
    "# Create the Dataset objects\n",
    "# The .cast_column tells the dataset to load the image from the path\n",
    "train_dataset = Dataset.from_dict(train_data_dict, features=features).cast_column(\"image\", HFImage())\n",
    "eval_dataset = Dataset.from_dict(test_data_dict, features=features).cast_column(\"image\", HFImage())\n",
    "\n",
    "print(\"\\n--- Created new Hugging Face Datasets ---\")\n",
    "print(train_dataset)\n",
    "print(eval_dataset)\n",
    "\n",
    "# ============================================================================\n",
    "# 2.1 APPLY  FORMATTING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "# Define the instruction prompt from cell [17]\n",
    "PROMPT = \"\"\"Analyze this breast tissue histopathology image and classify it.\n",
    "\n",
    "Classes (0-7):\n",
    "0: benign_adenosis\n",
    "1: benign_fibroadenoma\n",
    "2: benign_phyllodes_tumor\n",
    "3: benign_tubular_adenoma\n",
    "4: malignant_ductal_carcinoma\n",
    "5: malignant_lobular_carcinoma\n",
    "6: malignant_mucinous_carcinoma\n",
    "7: malignant_papillary_carcinoma\n",
    "\n",
    "Answer with only the number (0-7):\"\"\"\n",
    "\n",
    "def format_data(example):\n",
    "    \"\"\"Format dataset examples into chat-style messages for training.\"\"\"\n",
    "    \n",
    "    \n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},  # Image placeholder\n",
    "                {\"type\": \"text\", \"text\": PROMPT},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": str(example[\"label\"])},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "# Apply formatting\n",
    "# These are your new, correctly-split, formatted datasets\n",
    "formatted_train = train_dataset.map(format_data, batched=False)\n",
    "formatted_eval = eval_dataset.map(format_data, batched=False)\n",
    "\n",
    "print(\"\\n--- Applied formatting ---\")\n",
    "print(formatted_train)\n",
    "print(formatted_eval)\n",
    "\n",
    "# Check the first example\n",
    "print(\"\\n--- Example from new formatted_train ---\")\n",
    "print(formatted_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c19cebd3-799d-4e2d-979e-28b421c72ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: Loading MedGemma Model\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c59465455749229abc6160f30286bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded: google/medgemma-4b-it\n",
      "✓ Using dtype: bfloat16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 3. LOAD MODEL AND PROCESSOR\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Loading MedGemma Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_ID = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Model configuration\n",
    "# WHY BFLOAT16:\n",
    "# - More numerically stable than float16 (avoids NaN issues)\n",
    "# - Same memory footprint as float16\n",
    "# - Better for vision-language models\n",
    "# WHY device_map=\"auto\": Automatically distributes model across available GPUs\n",
    "model_kwargs = dict(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# # Override image size to something smaller\n",
    "# processor.image_processor.size = {\"height\": 448, \"width\": 448}  # Half size\n",
    "\n",
    "# Configure tokenizer for training\n",
    "# WHY right padding: Prevents issues with batched generation during training\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"✓ Model loaded: {MODEL_ID}\")\n",
    "print(f\"✓ Using dtype: bfloat16\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a90c7e8-3d55-41d2-b379-8a304117f587",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: Evaluating Baseline Model\n",
      "================================================================================\n",
      "Running baseline evaluation...\n",
      "\n",
      "--------------------------------BASELINE RESULTS--------------------------------\n",
      "Accuracy (8-class):   32.6%\n",
      "F1 Score (8-class):   0.241\n",
      "Accuracy (Binary):    59.6%\n",
      "F1 Score (Binary):    0.639\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 4. EVALUATE BASELINE MODEL (BEFORE FINE-TUNING)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: Evaluating Baseline Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Setup evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "# def compute_metrics(predictions, references):\n",
    "#     \"\"\"Compute accuracy and weighted F1 score\"\"\"\n",
    "#     return {\n",
    "#         **accuracy_metric.compute(predictions=predictions, references=references),\n",
    "#         **f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\n",
    "#     }\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    \"\"\"Compute 8-class and binary (benign/malignant) metrics.\"\"\"\n",
    "    \n",
    "    # --- 1. 8-class (multi-class) metrics (Original) ---\n",
    "    multi_class_acc = accuracy_metric.compute(predictions=predictions, references=references)\n",
    "    multi_class_f1 = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\n",
    "    \n",
    "    # --- 2. Binary (benign/malignant) metrics (New) ---\n",
    "    \n",
    "    # Convert lists to binary: \n",
    "    # Labels 0-3 are 'benign' (0)\n",
    "    # Labels 4-7 are 'malignant' (1)\n",
    "    # Parsed -1 (error) remains -1 so it's counted as wrong\n",
    "    binary_preds = [1 if p > 3 else 0 if p >= 0 else -1 for p in predictions]\n",
    "    binary_refs = [1 if r > 3 else 0 for r in references] # True labels are always 0-7\n",
    "\n",
    "    binary_acc = accuracy_metric.compute(predictions=binary_preds, references=binary_refs)\n",
    "    \n",
    "    # Use average='binary' for F1. This calculates F1 for the positive class (1, malignant)\n",
    "    binary_f1 = f1_metric.compute(predictions=binary_preds, references=binary_refs, average=\"binary\") \n",
    "\n",
    "    # --- 3. Return all metrics ---\n",
    "    return {\n",
    "        \"accuracy_8class\": multi_class_acc['accuracy'],\n",
    "        \"f1_8class_weighted\": multi_class_f1['f1'],\n",
    "        \"accuracy_binary\": binary_acc['accuracy'],\n",
    "        \"f1_binary_malignant\": binary_f1['f1']\n",
    "    }\n",
    "def postprocess_prediction(text):\n",
    "    \"\"\"\n",
    "    Extract predicted class number from model output.\n",
    "    \n",
    "    WHY THIS PARSING:\n",
    "    - Model may output \"Classification: 5\" or just \"5\"\n",
    "    - We use regex to find any digit 0-7 in the response\n",
    "    - Returns -1 if no valid digit found (counts as wrong prediction)\n",
    "    \"\"\"\n",
    "    digit_match = re.search(r'\\b([0-7])\\b', text.strip())\n",
    "    return int(digit_match.group(1)) if digit_match else -1\n",
    "\n",
    "def batch_predict(model, processor, prompts, images, batch_size=8, max_new_tokens=40):\n",
    "    \"\"\"\n",
    "    Run batch inference on the model.\n",
    "    \n",
    "    WHY BATCH_SIZE=8:\n",
    "    - Balance between speed and memory usage with bfloat16\n",
    "    - Can be increased if more VRAM available\n",
    "    \n",
    "    WHY max_new_tokens=40:\n",
    "    - We only need 1-2 tokens for the answer\n",
    "    - 40 gives buffer for any extra text model might generate\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_texts = prompts[i:i + batch_size]\n",
    "        batch_images = [[img] for img in images[i:i + batch_size]]\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = processor(\n",
    "            text=batch_texts,\n",
    "            images=batch_images,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\", torch.bfloat16)\n",
    "        \n",
    "        # Track prompt lengths to extract only generated text\n",
    "        prompt_lengths = inputs[\"attention_mask\"].sum(dim=1)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # Greedy decoding for deterministic results\n",
    "                pad_token_id=processor.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated part (not the prompt)\n",
    "        for seq, length in zip(outputs, prompt_lengths):\n",
    "            generated = processor.decode(seq[length:], skip_special_tokens=True)\n",
    "            predictions.append(postprocess_prediction(generated))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Prepare evaluation data\n",
    "eval_prompts = [\n",
    "    processor.apply_chat_template(\n",
    "        [msg[0]],  # Only user message, not assistant response\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "    for msg in formatted_eval[\"messages\"]\n",
    "]\n",
    "eval_images = formatted_eval[\"image\"]\n",
    "eval_labels = formatted_eval[\"label\"]\n",
    "\n",
    "# Run baseline evaluation\n",
    "print(\"Running baseline evaluation...\")\n",
    "baseline_preds = batch_predict(model, processor, eval_prompts, eval_images)\n",
    "baseline_metrics = compute_metrics(baseline_preds, eval_labels)\n",
    "\n",
    "# print(f\"\\n{'BASELINE RESULTS':-^80}\")\n",
    "# print(f\"Accuracy: {baseline_metrics['accuracy']:.1%}\")\n",
    "# print(f\"F1 Score: {baseline_metrics['f1']:.3f}\")\n",
    "# print(\"-\"*80)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n{'BASELINE RESULTS':-^80}\")\n",
    "print(f\"Accuracy (8-class):   {baseline_metrics['accuracy_8class']:.1%}\")\n",
    "print(f\"F1 Score (8-class):   {baseline_metrics['f1_8class_weighted']:.3f}\")\n",
    "print(f\"Accuracy (Binary):    {baseline_metrics['accuracy_binary']:.1%}\")\n",
    "print(f\"F1 Score (Binary):    {baseline_metrics['f1_binary_malignant']:.3f}\")\n",
    "print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f03336a-544b-4322-9be9-5993652e9600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: Fine-tuning with LoRA\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 5. CONFIGURE AND RUN FINE-TUNING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: Fine-tuning with LoRA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# LoRA Configuration\n",
    "# WHY LORA:\n",
    "# - Trains only a small fraction of parameters (~1% of model)\n",
    "# - Much faster and memory-efficient than full fine-tuning\n",
    "# - Often achieves comparable performance\n",
    "#\n",
    "# PARAMETER EXPLANATIONS:\n",
    "# - r=8: Rank of LoRA matrices (lower = fewer params, faster, less capacity)\n",
    "#   - Too low (r=2): May underfit, can't learn complex patterns\n",
    "#   - Too high (r=64): More params, slower, risk overfitting on small datasets\n",
    "#   - r=8 is good balance for 500 training samples\n",
    "#\n",
    "# - lora_alpha=16: Scaling factor for LoRA weights\n",
    "#   - Typically set to 2*r as a rule of thumb\n",
    "#   - Controls how much LoRA adapters affect base model\n",
    "#\n",
    "# - lora_dropout=0.1: Regularization to prevent overfitting\n",
    "#   - Higher values (0.2) = more regularization but may underfit\n",
    "#   - Lower values (0.05) = less regularization but may overfit\n",
    "#\n",
    "# - target_modules=\"all-linear\": Apply LoRA to all linear layers\n",
    "#   - Alternative: Specify specific layers like [\"q_proj\", \"v_proj\"]\n",
    "#   - \"all-linear\" is simpler and works well for most cases\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Custom data collator for vision-language training\n",
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Prepare batches for training with images and text.\n",
    "    \n",
    "    WHY CUSTOM COLLATOR:\n",
    "    - Need to handle both image and text inputs\n",
    "    - Must mask padding tokens and image tokens in loss computation\n",
    "    - MedGemma has special image token handling requirements\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        images.append([example[\"image\"]])\n",
    "        texts.append(\n",
    "            processor.apply_chat_template(\n",
    "                example[\"messages\"],\n",
    "                add_generation_prompt=False,\n",
    "                tokenize=False\n",
    "            ).strip()\n",
    "        )\n",
    "    \n",
    "    # Tokenize and process\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Create labels (same as input_ids but with masking)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Mask padding tokens (model shouldn't learn from padding)\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Mask image tokens (loss not computed on image embeddings)\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100  # Additional image-related token\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"medgemma-breastcancer-finetuned\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=5e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,  # Warm up LR for first 3% of training\n",
    "    max_grad_norm=0.3,  # Clip gradients to prevent instability\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"], \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6da7b1c-3a59-4da7-944f-731441687efb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Total training steps: ~478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tsave_steps: 100 (from args) != 500 (from trainer_state.json)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/480 : < :, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved to medgemma-breastcancer-finetuned\n",
      "Model training duration:  0.026576538550095088  minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_train,\n",
    "    eval_dataset=formatted_eval,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total training steps: ~{(len(formatted_train) * 5) // 8}\")\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "trainer.train()\n",
    "# If you would like to continue training that stopped from some reason - \n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model()\n",
    "print(f\"✓ Model saved to {training_args.output_dir}\")\n",
    "\n",
    "print(\"Model training duration: \", (end_time - start_time)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "739e421d-4832-4870-85f8-22d3c602aebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: Evaluating Fine-tuned Model\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302f49055f484153aa33f8b9bc2226e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fine-tuned model loaded\n",
      "Running fine-tuned evaluation...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 6. EVALUATE FINE-TUNED MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: Evaluating Fine-tuned Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Clear memory and load fine-tuned model\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapters and merge them\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, training_args.output_dir)\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "\n",
    "# Load processor from fine-tuned checkpoint\n",
    "processor_finetuned = AutoProcessor.from_pretrained(training_args.output_dir)\n",
    "\n",
    "# Configure for generation\n",
    "finetuned_model.generation_config.max_new_tokens = 50\n",
    "finetuned_model.generation_config.pad_token_id = processor_finetuned.tokenizer.pad_token_id\n",
    "finetuned_model.config.pad_token_id = processor_finetuned.tokenizer.pad_token_id\n",
    "\n",
    "print(\"✓ Fine-tuned model loaded\")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running fine-tuned evaluation...\")\n",
    "finetuned_preds = batch_predict(\n",
    "    finetuned_model,\n",
    "    processor_finetuned,\n",
    "    eval_prompts,\n",
    "    eval_images,\n",
    "    batch_size=4  # Smaller batch size for safety\n",
    ")\n",
    "finetuned_metrics = compute_metrics(finetuned_preds, eval_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "224d4053-f25a-4fcf-a2d4-35d2405d9e11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "--- 8-Class Classification (0-7) ---\n",
      "Model                Accuracy     F1 (Weighted)  \n",
      "-----------------------------------------------\n",
      "Baseline                  32.6%         0.241\n",
      "Fine-tuned                87.2%         0.865\n",
      "-----------------------------------------------\n",
      "\n",
      "--- Binary (Benign/Malignant) Classification ---\n",
      "Model                Accuracy     F1 (Malignant) \n",
      "-----------------------------------------------\n",
      "Baseline                  59.6%         0.639\n",
      "Fine-tuned                99.0%         0.991\n",
      "-----------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "\n",
      "✓ Fine-tuning successful! 8-class accuracy improved.\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. COMPARE RESULTS (with binary)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n--- 8-Class Classification (0-7) ---\")\n",
    "print(f\"{'Model':<20} {'Accuracy':<12} {'F1 (Weighted)':<15}\")\n",
    "print(\"-\" * 47)\n",
    "print(f\"{'Baseline':<20} {baseline_metrics['accuracy_8class']:>10.1%} {baseline_metrics['f1_8class_weighted']:>13.3f}\")\n",
    "print(f\"{'Fine-tuned':<20} {finetuned_metrics['accuracy_8class']:>10.1%} {finetuned_metrics['f1_8class_weighted']:>13.3f}\")\n",
    "print(\"-\" * 47)\n",
    "\n",
    "print(f\"\\n--- Binary (Benign/Malignant) Classification ---\")\n",
    "print(f\"{'Model':<20} {'Accuracy':<12} {'F1 (Malignant)':<15}\")\n",
    "print(\"-\" * 47)\n",
    "print(f\"{'Baseline':<20} {baseline_metrics['accuracy_binary']:>10.1%} {baseline_metrics['f1_binary_malignant']:>13.3f}\")\n",
    "print(f\"{'Fine-tuned':<20} {finetuned_metrics['accuracy_binary']:>10.1%} {finetuned_metrics['f1_binary_malignant']:>13.3f}\")\n",
    "print(\"-\" * 47)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "\n",
    "# Success indicators (checking 8-class accuracy)\n",
    "if finetuned_metrics['accuracy_8class'] > baseline_metrics['accuracy_8class']:\n",
    "    print(\"\\n✓ Fine-tuning successful! 8-class accuracy improved.\")\n",
    "else:\n",
    "    print(\"\\n⚠ Fine-tuning did not improve 8-class accuracy. Consider:\")\n",
    "    print(\"  - Training for more epochs\")\n",
    "    print(\"  - Using more training data\")\n",
    "    print(\"  - Adjusting learning rate or LoRA rank\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
