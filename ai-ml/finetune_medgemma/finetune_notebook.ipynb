{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942914b4-b30c-4b2a-ab33-059dcf8e8a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-tuning MedGemma-4B for Breast Cancer Histopathology Classification\n",
    "# This notebook demonstrates how to fine-tune Google's MedGemma vision-language model\n",
    "# on the BreakHis breast cancer dataset using LoRA (Low-Rank Adaptation)\n",
    "\n",
    "# ============================================================================\n",
    "# 0. SETUP AND INSTALLATIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages\n",
    "!pip install --upgrade --quiet transformers bitsandbytes datasets evaluate peft trl scikit-learn \n",
    "# Then, reinstall it, forcing a build from source\n",
    "# This will take a few minutes as it compiles the code\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import evaluate\n",
    "\n",
    "# Hugging Face authentication\n",
    "from huggingface_hub import login\n",
    "HF_TOKEN = 'YOUR_HF_TOKEN'\n",
    "login(HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17ef521a-da21-4751-bb71-9d555abbc5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: Loading and Preparing Dataset\n",
      "================================================================================\n",
      "Training samples: 500\n",
      "Evaluation samples: 1000\n",
      "\n",
      "Classes: ['benign_adenosis', 'benign_fibroadenoma', 'benign_phyllodes_tumor', 'benign_tubular_adenoma', 'malignant_ductal_carcinoma', 'malignant_lobular_carcinoma', 'malignant_mucinous_carcinoma', 'malignant_papillary_carcinoma']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: Loading and Preparing Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"sarath2003/BreakHis\"\n",
    "TRAIN_SIZE = 500  # Number of training samples\n",
    "EVAL_SIZE = 1000   # Number of evaluation samples\n",
    "\n",
    "# Load dataset\n",
    "# The BreakHis dataset contains histopathological images of breast tumors\n",
    "# at 200X magnification with 8 different classes (4 benign, 4 malignant)\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\").shuffle(seed=42)\n",
    "train_data = dataset.select(range(TRAIN_SIZE))\n",
    "eval_data = dataset.select(range(TRAIN_SIZE, TRAIN_SIZE + EVAL_SIZE))\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Evaluation samples: {len(eval_data)}\")\n",
    "\n",
    "# Extract class names from the dataset\n",
    "CANCER_CLASSES = train_data.features[\"label\"].names\n",
    "print(f\"\\nClasses: {CANCER_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c1ffc96-645c-4a0e-8a9d-34e822e8a12b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: Formatting Data with Prompts\n",
      "================================================================================\n",
      "✓ Data formatted with instruction prompts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 2. CREATE PROMPT AND FORMAT DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Formatting Data with Prompts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define the instruction prompt\n",
    "# WHY THIS PROMPT:\n",
    "# - Clear, concise task description\n",
    "# - 0-7 numbering matches actual label indices (critical for training)\n",
    "# - Asks for number only to simplify output parsing\n",
    "PROMPT = \"\"\"Analyze this breast tissue histopathology image and classify it.\n",
    "\n",
    "Classes (0-7):\n",
    "0: benign_adenosis\n",
    "1: benign_fibroadenoma\n",
    "2: benign_phyllodes_tumor\n",
    "3: benign_tubular_adenoma\n",
    "4: malignant_ductal_carcinoma\n",
    "5: malignant_lobular_carcinoma\n",
    "6: malignant_mucinous_carcinoma\n",
    "7: malignant_papillary_carcinoma\n",
    "\n",
    "Answer with only the number (0-7):\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def format_data(example):\n",
    "    \"\"\"\n",
    "    Format dataset examples into chat-style messages for training.\n",
    "    \n",
    "    WHY THIS FORMAT:\n",
    "    - MedGemma expects chat-based input with user/assistant roles\n",
    "    - Image is placed before text as per model's expected input order\n",
    "    - Assistant response is just the label number for simplicity\n",
    "    \"\"\"\n",
    "    example[\"messages\"] = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},  # Image placeholder\n",
    "                {\"type\": \"text\", \"text\": PROMPT},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": str(example[\"label\"])},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply formatting\n",
    "formatted_train = train_data.map(format_data)\n",
    "formatted_eval = eval_data.map(format_data)\n",
    "\n",
    "print(\"✓ Data formatted with instruction prompts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c19cebd3-799d-4e2d-979e-28b421c72ede",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: Loading MedGemma Model\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e658c6a20c614b81ae8f3561b3ec18a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded: google/medgemma-4b-it\n",
      "✓ Using dtype: bfloat16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 3. LOAD MODEL AND PROCESSOR\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: Loading MedGemma Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_ID = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Model configuration\n",
    "# WHY BFLOAT16:\n",
    "# - More numerically stable than float16 (avoids NaN issues)\n",
    "# - Same memory footprint as float16\n",
    "# - Better for vision-language models\n",
    "# WHY device_map=\"auto\": Automatically distributes model across available GPUs\n",
    "model_kwargs = dict(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(MODEL_ID, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "\n",
    "# Configure tokenizer for training\n",
    "# WHY right padding: Prevents issues with batched generation during training\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"✓ Model loaded: {MODEL_ID}\")\n",
    "print(f\"✓ Using dtype: bfloat16\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a90c7e8-3d55-41d2-b379-8a304117f587",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: Evaluating Baseline Model\n",
      "================================================================================\n",
      "Running baseline evaluation...\n",
      "\n",
      "--------------------------------BASELINE RESULTS--------------------------------\n",
      "Accuracy: 37.5%\n",
      "F1 Score: 0.283\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 4. EVALUATE BASELINE MODEL (BEFORE FINE-TUNING)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: Evaluating Baseline Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Setup evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    \"\"\"Compute accuracy and weighted F1 score\"\"\"\n",
    "    return {\n",
    "        **accuracy_metric.compute(predictions=predictions, references=references),\n",
    "        **f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "def postprocess_prediction(text):\n",
    "    \"\"\"\n",
    "    Extract predicted class number from model output.\n",
    "    \n",
    "    WHY THIS PARSING:\n",
    "    - Model may output \"Classification: 5\" or just \"5\"\n",
    "    - We use regex to find any digit 0-7 in the response\n",
    "    - Returns -1 if no valid digit found (counts as wrong prediction)\n",
    "    \"\"\"\n",
    "    digit_match = re.search(r'\\b([0-7])\\b', text.strip())\n",
    "    return int(digit_match.group(1)) if digit_match else -1\n",
    "\n",
    "def batch_predict(model, processor, prompts, images, batch_size=8, max_new_tokens=40):\n",
    "    \"\"\"\n",
    "    Run batch inference on the model.\n",
    "    \n",
    "    WHY BATCH_SIZE=8:\n",
    "    - Balance between speed and memory usage with bfloat16\n",
    "    - Can be increased if more VRAM available\n",
    "    \n",
    "    WHY max_new_tokens=40:\n",
    "    - We only need 1-2 tokens for the answer\n",
    "    - 40 gives buffer for any extra text model might generate\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch_texts = prompts[i:i + batch_size]\n",
    "        batch_images = [[img] for img in images[i:i + batch_size]]\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = processor(\n",
    "            text=batch_texts,\n",
    "            images=batch_images,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\", torch.bfloat16)\n",
    "        \n",
    "        # Track prompt lengths to extract only generated text\n",
    "        prompt_lengths = inputs[\"attention_mask\"].sum(dim=1)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # Greedy decoding for deterministic results\n",
    "                pad_token_id=processor.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode only the generated part (not the prompt)\n",
    "        for seq, length in zip(outputs, prompt_lengths):\n",
    "            generated = processor.decode(seq[length:], skip_special_tokens=True)\n",
    "            predictions.append(postprocess_prediction(generated))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Prepare evaluation data\n",
    "eval_prompts = [\n",
    "    processor.apply_chat_template(\n",
    "        [msg[0]],  # Only user message, not assistant response\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "    for msg in formatted_eval[\"messages\"]\n",
    "]\n",
    "eval_images = formatted_eval[\"image\"]\n",
    "eval_labels = formatted_eval[\"label\"]\n",
    "\n",
    "# Run baseline evaluation\n",
    "print(\"Running baseline evaluation...\")\n",
    "baseline_preds = batch_predict(model, processor, eval_prompts, eval_images)\n",
    "baseline_metrics = compute_metrics(baseline_preds, eval_labels)\n",
    "\n",
    "print(f\"\\n{'BASELINE RESULTS':-^80}\")\n",
    "print(f\"Accuracy: {baseline_metrics['accuracy']:.1%}\")\n",
    "print(f\"F1 Score: {baseline_metrics['f1']:.3f}\")\n",
    "print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f03336a-544b-4322-9be9-5993652e9600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: Fine-tuning with LoRA\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 5. CONFIGURE AND RUN FINE-TUNING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: Fine-tuning with LoRA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# LoRA Configuration\n",
    "# WHY LORA:\n",
    "# - Trains only a small fraction of parameters (~1% of model)\n",
    "# - Much faster and memory-efficient than full fine-tuning\n",
    "# - Often achieves comparable performance\n",
    "#\n",
    "# PARAMETER EXPLANATIONS:\n",
    "# - r=8: Rank of LoRA matrices (lower = fewer params, faster, less capacity)\n",
    "#   - Too low (r=2): May underfit, can't learn complex patterns\n",
    "#   - Too high (r=64): More params, slower, risk overfitting on small datasets\n",
    "#   - r=8 is good balance for 500 training samples\n",
    "#\n",
    "# - lora_alpha=16: Scaling factor for LoRA weights\n",
    "#   - Typically set to 2*r as a rule of thumb\n",
    "#   - Controls how much LoRA adapters affect base model\n",
    "#\n",
    "# - lora_dropout=0.1: Regularization to prevent overfitting\n",
    "#   - Higher values (0.2) = more regularization but may underfit\n",
    "#   - Lower values (0.05) = less regularization but may overfit\n",
    "#\n",
    "# - target_modules=\"all-linear\": Apply LoRA to all linear layers\n",
    "#   - Alternative: Specify specific layers like [\"q_proj\", \"v_proj\"]\n",
    "#   - \"all-linear\" is simpler and works well for most cases\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Custom data collator for vision-language training\n",
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Prepare batches for training with images and text.\n",
    "    \n",
    "    WHY CUSTOM COLLATOR:\n",
    "    - Need to handle both image and text inputs\n",
    "    - Must mask padding tokens and image tokens in loss computation\n",
    "    - MedGemma has special image token handling requirements\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    for example in examples:\n",
    "        images.append([example[\"image\"]])\n",
    "        texts.append(\n",
    "            processor.apply_chat_template(\n",
    "                example[\"messages\"],\n",
    "                add_generation_prompt=False,\n",
    "                tokenize=False\n",
    "            ).strip()\n",
    "        )\n",
    "    \n",
    "    # Tokenize and process\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Create labels (same as input_ids but with masking)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Mask padding tokens (model shouldn't learn from padding)\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Mask image tokens (loss not computed on image embeddings)\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(\n",
    "        processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "    )\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100  # Additional image-related token\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"medgemma-breastcancer-finetuned\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=5e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,  # Warm up LR for first 3% of training\n",
    "    max_grad_norm=0.3,  # Clip gradients to prevent instability\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"], \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da7b1c-3a59-4da7-944f-731441687efb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Total training steps: ~312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 1:21:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.028996</td>\n",
       "      <td>0.074295</td>\n",
       "      <td>187500.000000</td>\n",
       "      <td>0.986325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.026479</td>\n",
       "      <td>0.048433</td>\n",
       "      <td>375000.000000</td>\n",
       "      <td>0.987034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.024792</td>\n",
       "      <td>0.070041</td>\n",
       "      <td>562500.000000</td>\n",
       "      <td>0.987188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.022377</td>\n",
       "      <td>0.181104</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>0.987573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.164053</td>\n",
       "      <td>937500.000000</td>\n",
       "      <td>0.987838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved to medgemma-breastcancer-finetuned\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_train,\n",
    "    eval_dataset=formatted_eval,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(f\"Total training steps: ~{(TRAIN_SIZE * 5) // 8}\")\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "trainer.train()\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model()\n",
    "print(f\"✓ Model saved to {training_args.output_dir}\")\n",
    "\n",
    "print(\"Model training duration: \", (end_time - start_time)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e421d-4832-4870-85f8-22d3c602aebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: Evaluating Fine-tuned Model\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0329b48d30a4520ac169479712f57bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fine-tuned model loaded\n",
      "Running fine-tuned evaluation...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 6. EVALUATE FINE-TUNED MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: Evaluating Fine-tuned Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Clear memory and load fine-tuned model\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapters and merge them\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, training_args.output_dir)\n",
    "finetuned_model = finetuned_model.merge_and_unload()\n",
    "\n",
    "# Load processor from fine-tuned checkpoint\n",
    "processor_finetuned = AutoProcessor.from_pretrained(training_args.output_dir)\n",
    "\n",
    "# Configure for generation\n",
    "finetuned_model.generation_config.max_new_tokens = 50\n",
    "finetuned_model.generation_config.pad_token_id = processor_finetuned.tokenizer.pad_token_id\n",
    "finetuned_model.config.pad_token_id = processor_finetuned.tokenizer.pad_token_id\n",
    "\n",
    "print(\"✓ Fine-tuned model loaded\")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running fine-tuned evaluation...\")\n",
    "finetuned_preds = batch_predict(\n",
    "    finetuned_model,\n",
    "    processor_finetuned,\n",
    "    eval_prompts,\n",
    "    eval_images,\n",
    "    batch_size=4  # Smaller batch size for safety\n",
    ")\n",
    "finetuned_metrics = compute_metrics(finetuned_preds, eval_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75abb4-5aa7-4334-8c82-3806cde94803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Model                Accuracy     F1 Score    \n",
      "--------------------------------------------\n",
      "Baseline                  37.5%       0.283\n",
      "Fine-tuned                57.6%       0.511\n",
      "--------------------------------------------\n",
      "\n",
      "Improvement              +20.1%      +0.228\n",
      "================================================================================\n",
      "\n",
      "✓ Fine-tuning successful! Accuracy improved.\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 7. COMPARE RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Model':<20} {'Accuracy':<12} {'F1 Score':<12}\")\n",
    "print(\"-\" * 44)\n",
    "print(f\"{'Baseline':<20} {baseline_metrics['accuracy']:>10.1%}  {baseline_metrics['f1']:>10.3f}\")\n",
    "print(f\"{'Fine-tuned':<20} {finetuned_metrics['accuracy']:>10.1%}  {finetuned_metrics['f1']:>10.3f}\")\n",
    "print(\"-\" * 44)\n",
    "\n",
    "# Calculate improvement\n",
    "accuracy_improvement = (finetuned_metrics['accuracy'] - baseline_metrics['accuracy']) * 100\n",
    "f1_improvement = finetuned_metrics['f1'] - baseline_metrics['f1']\n",
    "\n",
    "print(f\"\\n{'Improvement':<20} {accuracy_improvement:>+9.1f}%  {f1_improvement:>+10.3f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Success indicators\n",
    "if finetuned_metrics['accuracy'] > baseline_metrics['accuracy']:\n",
    "    print(\"\\n✓ Fine-tuning successful! Accuracy improved.\")\n",
    "else:\n",
    "    print(\"\\n⚠ Fine-tuning did not improve accuracy. Consider:\")\n",
    "    print(\"  - Training for more epochs\")\n",
    "    print(\"  - Using more training data\")\n",
    "    print(\"  - Adjusting learning rate or LoRA rank\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85278d-b9f2-48f4-9901-e227ec3e3dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m134",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m134"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
