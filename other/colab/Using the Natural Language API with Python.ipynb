{"cells":[{"cell_type":"markdown","metadata":{"id":"7vEBNTHjuvon"},"source":["```text\n","SPDX-FileCopyrightText: 2023 Google LLC\n","SPDX-License-Identifier: Apache-2.0\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"yxKFLozC5tfO"},"source":["# ü§Ø Using the Natural Language API with Python\n","\n","<center>\n","<table><tr><td>\n","<img src=\"pics/natural_language_api.png\" style=\"height:200px\" height=\"200\" />\n","</td></tr></table>\n","<table><tr>\n","<td><a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/devrel-demos/blob/main/other/colab/Using%20the%20Natural%20Language%20API%20with%20Python.ipynb\">\n","<img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\" align=\"center\"> Open in Colab\n","</a></td>\n","<td><a href=\"https://github.com/GoogleCloudPlatform/devrel-demos/blob/main/other/colab/Using%20the%20Natural%20Language%20API%20with%20Python.ipynb\">\n","<img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\" align=\"center\"> View on GitHub\n","</a></td>\n","</tr></table>\n","</center>\n","\n","The [Natural Language API](https://cloud.google.com/natural-language/docs/) lets you extract information from unstructured text using Google machine learning. In this tutorial, you'll focus on using its Python client library to perform the following:\n","\n","- Sentiment analysis\n","- Entity analysis\n","- Syntax analysis\n","- Content classification\n","- Text moderation (powered by [PaLM 2](https://blog.google/technology/ai/google-palm-2-ai-large-language-model))\n","\n","This notebook requires a Google Cloud project:\n","\n","- If needed, [create a new Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n","- Make sure that billing is enabled for your project.\n","- It uses billable services but not should generate any cost (see the Natural Language API [free monthly thresholds](https://cloud.google.com/natural-language/pricing)).\n","\n","It can run in autopilot mode:\n","\n","- Launch \"Run all\".\n","- If the setup installs packages and restarts the kernel, launch \"Run all\" again.\n","- If requested, allow this notebook to access your Google credentials by signing in with your Google Cloud account.\n","- If requested, select your Google Cloud project and continue (\"Run after\").\n","\n","> This port to a notebook was originally published on [Google Developers Codelabs](https://codelabs.developers.google.com/codelabs/cloud-natural-language-python3).\n"]},{"cell_type":"markdown","metadata":{"id":"JAUW8f9oi8mi"},"source":["---\n","\n","## ‚úîÔ∏è Setup\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mn2UJ8_VUtFa"},"outputs":[],"source":["# @title üì¶Ô∏è Packages (may restart) {display-mode: \"form\"}\n","\n","# Dependencies (using Colab defaults for minimum versions)\n","# Assumption: later versions are backward compatible\n","PYTHON_MIN_VERSION = 3, 10\n","NOTEBOOK_PACKAGES = [\n","    (\"pandas\", \"1.5.3\"),\n","    (\"Jinja2\", \"3.1.2\"),\n","    (\"ipywidgets\", \"7.7.1\"),\n","]\n","NOTEBOOK_DEPENDENCIES = [\n","    (package, min_version_str, package.lower())\n","    for package, min_version_str in NOTEBOOK_PACKAGES\n","]\n","# Google Cloud APIs needed for this notebook\n","# - {API}.googleapis.com is the service name\n","# - google-cloud-{API} is its Python client library\n","GOOGLE_CLOUD_APIS = [\n","    (\"language\", \"2.11.0\"),\n","]\n","GOOGLE_CLOUD_SERVICES = [\n","    f\"{API}.googleapis.com\" for API, _ in GOOGLE_CLOUD_APIS\n","]\n","NOTEBOOK_DEPENDENCIES += [\n","    (f\"google-cloud-{API}\", min_version_str, f\"google.cloud.{API}\")\n","    for API, min_version_str in GOOGLE_CLOUD_APIS\n","]\n","\n","import importlib.metadata\n","import sys\n","\n","import packaging.version\n","from IPython.core.getipython import get_ipython\n","\n","running_in_colab = \"google.colab\" in sys.modules\n","if running_in_colab:\n","    from google.colab import auth as colab_auth\n","restart_after_installation = running_in_colab\n","\n","\n","class StopExecution(Exception):\n","    def _render_traceback_(self):  # Suppress traceback in notebook\n","        return [self.args[0]] if self.args else None\n","\n","\n","def check_python_version():\n","    version = \".\".join(map(str, sys.version_info[0:3]))\n","    if PYTHON_MIN_VERSION <= sys.version_info:\n","        print(f\"‚úîÔ∏è Python: {version}\")\n","        return\n","    required = \".\".join(map(str, PYTHON_MIN_VERSION))\n","    raise StopExecution(f\"‚ùå Python: {version} (version {required}+ is required)\")\n","\n","\n","def check_packages():\n","    needed_packages = [\n","        package\n","        for package, min_version_str, lib in NOTEBOOK_DEPENDENCIES\n","        if need_package(package, min_version_str, lib)\n","    ]\n","    if not needed_packages:\n","        return\n","    requirements = \" \".join(needed_packages)\n","    %pip install --upgrade $requirements --quiet\n","    if restart_after_installation:\n","        restart_kernel()\n","\n","\n","def need_package(package: str, min_version_str: str, lib: str) -> bool:\n","    min_version = packaging.version.parse(min_version_str)\n","    try:\n","        lib_version = packaging.version.parse(importlib.metadata.version(lib))\n","        if min_version <= lib_version:\n","            print(f\"‚úîÔ∏è {package}=={lib_version!s}\")\n","            return False\n","        print(f\"üì¶Ô∏è {package} needs to be updated‚Ä¶\")\n","    except importlib.metadata.PackageNotFoundError:\n","        print(f\"üì¶Ô∏è {package} needs to be installed‚Ä¶\")\n","    return True\n","\n","\n","def restart_kernel():\n","    if ipython := get_ipython():\n","        ipython.kernel.do_shutdown(True)\n","    raise StopExecution(\"‚ùå Restarting the kernel‚Ä¶ Please run again\")\n","\n","\n","def gcloud(command: str) -> list[str]:\n","    command = command.replace(\"'\", '\"')\n","    lines = !gcloud $command\n","    if any(\"ERROR:\" in line for line in lines):\n","        raise StopExecution(lines.nlstr)\n","    return lines.list\n","\n","\n","check_python_version()\n","check_packages()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Xt81s9pOlfI"},"outputs":[],"source":["# @title üîë Authentication {display-mode: \"form\"}\n","\n","\n","def get_active_account() -> str:\n","    lines = gcloud(\"config list --format 'value(core.account)'\")\n","    return lines[0] if lines else \"\"\n","\n","\n","def is_authenticated() -> bool:\n","    return get_active_account() not in [\"\", \"default\"]\n","\n","\n","if \"PYTHON_MIN_VERSION\" not in globals():\n","    raise RuntimeWarning(\"‚ùå Please run the previous cell\")\n","\n","if not (authenticated := is_authenticated()):\n","    print(f\"üîë Authenticate to access your Google Cloud services\")\n","    if running_in_colab:\n","        colab_auth.authenticate_user()\n","    else:\n","        gcloud(\"auth login --brief\")\n","    authenticated = is_authenticated()\n","\n","if authenticated:\n","    print(f\"‚úîÔ∏è Authenticated\")\n","else:\n","    raise StopExecution(\"‚ùå Could not authenticate\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Df7bSS-b7rdW"},"outputs":[],"source":["# @title ‚öôÔ∏è Project ID {display-mode: \"form\"}\n","\n","\n","def get_default_project_id() -> str:\n","    lines = gcloud(\"config list --format 'value(core.project)'\")\n","    return lines[0] if lines else \"\"\n","\n","\n","def get_active_project_ids() -> list[str]:\n","    return gcloud(\"projects list --format 'value(projectId)'\")\n","\n","\n","def update_project_id(project_id: str | None = None):\n","    if not project_id:\n","        raise StopExecution(\"‚ùå Please select your project\")\n","    global PROJECT_ID\n","    if PROJECT_ID != project_id:\n","        PROJECT_ID = project_id\n","        if running_in_colab:\n","            colab_auth.authenticate_user(project_id=PROJECT_ID)\n","        else:\n","            print(\"‚öôÔ∏è Revoking Application Default Credentials‚Ä¶\")\n","            gcloud(\"auth application-default revoke --quiet\")\n","            print(\"‚öôÔ∏è Setting Application Default Credentials‚Ä¶\")\n","            gcloud(f\"auth application-default login --project {PROJECT_ID}\")\n","            print(\"‚öôÔ∏è Setting default project‚Ä¶\")\n","            gcloud(f\"config set project {PROJECT_ID}\")\n","    print(f\"‚úîÔ∏è PROJECT_ID: {PROJECT_ID}\")\n","\n","\n","def show_project_ids(project_ids: list[str]):\n","    import ipywidgets\n","\n","    @ipywidgets.interact(PROJECT_ID=project_ids)\n","    def on_project_id(PROJECT_ID: str | None = None):\n","        update_project_id(PROJECT_ID)\n","\n","\n","if PROJECT_ID := get_default_project_id():\n","    update_project_id(PROJECT_ID)\n","else:\n","    project_ids = get_active_project_ids()\n","    if len(project_ids) == 1:  # Single project (e.g. onboarding developer)\n","        update_project_id(project_ids[0])\n","    else:\n","        show_project_ids(project_ids)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-QXa96aXzgw"},"outputs":[],"source":["# @title üîì Project APIs {display-mode: \"form\"}\n","\n","\n","def get_enabled_services() -> list[str]:\n","    return gcloud(\"services list --enabled --format 'value(config.name)'\")\n","\n","\n","if not PROJECT_ID:\n","    raise StopExecution(\"‚ùå PROJECT_ID is undefined\")\n","\n","if not GOOGLE_CLOUD_SERVICES:\n","    print(f\"‚úîÔ∏è No specific API needed\")\n","else:\n","    enabled_services = get_enabled_services()\n","    services_to_enable = [\n","        service for service in GOOGLE_CLOUD_SERVICES if service not in enabled_services\n","    ]\n","    if services_to_enable:\n","        api_or_apis = \"APIs\" if 1 < len(services_to_enable) else \"API\"\n","        spaced_services = \" \".join(services_to_enable)\n","        print(f'üîì Enabling {api_or_apis} \"{spaced_services}\"‚Ä¶')\n","        gcloud(f\"services enable {spaced_services}\")\n","        enabled_services = get_enabled_services()\n","\n","    for service in GOOGLE_CLOUD_SERVICES:\n","        if service in enabled_services:\n","            print(f'‚úîÔ∏è API \"{service}\" is enabled')\n","        else:\n","            raise StopExecution(f'‚ùå Failed to enable API \"{service}\"')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zA4DxGDagbv"},"outputs":[],"source":["# @title üõ†Ô∏è Helper functions {display-mode: \"form\"}\n","\n","import pandas as pd\n","from IPython.display import display\n","\n","\n","def show_table(columns, data, formats=None, remove_empty_columns=False):\n","    df = pd.DataFrame(columns=columns, data=data)\n","    if remove_empty_columns:\n","        empty_cols = [col for col in df if df[col].eq(\"\").all()]\n","        df.drop(empty_cols, axis=1, inplace=True)\n","    # Customize formatting\n","    styler = df.style\n","    if formats:\n","        styler.format(formats)\n","    # Left-align string columns\n","    df = df.convert_dtypes()\n","    str_cols = list(df.select_dtypes(\"string\").keys())\n","    styler = styler.set_properties(subset=str_cols, **{\"text-align\": \"left\"})\n","    # Center headers\n","    styler.set_table_styles([{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}])\n","    styler.hide()\n","    display(styler)\n","\n","\n","print(f\"‚úîÔ∏è Helpers defined\")"]},{"cell_type":"markdown","metadata":{"id":"YuhPWlD5M1Kp"},"source":["---\n","\n","## üêç Using the Python client library\n","\n","You can use the Natural Language API in Python with the client library `google-cloud-language` and the following import:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from google.cloud import language_v1 as language\n"]},{"cell_type":"markdown","metadata":{"id":"o83udK7BZOBC"},"source":["> Note: Version `v1` is Generally Available (GA). Version `v2` is currently in Preview.\n","\n","---\n","\n","## 1Ô∏è‚É£ Sentiment analysis\n","\n","Sentiment analysis inspects the given text and identifies the prevailing emotional opinions within the text, especially to determine expressed sentiments as positive, negative, or neutral, both at the sentence and the document levels. It is performed with the `analyze_sentiment` method which returns an `AnalyzeSentimentResponse`.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ELEmIsYJZQ-i"},"outputs":[],"source":["from google.cloud import language_v1 as language\n","\n","\n","def analyze_text_sentiment(text: str) -> language.AnalyzeSentimentResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.analyze_sentiment(document=document)\n","\n","\n","def show_text_sentiment(response: language.AnalyzeSentimentResponse):\n","    columns = [\"score\", \"sentence\"]\n","    data = [(s.sentiment.score, s.text.content) for s in response.sentences]\n","    formats = {\"score\": \"{:+.1f}\"}\n","    print(\"At sentence level:\")\n","    show_table(columns, data, formats)\n","\n","    sentiment = response.document_sentiment\n","    columns = [\"score\", \"magnitude\", \"language\"]\n","    data = [(sentiment.score, sentiment.magnitude, response.language)]\n","    formats = {\"score\": \"{:+.1f}\", \"magnitude\": \"{:.1f}\"}\n","    print(\"\")\n","    print(\"At document level:\")\n","    show_table(columns, data, formats)"]},{"cell_type":"markdown","metadata":{"id":"AB2X-SKMwjLU"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"M9mzah76Y0gl"},"outputs":[{"name":"stdout","output_type":"stream","text":["At sentence level:\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_39f98 th {\n","  text-align: center;\n","}\n","#T_39f98_row0_col1, #T_39f98_row1_col1, #T_39f98_row2_col1 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_39f98\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_39f98_level0_col0\" class=\"col_heading level0 col0\" >score</th>\n","      <th id=\"T_39f98_level0_col1\" class=\"col_heading level0 col1\" >sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_39f98_row0_col0\" class=\"data row0 col0\" >+0.8</td>\n","      <td id=\"T_39f98_row0_col1\" class=\"data row0 col1\" >Python is a very readable language, which makes it easy to understand and maintain code.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_39f98_row1_col0\" class=\"data row1 col0\" >+0.9</td>\n","      <td id=\"T_39f98_row1_col1\" class=\"data row1 col1\" >It's simple, very flexible, easy to learn, and suitable for a wide variety of tasks.</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_39f98_row2_col0\" class=\"data row2 col0\" >-0.4</td>\n","      <td id=\"T_39f98_row2_col1\" class=\"data row2 col1\" >One disadvantage is its speed: it's not as fast as some other programming languages.</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x160fb877f10>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","At document level:\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_f57c5 th {\n","  text-align: center;\n","}\n","#T_f57c5_row0_col2 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_f57c5\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_f57c5_level0_col0\" class=\"col_heading level0 col0\" >score</th>\n","      <th id=\"T_f57c5_level0_col1\" class=\"col_heading level0 col1\" >magnitude</th>\n","      <th id=\"T_f57c5_level0_col2\" class=\"col_heading level0 col2\" >language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_f57c5_row0_col0\" class=\"data row0 col0\" >+0.4</td>\n","      <td id=\"T_f57c5_row0_col1\" class=\"data row0 col1\" >2.2</td>\n","      <td id=\"T_f57c5_row0_col2\" class=\"data row0 col2\" >en</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x160fb5b6f90>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Python is a very readable language, which makes it easy to understand and maintain code. It's simple, very flexible, easy to learn, and suitable for a wide variety of tasks. One disadvantage is its speed: it's not as fast as some other programming languages.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","analyze_sentiment_response = analyze_text_sentiment(text)\n","\n","# Show the results\n","show_text_sentiment(analyze_sentiment_response)"]},{"cell_type":"markdown","metadata":{"id":"7hU8WO15c0jn"},"source":["Notes:\n","\n","- For information on which languages are supported by the Natural Language API, see [Language Support](https://cloud.google.com/natural-language/docs/languages#sentiment_analysis).\n","- The `score` of the sentiment ranges between -1.0 (negative) and 1.0 (positive) and corresponds to the overall sentiment from the given information.\n","- The `magnitude` of the sentiment ranges from 0.0 to +infinity and indicates the overall strength of sentiment from the given information. The more information provided, the higher the magnitude.\n","- For more information on how to interpret the `score` and `magnitude` sentiment values included in the analysis, see [Interpreting sentiment analysis values](https://cloud.google.com/natural-language/docs/basics#interpreting_sentiment_analysis_values).\n","- Each API response returns the document automatically-detected language (in ISO-639-1). It is shown here and will be skipped in the next analysis examples.\n"]},{"cell_type":"markdown","metadata":{"id":"HHY_gesZdrM1"},"source":["---\n","\n","## 2Ô∏è‚É£ Entity analysis\n","\n","Entity analysis inspects the given text for known entities (proper nouns such as public figures, landmarks, etc.), and returns information about those entities. It is performed with the `analyze_entities` method which returns an `AnalyzeEntitiesResponse`.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"6NOJl4LQdrM8"},"outputs":[],"source":["from google.cloud import language_v1 as language\n","\n","\n","def analyze_text_entities(text: str) -> language.AnalyzeEntitiesResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.analyze_entities(document=document)\n","\n","\n","def show_text_entities(response: language.AnalyzeEntitiesResponse):\n","    columns = (\"name\", \"type\", \"salience\", \"mid\", \"wikipedia_url\")\n","    data = (\n","        (\n","            entity.name,\n","            entity.type_.name,\n","            entity.salience,\n","            entity.metadata.get(\"mid\", \"\"),\n","            entity.metadata.get(\"wikipedia_url\", \"\"),\n","        )\n","        for entity in response.entities\n","    )\n","    formats = {\"salience\": \"{:.1%}\"}\n","    show_table(columns, data, formats)"]},{"cell_type":"markdown","metadata":{"id":"tdiVhvLrxJx7"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"28jLkpSqZ5Fc"},"outputs":[{"data":{"text/html":["<style type=\"text/css\">\n","#T_9c2ff th {\n","  text-align: center;\n","}\n","#T_9c2ff_row0_col0, #T_9c2ff_row0_col1, #T_9c2ff_row0_col3, #T_9c2ff_row0_col4, #T_9c2ff_row1_col0, #T_9c2ff_row1_col1, #T_9c2ff_row1_col3, #T_9c2ff_row1_col4, #T_9c2ff_row2_col0, #T_9c2ff_row2_col1, #T_9c2ff_row2_col3, #T_9c2ff_row2_col4, #T_9c2ff_row3_col0, #T_9c2ff_row3_col1, #T_9c2ff_row3_col3, #T_9c2ff_row3_col4, #T_9c2ff_row4_col0, #T_9c2ff_row4_col1, #T_9c2ff_row4_col3, #T_9c2ff_row4_col4, #T_9c2ff_row5_col0, #T_9c2ff_row5_col1, #T_9c2ff_row5_col3, #T_9c2ff_row5_col4, #T_9c2ff_row6_col0, #T_9c2ff_row6_col1, #T_9c2ff_row6_col3, #T_9c2ff_row6_col4 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_9c2ff\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_9c2ff_level0_col0\" class=\"col_heading level0 col0\" >name</th>\n","      <th id=\"T_9c2ff_level0_col1\" class=\"col_heading level0 col1\" >type</th>\n","      <th id=\"T_9c2ff_level0_col2\" class=\"col_heading level0 col2\" >salience</th>\n","      <th id=\"T_9c2ff_level0_col3\" class=\"col_heading level0 col3\" >mid</th>\n","      <th id=\"T_9c2ff_level0_col4\" class=\"col_heading level0 col4\" >wikipedia_url</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_9c2ff_row0_col0\" class=\"data row0 col0\" >Guido van Rossum</td>\n","      <td id=\"T_9c2ff_row0_col1\" class=\"data row0 col1\" >PERSON</td>\n","      <td id=\"T_9c2ff_row0_col2\" class=\"data row0 col2\" >49.8%</td>\n","      <td id=\"T_9c2ff_row0_col3\" class=\"data row0 col3\" >/m/01h05c</td>\n","      <td id=\"T_9c2ff_row0_col4\" class=\"data row0 col4\" >https://en.wikipedia.org/wiki/Guido_van_Rossum</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_9c2ff_row1_col0\" class=\"data row1 col0\" >Python</td>\n","      <td id=\"T_9c2ff_row1_col1\" class=\"data row1 col1\" >ORGANIZATION</td>\n","      <td id=\"T_9c2ff_row1_col2\" class=\"data row1 col2\" >38.4%</td>\n","      <td id=\"T_9c2ff_row1_col3\" class=\"data row1 col3\" >/m/05z1_</td>\n","      <td id=\"T_9c2ff_row1_col4\" class=\"data row1 col4\" >https://en.wikipedia.org/wiki/Python_(programming_language)</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_9c2ff_row2_col0\" class=\"data row2 col0\" >creator</td>\n","      <td id=\"T_9c2ff_row2_col1\" class=\"data row2 col1\" >PERSON</td>\n","      <td id=\"T_9c2ff_row2_col2\" class=\"data row2 col2\" >5.1%</td>\n","      <td id=\"T_9c2ff_row2_col3\" class=\"data row2 col3\" ></td>\n","      <td id=\"T_9c2ff_row2_col4\" class=\"data row2 col4\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_9c2ff_row3_col0\" class=\"data row3 col0\" >Monty Python</td>\n","      <td id=\"T_9c2ff_row3_col1\" class=\"data row3 col1\" >PERSON</td>\n","      <td id=\"T_9c2ff_row3_col2\" class=\"data row3 col2\" >3.2%</td>\n","      <td id=\"T_9c2ff_row3_col3\" class=\"data row3 col3\" >/m/04sd0</td>\n","      <td id=\"T_9c2ff_row3_col4\" class=\"data row3 col4\" >https://en.wikipedia.org/wiki/Monty_Python</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_9c2ff_row4_col0\" class=\"data row4 col0\" >comedy troupe</td>\n","      <td id=\"T_9c2ff_row4_col1\" class=\"data row4 col1\" >PERSON</td>\n","      <td id=\"T_9c2ff_row4_col2\" class=\"data row4 col2\" >1.6%</td>\n","      <td id=\"T_9c2ff_row4_col3\" class=\"data row4 col3\" ></td>\n","      <td id=\"T_9c2ff_row4_col4\" class=\"data row4 col4\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_9c2ff_row5_col0\" class=\"data row5 col0\" >Haarlem</td>\n","      <td id=\"T_9c2ff_row5_col1\" class=\"data row5 col1\" >LOCATION</td>\n","      <td id=\"T_9c2ff_row5_col2\" class=\"data row5 col2\" >1.0%</td>\n","      <td id=\"T_9c2ff_row5_col3\" class=\"data row5 col3\" >/m/0h095</td>\n","      <td id=\"T_9c2ff_row5_col4\" class=\"data row5 col4\" >https://en.wikipedia.org/wiki/Haarlem</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_9c2ff_row6_col0\" class=\"data row6 col0\" >Netherlands</td>\n","      <td id=\"T_9c2ff_row6_col1\" class=\"data row6 col1\" >LOCATION</td>\n","      <td id=\"T_9c2ff_row6_col2\" class=\"data row6 col2\" >0.7%</td>\n","      <td id=\"T_9c2ff_row6_col3\" class=\"data row6 col3\" >/m/059j2</td>\n","      <td id=\"T_9c2ff_row6_col4\" class=\"data row6 col4\" >https://en.wikipedia.org/wiki/Netherlands</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x160fb63bdd0>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Guido van Rossum is best known as the creator of Python, which he named after the Monty Python comedy troupe. He was born in Haarlem, Netherlands.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","analyze_entities_response = analyze_text_entities(text)\n","\n","# Show the results\n","show_text_entities(analyze_entities_response)"]},{"cell_type":"markdown","metadata":{"id":"ByF1lee3vCIE"},"source":["Notes:\n","\n","- For information on which languages are supported by this method, see [Language Support](https://cloud.google.com/natural-language/docs/languages#entity_analysis).\n","- The `type` of the entity is an enum that lets you classify or differentiate entities. For example, this can help distinguish the similarly named entities _‚ÄúT.E. Lawrence‚Äù_ (a `PERSON`) from _‚ÄúLawrence of Arabia‚Äù_ (the film) (tagged as a `WORK_OF_ART`). See [`Entity.Type`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.Entity.Type).\n","- The entity `salience` indicates the importance or relevance of this entity to the entire document text. This score can assist information retrieval and summarization by prioritizing salient entities. Scores closer to 0.0 are less important, while scores closer to 1.0 are highly important.\n","- For more information, see [Entity analysis](https://cloud.google.com/natural-language/docs/basics#entity_analysis).\n","- You can also combine both entity analysis and sentiment analysis with the `analyze_entity_sentiment` method. See [Entity sentiment analysis](https://cloud.google.com/natural-language/docs/basics#entity_analysis).\n"]},{"cell_type":"markdown","metadata":{"id":"uqzc9quwdurW"},"source":["---\n","\n","## 3Ô∏è‚É£ Syntax analysis\n","\n","Syntax analysis extracts linguistic information, breaking up the given text into a series of sentences and tokens (generally based on word boundaries), providing further analysis on those tokens. It is performed with the `analyze_syntax` method which returns an `AnalyzeSyntaxResponse`.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"iAOfoJEUdurW"},"outputs":[],"source":["from google.cloud import language_v1 as language\n","\n","\n","def analyze_text_syntax(text: str) -> language.AnalyzeSyntaxResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.analyze_syntax(document=document)\n","\n","\n","def get_token_info(token: language.Token | None) -> list[str]:\n","    parts = [\n","        \"tag\",\n","        \"aspect\",\n","        \"case\",\n","        \"form\",\n","        \"gender\",\n","        \"mood\",\n","        \"number\",\n","        \"person\",\n","        \"proper\",\n","        \"reciprocity\",\n","        \"tense\",\n","        \"voice\",\n","    ]\n","    if not token:\n","        return [\"token\", \"lemma\"] + parts\n","\n","    text = token.text.content\n","    lemma = token.lemma if token.lemma != token.text.content else \"\"\n","    info = [text, lemma]\n","    for part in parts:\n","        pos = token.part_of_speech\n","        info.append(getattr(pos, part).name if part in pos else \"\")\n","\n","    return info\n","\n","\n","def show_text_syntax(response: language.AnalyzeSyntaxResponse):\n","    tokens = len(response.tokens)\n","    sentences = len(response.sentences)\n","    columns = get_token_info(None)\n","    data = (get_token_info(token) for token in response.tokens)\n","\n","    print(f\"Analyzed {tokens} token(s) from {sentences} sentence(s)\")\n","    show_table(columns, data, remove_empty_columns=True)"]},{"cell_type":"markdown","metadata":{"id":"YSDS0-o0xNqf"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"spjtSAfaDqQL"},"outputs":[{"name":"stdout","output_type":"stream","text":["Analyzed 20 token(s) from 2 sentence(s)\n"]},{"data":{"text/html":["<style type=\"text/css\">\n","#T_c0c17 th {\n","  text-align: center;\n","}\n","#T_c0c17_row0_col0, #T_c0c17_row0_col1, #T_c0c17_row0_col2, #T_c0c17_row0_col3, #T_c0c17_row0_col4, #T_c0c17_row0_col5, #T_c0c17_row0_col6, #T_c0c17_row0_col7, #T_c0c17_row0_col8, #T_c0c17_row0_col9, #T_c0c17_row0_col10, #T_c0c17_row1_col0, #T_c0c17_row1_col1, #T_c0c17_row1_col2, #T_c0c17_row1_col3, #T_c0c17_row1_col4, #T_c0c17_row1_col5, #T_c0c17_row1_col6, #T_c0c17_row1_col7, #T_c0c17_row1_col8, #T_c0c17_row1_col9, #T_c0c17_row1_col10, #T_c0c17_row2_col0, #T_c0c17_row2_col1, #T_c0c17_row2_col2, #T_c0c17_row2_col3, #T_c0c17_row2_col4, #T_c0c17_row2_col5, #T_c0c17_row2_col6, #T_c0c17_row2_col7, #T_c0c17_row2_col8, #T_c0c17_row2_col9, #T_c0c17_row2_col10, #T_c0c17_row3_col0, #T_c0c17_row3_col1, #T_c0c17_row3_col2, #T_c0c17_row3_col3, #T_c0c17_row3_col4, #T_c0c17_row3_col5, #T_c0c17_row3_col6, #T_c0c17_row3_col7, #T_c0c17_row3_col8, #T_c0c17_row3_col9, #T_c0c17_row3_col10, #T_c0c17_row4_col0, #T_c0c17_row4_col1, #T_c0c17_row4_col2, #T_c0c17_row4_col3, #T_c0c17_row4_col4, #T_c0c17_row4_col5, #T_c0c17_row4_col6, #T_c0c17_row4_col7, #T_c0c17_row4_col8, #T_c0c17_row4_col9, #T_c0c17_row4_col10, #T_c0c17_row5_col0, #T_c0c17_row5_col1, #T_c0c17_row5_col2, #T_c0c17_row5_col3, #T_c0c17_row5_col4, #T_c0c17_row5_col5, #T_c0c17_row5_col6, #T_c0c17_row5_col7, #T_c0c17_row5_col8, #T_c0c17_row5_col9, #T_c0c17_row5_col10, #T_c0c17_row6_col0, #T_c0c17_row6_col1, #T_c0c17_row6_col2, #T_c0c17_row6_col3, #T_c0c17_row6_col4, #T_c0c17_row6_col5, #T_c0c17_row6_col6, #T_c0c17_row6_col7, #T_c0c17_row6_col8, #T_c0c17_row6_col9, #T_c0c17_row6_col10, #T_c0c17_row7_col0, #T_c0c17_row7_col1, #T_c0c17_row7_col2, #T_c0c17_row7_col3, #T_c0c17_row7_col4, #T_c0c17_row7_col5, #T_c0c17_row7_col6, #T_c0c17_row7_col7, #T_c0c17_row7_col8, #T_c0c17_row7_col9, #T_c0c17_row7_col10, #T_c0c17_row8_col0, #T_c0c17_row8_col1, #T_c0c17_row8_col2, #T_c0c17_row8_col3, #T_c0c17_row8_col4, #T_c0c17_row8_col5, #T_c0c17_row8_col6, #T_c0c17_row8_col7, #T_c0c17_row8_col8, #T_c0c17_row8_col9, #T_c0c17_row8_col10, #T_c0c17_row9_col0, #T_c0c17_row9_col1, #T_c0c17_row9_col2, #T_c0c17_row9_col3, #T_c0c17_row9_col4, #T_c0c17_row9_col5, #T_c0c17_row9_col6, #T_c0c17_row9_col7, #T_c0c17_row9_col8, #T_c0c17_row9_col9, #T_c0c17_row9_col10, #T_c0c17_row10_col0, #T_c0c17_row10_col1, #T_c0c17_row10_col2, #T_c0c17_row10_col3, #T_c0c17_row10_col4, #T_c0c17_row10_col5, #T_c0c17_row10_col6, #T_c0c17_row10_col7, #T_c0c17_row10_col8, #T_c0c17_row10_col9, #T_c0c17_row10_col10, #T_c0c17_row11_col0, #T_c0c17_row11_col1, #T_c0c17_row11_col2, #T_c0c17_row11_col3, #T_c0c17_row11_col4, #T_c0c17_row11_col5, #T_c0c17_row11_col6, #T_c0c17_row11_col7, #T_c0c17_row11_col8, #T_c0c17_row11_col9, #T_c0c17_row11_col10, #T_c0c17_row12_col0, #T_c0c17_row12_col1, #T_c0c17_row12_col2, #T_c0c17_row12_col3, #T_c0c17_row12_col4, #T_c0c17_row12_col5, #T_c0c17_row12_col6, #T_c0c17_row12_col7, #T_c0c17_row12_col8, #T_c0c17_row12_col9, #T_c0c17_row12_col10, #T_c0c17_row13_col0, #T_c0c17_row13_col1, #T_c0c17_row13_col2, #T_c0c17_row13_col3, #T_c0c17_row13_col4, #T_c0c17_row13_col5, #T_c0c17_row13_col6, #T_c0c17_row13_col7, #T_c0c17_row13_col8, #T_c0c17_row13_col9, #T_c0c17_row13_col10, #T_c0c17_row14_col0, #T_c0c17_row14_col1, #T_c0c17_row14_col2, #T_c0c17_row14_col3, #T_c0c17_row14_col4, #T_c0c17_row14_col5, #T_c0c17_row14_col6, #T_c0c17_row14_col7, #T_c0c17_row14_col8, #T_c0c17_row14_col9, #T_c0c17_row14_col10, #T_c0c17_row15_col0, #T_c0c17_row15_col1, #T_c0c17_row15_col2, #T_c0c17_row15_col3, #T_c0c17_row15_col4, #T_c0c17_row15_col5, #T_c0c17_row15_col6, #T_c0c17_row15_col7, #T_c0c17_row15_col8, #T_c0c17_row15_col9, #T_c0c17_row15_col10, #T_c0c17_row16_col0, #T_c0c17_row16_col1, #T_c0c17_row16_col2, #T_c0c17_row16_col3, #T_c0c17_row16_col4, #T_c0c17_row16_col5, #T_c0c17_row16_col6, #T_c0c17_row16_col7, #T_c0c17_row16_col8, #T_c0c17_row16_col9, #T_c0c17_row16_col10, #T_c0c17_row17_col0, #T_c0c17_row17_col1, #T_c0c17_row17_col2, #T_c0c17_row17_col3, #T_c0c17_row17_col4, #T_c0c17_row17_col5, #T_c0c17_row17_col6, #T_c0c17_row17_col7, #T_c0c17_row17_col8, #T_c0c17_row17_col9, #T_c0c17_row17_col10, #T_c0c17_row18_col0, #T_c0c17_row18_col1, #T_c0c17_row18_col2, #T_c0c17_row18_col3, #T_c0c17_row18_col4, #T_c0c17_row18_col5, #T_c0c17_row18_col6, #T_c0c17_row18_col7, #T_c0c17_row18_col8, #T_c0c17_row18_col9, #T_c0c17_row18_col10, #T_c0c17_row19_col0, #T_c0c17_row19_col1, #T_c0c17_row19_col2, #T_c0c17_row19_col3, #T_c0c17_row19_col4, #T_c0c17_row19_col5, #T_c0c17_row19_col6, #T_c0c17_row19_col7, #T_c0c17_row19_col8, #T_c0c17_row19_col9, #T_c0c17_row19_col10 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_c0c17\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_c0c17_level0_col0\" class=\"col_heading level0 col0\" >token</th>\n","      <th id=\"T_c0c17_level0_col1\" class=\"col_heading level0 col1\" >lemma</th>\n","      <th id=\"T_c0c17_level0_col2\" class=\"col_heading level0 col2\" >tag</th>\n","      <th id=\"T_c0c17_level0_col3\" class=\"col_heading level0 col3\" >case</th>\n","      <th id=\"T_c0c17_level0_col4\" class=\"col_heading level0 col4\" >gender</th>\n","      <th id=\"T_c0c17_level0_col5\" class=\"col_heading level0 col5\" >mood</th>\n","      <th id=\"T_c0c17_level0_col6\" class=\"col_heading level0 col6\" >number</th>\n","      <th id=\"T_c0c17_level0_col7\" class=\"col_heading level0 col7\" >person</th>\n","      <th id=\"T_c0c17_level0_col8\" class=\"col_heading level0 col8\" >proper</th>\n","      <th id=\"T_c0c17_level0_col9\" class=\"col_heading level0 col9\" >tense</th>\n","      <th id=\"T_c0c17_level0_col10\" class=\"col_heading level0 col10\" >voice</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_c0c17_row0_col0\" class=\"data row0 col0\" >Guido</td>\n","      <td id=\"T_c0c17_row0_col1\" class=\"data row0 col1\" ></td>\n","      <td id=\"T_c0c17_row0_col2\" class=\"data row0 col2\" >NOUN</td>\n","      <td id=\"T_c0c17_row0_col3\" class=\"data row0 col3\" ></td>\n","      <td id=\"T_c0c17_row0_col4\" class=\"data row0 col4\" ></td>\n","      <td id=\"T_c0c17_row0_col5\" class=\"data row0 col5\" ></td>\n","      <td id=\"T_c0c17_row0_col6\" class=\"data row0 col6\" >SINGULAR</td>\n","      <td id=\"T_c0c17_row0_col7\" class=\"data row0 col7\" ></td>\n","      <td id=\"T_c0c17_row0_col8\" class=\"data row0 col8\" >PROPER</td>\n","      <td id=\"T_c0c17_row0_col9\" class=\"data row0 col9\" ></td>\n","      <td id=\"T_c0c17_row0_col10\" class=\"data row0 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row1_col0\" class=\"data row1 col0\" >van</td>\n","      <td id=\"T_c0c17_row1_col1\" class=\"data row1 col1\" ></td>\n","      <td id=\"T_c0c17_row1_col2\" class=\"data row1 col2\" >NOUN</td>\n","      <td id=\"T_c0c17_row1_col3\" class=\"data row1 col3\" ></td>\n","      <td id=\"T_c0c17_row1_col4\" class=\"data row1 col4\" ></td>\n","      <td id=\"T_c0c17_row1_col5\" class=\"data row1 col5\" ></td>\n","      <td id=\"T_c0c17_row1_col6\" class=\"data row1 col6\" >SINGULAR</td>\n","      <td id=\"T_c0c17_row1_col7\" class=\"data row1 col7\" ></td>\n","      <td id=\"T_c0c17_row1_col8\" class=\"data row1 col8\" >PROPER</td>\n","      <td id=\"T_c0c17_row1_col9\" class=\"data row1 col9\" ></td>\n","      <td id=\"T_c0c17_row1_col10\" class=\"data row1 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row2_col0\" class=\"data row2 col0\" >Rossum</td>\n","      <td id=\"T_c0c17_row2_col1\" class=\"data row2 col1\" ></td>\n","      <td id=\"T_c0c17_row2_col2\" class=\"data row2 col2\" >NOUN</td>\n","      <td id=\"T_c0c17_row2_col3\" class=\"data row2 col3\" ></td>\n","      <td id=\"T_c0c17_row2_col4\" class=\"data row2 col4\" ></td>\n","      <td id=\"T_c0c17_row2_col5\" class=\"data row2 col5\" ></td>\n","      <td id=\"T_c0c17_row2_col6\" class=\"data row2 col6\" >SINGULAR</td>\n","      <td id=\"T_c0c17_row2_col7\" class=\"data row2 col7\" ></td>\n","      <td id=\"T_c0c17_row2_col8\" class=\"data row2 col8\" >PROPER</td>\n","      <td id=\"T_c0c17_row2_col9\" class=\"data row2 col9\" ></td>\n","      <td id=\"T_c0c17_row2_col10\" class=\"data row2 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row3_col0\" class=\"data row3 col0\" >is</td>\n","      <td id=\"T_c0c17_row3_col1\" class=\"data row3 col1\" >be</td>\n","      <td id=\"T_c0c17_row3_col2\" class=\"data row3 col2\" >VERB</td>\n","      <td id=\"T_c0c17_row3_col3\" class=\"data row3 col3\" ></td>\n","      <td id=\"T_c0c17_row3_col4\" class=\"data row3 col4\" ></td>\n","      <td id=\"T_c0c17_row3_col5\" class=\"data row3 col5\" >INDICATIVE</td>\n","      <td id=\"T_c0c17_row3_col6\" class=\"data row3 col6\" >SINGULAR</td>\n","      <td id=\"T_c0c17_row3_col7\" class=\"data row3 col7\" >THIRD</td>\n","      <td id=\"T_c0c17_row3_col8\" class=\"data row3 col8\" ></td>\n","      <td id=\"T_c0c17_row3_col9\" class=\"data row3 col9\" >PRESENT</td>\n","      <td id=\"T_c0c17_row3_col10\" class=\"data row3 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row4_col0\" class=\"data row4 col0\" >best</td>\n","      <td id=\"T_c0c17_row4_col1\" class=\"data row4 col1\" >well</td>\n","      <td id=\"T_c0c17_row4_col2\" class=\"data row4 col2\" >ADV</td>\n","      <td id=\"T_c0c17_row4_col3\" class=\"data row4 col3\" ></td>\n","      <td id=\"T_c0c17_row4_col4\" class=\"data row4 col4\" ></td>\n","      <td id=\"T_c0c17_row4_col5\" class=\"data row4 col5\" ></td>\n","      <td id=\"T_c0c17_row4_col6\" class=\"data row4 col6\" ></td>\n","      <td id=\"T_c0c17_row4_col7\" class=\"data row4 col7\" ></td>\n","      <td id=\"T_c0c17_row4_col8\" class=\"data row4 col8\" ></td>\n","      <td id=\"T_c0c17_row4_col9\" class=\"data row4 col9\" ></td>\n","      <td id=\"T_c0c17_row4_col10\" class=\"data row4 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row5_col0\" class=\"data row5 col0\" >known</td>\n","      <td id=\"T_c0c17_row5_col1\" class=\"data row5 col1\" >know</td>\n","      <td id=\"T_c0c17_row5_col2\" class=\"data row5 col2\" >VERB</td>\n","      <td id=\"T_c0c17_row5_col3\" class=\"data row5 col3\" ></td>\n","      <td id=\"T_c0c17_row5_col4\" class=\"data row5 col4\" ></td>\n","      <td id=\"T_c0c17_row5_col5\" class=\"data row5 col5\" ></td>\n","      <td id=\"T_c0c17_row5_col6\" class=\"data row5 col6\" ></td>\n","      <td id=\"T_c0c17_row5_col7\" class=\"data row5 col7\" ></td>\n","      <td id=\"T_c0c17_row5_col8\" class=\"data row5 col8\" ></td>\n","      <td id=\"T_c0c17_row5_col9\" class=\"data row5 col9\" >PAST</td>\n","      <td id=\"T_c0c17_row5_col10\" class=\"data row5 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row6_col0\" class=\"data row6 col0\" >as</td>\n","      <td id=\"T_c0c17_row6_col1\" class=\"data row6 col1\" ></td>\n","      <td id=\"T_c0c17_row6_col2\" class=\"data row6 col2\" >ADP</td>\n","      <td id=\"T_c0c17_row6_col3\" class=\"data row6 col3\" ></td>\n","      <td id=\"T_c0c17_row6_col4\" class=\"data row6 col4\" ></td>\n","      <td id=\"T_c0c17_row6_col5\" class=\"data row6 col5\" ></td>\n","      <td id=\"T_c0c17_row6_col6\" class=\"data row6 col6\" ></td>\n","      <td id=\"T_c0c17_row6_col7\" class=\"data row6 col7\" ></td>\n","      <td id=\"T_c0c17_row6_col8\" class=\"data row6 col8\" ></td>\n","      <td id=\"T_c0c17_row6_col9\" class=\"data row6 col9\" ></td>\n","      <td id=\"T_c0c17_row6_col10\" class=\"data row6 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row7_col0\" class=\"data row7 col0\" >the</td>\n","      <td id=\"T_c0c17_row7_col1\" class=\"data row7 col1\" ></td>\n","      <td id=\"T_c0c17_row7_col2\" class=\"data row7 col2\" >DET</td>\n","      <td id=\"T_c0c17_row7_col3\" class=\"data row7 col3\" ></td>\n","      <td id=\"T_c0c17_row7_col4\" class=\"data row7 col4\" ></td>\n","      <td id=\"T_c0c17_row7_col5\" class=\"data row7 col5\" ></td>\n","      <td id=\"T_c0c17_row7_col6\" class=\"data row7 col6\" ></td>\n","      <td id=\"T_c0c17_row7_col7\" class=\"data row7 col7\" ></td>\n","      <td id=\"T_c0c17_row7_col8\" class=\"data row7 col8\" ></td>\n","      <td id=\"T_c0c17_row7_col9\" class=\"data row7 col9\" ></td>\n","      <td id=\"T_c0c17_row7_col10\" class=\"data row7 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row8_col0\" class=\"data row8 col0\" >creator</td>\n","      <td id=\"T_c0c17_row8_col1\" class=\"data row8 col1\" ></td>\n","      <td id=\"T_c0c17_row8_col2\" class=\"data row8 col2\" >NOUN</td>\n","      <td id=\"T_c0c17_row8_col3\" class=\"data row8 col3\" ></td>\n","      <td id=\"T_c0c17_row8_col4\" class=\"data row8 col4\" ></td>\n","      <td id=\"T_c0c17_row8_col5\" class=\"data row8 col5\" ></td>\n","      <td id=\"T_c0c17_row8_col6\" class=\"data row8 col6\" >SINGULAR</td>\n","      <td id=\"T_c0c17_row8_col7\" class=\"data row8 col7\" ></td>\n","      <td id=\"T_c0c17_row8_col8\" class=\"data row8 col8\" ></td>\n","      <td id=\"T_c0c17_row8_col9\" class=\"data row8 col9\" ></td>\n","      <td id=\"T_c0c17_row8_col10\" class=\"data row8 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row9_col0\" class=\"data row9 col0\" >of</td>\n","      <td id=\"T_c0c17_row9_col1\" class=\"data row9 col1\" ></td>\n","      <td id=\"T_c0c17_row9_col2\" class=\"data row9 col2\" >ADP</td>\n","      <td id=\"T_c0c17_row9_col3\" class=\"data row9 col3\" ></td>\n","      <td id=\"T_c0c17_row9_col4\" class=\"data row9 col4\" ></td>\n","      <td id=\"T_c0c17_row9_col5\" class=\"data row9 col5\" ></td>\n","      <td id=\"T_c0c17_row9_col6\" class=\"data row9 col6\" ></td>\n","      <td id=\"T_c0c17_row9_col7\" class=\"data row9 col7\" ></td>\n","      <td id=\"T_c0c17_row9_col8\" class=\"data row9 col8\" ></td>\n","      <td id=\"T_c0c17_row9_col9\" class=\"data row9 col9\" ></td>\n","      <td id=\"T_c0c17_row9_col10\" class=\"data row9 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row10_col0\" class=\"data row10 col0\" >Python</td>\n","      <td id=\"T_c0c17_row10_col1\" class=\"data row10 col1\" ></td>\n","      <td id=\"T_c0c17_row10_col2\" class=\"data row10 col2\" >NOUN</td>\n","      <td id=\"T_c0c17_row10_col3\" class=\"data row10 col3\" ></td>\n","      <td id=\"T_c0c17_row10_col4\" class=\"data row10 col4\" ></td>\n","      <td id=\"T_c0c17_row10_col5\" class=\"data row10 col5\" ></td>\n","      <td id=\"T_c0c17_row10_col6\" class=\"data row10 col6\" >SINGULAR</td>\n","      <td id=\"T_c0c17_row10_col7\" class=\"data row10 col7\" ></td>\n","      <td id=\"T_c0c17_row10_col8\" class=\"data row10 col8\" >PROPER</td>\n","      <td id=\"T_c0c17_row10_col9\" class=\"data row10 col9\" ></td>\n","      <td id=\"T_c0c17_row10_col10\" class=\"data row10 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row11_col0\" class=\"data row11 col0\" >.</td>\n","      <td id=\"T_c0c17_row11_col1\" class=\"data row11 col1\" ></td>\n","      <td id=\"T_c0c17_row11_col2\" class=\"data row11 col2\" >PUNCT</td>\n","      <td id=\"T_c0c17_row11_col3\" class=\"data row11 col3\" ></td>\n","      <td id=\"T_c0c17_row11_col4\" class=\"data row11 col4\" ></td>\n","      <td id=\"T_c0c17_row11_col5\" class=\"data row11 col5\" ></td>\n","      <td id=\"T_c0c17_row11_col6\" class=\"data row11 col6\" ></td>\n","      <td id=\"T_c0c17_row11_col7\" class=\"data row11 col7\" ></td>\n","      <td id=\"T_c0c17_row11_col8\" class=\"data row11 col8\" ></td>\n","      <td id=\"T_c0c17_row11_col9\" class=\"data row11 col9\" ></td>\n","      <td id=\"T_c0c17_row11_col10\" class=\"data row11 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row12_col0\" class=\"data row12 col0\" >He</td>\n","      <td id=\"T_c0c17_row12_col1\" class=\"data row12 col1\" ></td>\n","      <td id=\"T_c0c17_row12_col2\" class=\"data row12 col2\" >PRON</td>\n","      <td id=\"T_c0c17_row12_col3\" class=\"data row12 col3\" >NOMINATIVE</td>\n","      <td id=\"T_c0c17_row12_col4\" class=\"data row12 col4\" >MASCULINE</td>\n","      <td id=\"T_c0c17_row12_col5\" class=\"data row12 col5\" ></td>\n","      <td id=\"T_c0c17_row12_col6\" class=\"data row12 col6\" >SINGULAR</td>\n","      <td id=\"T_c0c17_row12_col7\" class=\"data row12 col7\" >THIRD</td>\n","      <td id=\"T_c0c17_row12_col8\" class=\"data row12 col8\" ></td>\n","      <td id=\"T_c0c17_row12_col9\" class=\"data row12 col9\" ></td>\n","      <td id=\"T_c0c17_row12_col10\" class=\"data row12 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row13_col0\" class=\"data row13 col0\" >was</td>\n","      <td id=\"T_c0c17_row13_col1\" class=\"data row13 col1\" >be</td>\n","      <td id=\"T_c0c17_row13_col2\" class=\"data row13 col2\" >VERB</td>\n","      <td id=\"T_c0c17_row13_col3\" class=\"data row13 col3\" ></td>\n","      <td id=\"T_c0c17_row13_col4\" class=\"data row13 col4\" ></td>\n","      <td id=\"T_c0c17_row13_col5\" class=\"data row13 col5\" >INDICATIVE</td>\n","      <td id=\"T_c0c17_row13_col6\" class=\"data row13 col6\" >SINGULAR</td>\n","      <td id=\"T_c0c17_row13_col7\" class=\"data row13 col7\" >THIRD</td>\n","      <td id=\"T_c0c17_row13_col8\" class=\"data row13 col8\" ></td>\n","      <td id=\"T_c0c17_row13_col9\" class=\"data row13 col9\" >PAST</td>\n","      <td id=\"T_c0c17_row13_col10\" class=\"data row13 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row14_col0\" class=\"data row14 col0\" >born</td>\n","      <td id=\"T_c0c17_row14_col1\" class=\"data row14 col1\" >bear</td>\n","      <td id=\"T_c0c17_row14_col2\" class=\"data row14 col2\" >VERB</td>\n","      <td id=\"T_c0c17_row14_col3\" class=\"data row14 col3\" ></td>\n","      <td id=\"T_c0c17_row14_col4\" class=\"data row14 col4\" ></td>\n","      <td id=\"T_c0c17_row14_col5\" class=\"data row14 col5\" ></td>\n","      <td id=\"T_c0c17_row14_col6\" class=\"data row14 col6\" ></td>\n","      <td id=\"T_c0c17_row14_col7\" class=\"data row14 col7\" ></td>\n","      <td id=\"T_c0c17_row14_col8\" class=\"data row14 col8\" ></td>\n","      <td id=\"T_c0c17_row14_col9\" class=\"data row14 col9\" >PAST</td>\n","      <td id=\"T_c0c17_row14_col10\" class=\"data row14 col10\" >PASSIVE</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row15_col0\" class=\"data row15 col0\" >in</td>\n","      <td id=\"T_c0c17_row15_col1\" class=\"data row15 col1\" ></td>\n","      <td id=\"T_c0c17_row15_col2\" class=\"data row15 col2\" >ADP</td>\n","      <td id=\"T_c0c17_row15_col3\" class=\"data row15 col3\" ></td>\n","      <td id=\"T_c0c17_row15_col4\" class=\"data row15 col4\" ></td>\n","      <td id=\"T_c0c17_row15_col5\" class=\"data row15 col5\" ></td>\n","      <td id=\"T_c0c17_row15_col6\" class=\"data row15 col6\" ></td>\n","      <td id=\"T_c0c17_row15_col7\" class=\"data row15 col7\" ></td>\n","      <td id=\"T_c0c17_row15_col8\" class=\"data row15 col8\" ></td>\n","      <td id=\"T_c0c17_row15_col9\" class=\"data row15 col9\" ></td>\n","      <td id=\"T_c0c17_row15_col10\" class=\"data row15 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row16_col0\" class=\"data row16 col0\" >Haarlem</td>\n","      <td id=\"T_c0c17_row16_col1\" class=\"data row16 col1\" ></td>\n","      <td id=\"T_c0c17_row16_col2\" class=\"data row16 col2\" >NOUN</td>\n","      <td id=\"T_c0c17_row16_col3\" class=\"data row16 col3\" ></td>\n","      <td id=\"T_c0c17_row16_col4\" class=\"data row16 col4\" ></td>\n","      <td id=\"T_c0c17_row16_col5\" class=\"data row16 col5\" ></td>\n","      <td id=\"T_c0c17_row16_col6\" class=\"data row16 col6\" >SINGULAR</td>\n","      <td id=\"T_c0c17_row16_col7\" class=\"data row16 col7\" ></td>\n","      <td id=\"T_c0c17_row16_col8\" class=\"data row16 col8\" >PROPER</td>\n","      <td id=\"T_c0c17_row16_col9\" class=\"data row16 col9\" ></td>\n","      <td id=\"T_c0c17_row16_col10\" class=\"data row16 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row17_col0\" class=\"data row17 col0\" >,</td>\n","      <td id=\"T_c0c17_row17_col1\" class=\"data row17 col1\" ></td>\n","      <td id=\"T_c0c17_row17_col2\" class=\"data row17 col2\" >PUNCT</td>\n","      <td id=\"T_c0c17_row17_col3\" class=\"data row17 col3\" ></td>\n","      <td id=\"T_c0c17_row17_col4\" class=\"data row17 col4\" ></td>\n","      <td id=\"T_c0c17_row17_col5\" class=\"data row17 col5\" ></td>\n","      <td id=\"T_c0c17_row17_col6\" class=\"data row17 col6\" ></td>\n","      <td id=\"T_c0c17_row17_col7\" class=\"data row17 col7\" ></td>\n","      <td id=\"T_c0c17_row17_col8\" class=\"data row17 col8\" ></td>\n","      <td id=\"T_c0c17_row17_col9\" class=\"data row17 col9\" ></td>\n","      <td id=\"T_c0c17_row17_col10\" class=\"data row17 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row18_col0\" class=\"data row18 col0\" >Netherlands</td>\n","      <td id=\"T_c0c17_row18_col1\" class=\"data row18 col1\" ></td>\n","      <td id=\"T_c0c17_row18_col2\" class=\"data row18 col2\" >NOUN</td>\n","      <td id=\"T_c0c17_row18_col3\" class=\"data row18 col3\" ></td>\n","      <td id=\"T_c0c17_row18_col4\" class=\"data row18 col4\" ></td>\n","      <td id=\"T_c0c17_row18_col5\" class=\"data row18 col5\" ></td>\n","      <td id=\"T_c0c17_row18_col6\" class=\"data row18 col6\" >SINGULAR</td>\n","      <td id=\"T_c0c17_row18_col7\" class=\"data row18 col7\" ></td>\n","      <td id=\"T_c0c17_row18_col8\" class=\"data row18 col8\" >PROPER</td>\n","      <td id=\"T_c0c17_row18_col9\" class=\"data row18 col9\" ></td>\n","      <td id=\"T_c0c17_row18_col10\" class=\"data row18 col10\" ></td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_c0c17_row19_col0\" class=\"data row19 col0\" >.</td>\n","      <td id=\"T_c0c17_row19_col1\" class=\"data row19 col1\" ></td>\n","      <td id=\"T_c0c17_row19_col2\" class=\"data row19 col2\" >PUNCT</td>\n","      <td id=\"T_c0c17_row19_col3\" class=\"data row19 col3\" ></td>\n","      <td id=\"T_c0c17_row19_col4\" class=\"data row19 col4\" ></td>\n","      <td id=\"T_c0c17_row19_col5\" class=\"data row19 col5\" ></td>\n","      <td id=\"T_c0c17_row19_col6\" class=\"data row19 col6\" ></td>\n","      <td id=\"T_c0c17_row19_col7\" class=\"data row19 col7\" ></td>\n","      <td id=\"T_c0c17_row19_col8\" class=\"data row19 col8\" ></td>\n","      <td id=\"T_c0c17_row19_col9\" class=\"data row19 col9\" ></td>\n","      <td id=\"T_c0c17_row19_col10\" class=\"data row19 col10\" ></td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x160eab12010>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Guido van Rossum is best known as the creator of Python. He was born in Haarlem, Netherlands.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","analyze_syntax_response = analyze_text_syntax(text)\n","\n","# Show the results\n","show_text_syntax(analyze_syntax_response)"]},{"cell_type":"markdown","metadata":{"id":"VmRISErnz7iq"},"source":["There are multiple benefits to extracting the syntax information. One of them is to extract the lemmas. A `lemma` contains the \"root\" word upon which this token is based, which allows you to manage words with their canonical forms.\n","\n","If you dive deeper into the response insights, you'll also find the relationships between the tokens. Here is a visual interpretation showing the complete syntax analysis for this example:\n","\n","![Syntax Analysis](./pics/natural_language_syntax.png)\n","\n","> This is a screenshot from the online [Natural Language demo](https://cloud.google.com/natural-language/#natural-language-api-demo) with which you can create your own parse trees.\n","\n","For more information, see the following:\n","\n","- [`language.AnalyzeSyntaxResponse`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.AnalyzeSyntaxResponse)\n","- [Language Support](https://cloud.google.com/natural-language/docs/languages#syntactic_analysis)\n","- [Syntactic analysis](https://cloud.google.com/natural-language/docs/basics#syntactic_analysis)\n","- [Morphology & Dependency Trees](https://cloud.google.com/natural-language/docs/morphology)\n"]},{"cell_type":"markdown","metadata":{"id":"KlDeFxQXdxHw"},"source":["---\n","\n","## 4Ô∏è‚É£ Content classification\n","\n","Content classification analyzes a document and returns a list of content categories that apply to the text found in the document. It is performed with the `classify_text` method which returns a `ClassifyTextResponse`.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Q0dk8qaldxHw"},"outputs":[],"source":["from google.cloud import language_v1 as language\n","\n","\n","def classify_text(text: str) -> language.ClassifyTextResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.classify_text(document=document)\n","\n","\n","def show_text_classification(response: language.ClassifyTextResponse):\n","    columns = [\"category\", \"confidence\"]\n","    data = ((category.name, category.confidence) for category in response.categories)\n","    formats = {\"confidence\": \"{:.0%}\"}\n","    show_table(columns, data, formats)"]},{"cell_type":"markdown","metadata":{"id":"1e6v4_D_xOpC"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"rX5uDw7gbqeW"},"outputs":[{"data":{"text/html":["<style type=\"text/css\">\n","#T_e5920 th {\n","  text-align: center;\n","}\n","#T_e5920_row0_col0, #T_e5920_row1_col0 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_e5920\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_e5920_level0_col0\" class=\"col_heading level0 col0\" >category</th>\n","      <th id=\"T_e5920_level0_col1\" class=\"col_heading level0 col1\" >confidence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_e5920_row0_col0\" class=\"data row0 col0\" >/Computers & Electronics/Programming</td>\n","      <td id=\"T_e5920_row0_col1\" class=\"data row0 col1\" >99%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_e5920_row1_col0\" class=\"data row1 col0\" >/Science/Computer Science</td>\n","      <td id=\"T_e5920_row1_col1\" class=\"data row1 col1\" >99%</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x160fb691690>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","classify_text_response = classify_text(text)\n","\n","# Show the results\n","show_text_classification(classify_text_response)"]},{"cell_type":"markdown","metadata":{"id":"lmTbxLTcU0DD"},"source":["> Important: You must supply a text block (document) with at least twenty tokens.\n","\n","For more information, see the following docs:\n","\n","- [`ClassifyTextResponse`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.ClassifyTextResponse)\n","- [Language Support](https://cloud.google.com/natural-language/docs/languages#content_classification)\n","- [Content Classification](https://cloud.google.com/natural-language/docs/basics#content-classification)\n"]},{"cell_type":"markdown","metadata":{"id":"6kwyisBaPjdy"},"source":["---\n","\n","## 5Ô∏è‚É£ Text moderation\n","\n","Powered by Google's latest [PaLM 2](https://blog.google/technology/ai/google-palm-2-ai-large-language-model) foundation model, text moderation identifies a wide range of harmful content, including hate speech, bullying, and sexual harassment. It is performed with the `moderate_text` method which returns a `ModerateTextResponse`.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"lCnXARVgQeEG"},"outputs":[],"source":["from google.cloud import language_v1 as language\n","\n","\n","def moderate_text(text: str) -> language.ModerateTextResponse:\n","    client = language.LanguageServiceClient()\n","    document = language.Document(\n","        content=text,\n","        type_=language.Document.Type.PLAIN_TEXT,\n","    )\n","    return client.moderate_text(document=document)\n","\n","\n","def show_text_moderation(response: language.ModerateTextResponse):\n","    def confidence(category: language.ClassificationCategory) -> float:\n","        return category.confidence\n","\n","    columns = [\"category\", \"confidence\"]\n","    categories = response.moderation_categories\n","    sorted_categories = sorted(categories, key=confidence, reverse=True)\n","    data = ((category.name, category.confidence) for category in sorted_categories)\n","    formats = {\"confidence\": \"{:.0%}\"}\n","    show_table(columns, data, formats)"]},{"cell_type":"markdown","metadata":{"id":"bYCoQUl2AVyY"},"source":["Now, let's perform an analysis:\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"EAvFP85eAVye"},"outputs":[{"data":{"text/html":["<style type=\"text/css\">\n","#T_cfccf th {\n","  text-align: center;\n","}\n","#T_cfccf_row0_col0, #T_cfccf_row1_col0, #T_cfccf_row2_col0, #T_cfccf_row3_col0, #T_cfccf_row4_col0, #T_cfccf_row5_col0, #T_cfccf_row6_col0, #T_cfccf_row7_col0, #T_cfccf_row8_col0, #T_cfccf_row9_col0, #T_cfccf_row10_col0, #T_cfccf_row11_col0, #T_cfccf_row12_col0, #T_cfccf_row13_col0, #T_cfccf_row14_col0, #T_cfccf_row15_col0 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_cfccf\">\n","  <thead>\n","    <tr>\n","      <th id=\"T_cfccf_level0_col0\" class=\"col_heading level0 col0\" >category</th>\n","      <th id=\"T_cfccf_level0_col1\" class=\"col_heading level0 col1\" >confidence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td id=\"T_cfccf_row0_col0\" class=\"data row0 col0\" >Toxic</td>\n","      <td id=\"T_cfccf_row0_col1\" class=\"data row0 col1\" >68%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row1_col0\" class=\"data row1 col0\" >Insult</td>\n","      <td id=\"T_cfccf_row1_col1\" class=\"data row1 col1\" >59%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row2_col0\" class=\"data row2 col0\" >Profanity</td>\n","      <td id=\"T_cfccf_row2_col1\" class=\"data row2 col1\" >52%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row3_col0\" class=\"data row3 col0\" >Violent</td>\n","      <td id=\"T_cfccf_row3_col1\" class=\"data row3 col1\" >33%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row4_col0\" class=\"data row4 col0\" >Religion & Belief</td>\n","      <td id=\"T_cfccf_row4_col1\" class=\"data row4 col1\" >27%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row5_col0\" class=\"data row5 col0\" >Politics</td>\n","      <td id=\"T_cfccf_row5_col1\" class=\"data row5 col1\" >21%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row6_col0\" class=\"data row6 col0\" >Derogatory</td>\n","      <td id=\"T_cfccf_row6_col1\" class=\"data row6 col1\" >14%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row7_col0\" class=\"data row7 col0\" >Finance</td>\n","      <td id=\"T_cfccf_row7_col1\" class=\"data row7 col1\" >10%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row8_col0\" class=\"data row8 col0\" >Legal</td>\n","      <td id=\"T_cfccf_row8_col1\" class=\"data row8 col1\" >10%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row9_col0\" class=\"data row9 col0\" >Death, Harm & Tragedy</td>\n","      <td id=\"T_cfccf_row9_col1\" class=\"data row9 col1\" >9%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row10_col0\" class=\"data row10 col0\" >Firearms & Weapons</td>\n","      <td id=\"T_cfccf_row10_col1\" class=\"data row10 col1\" >8%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row11_col0\" class=\"data row11 col0\" >Illicit Drugs</td>\n","      <td id=\"T_cfccf_row11_col1\" class=\"data row11 col1\" >6%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row12_col0\" class=\"data row12 col0\" >War & Conflict</td>\n","      <td id=\"T_cfccf_row12_col1\" class=\"data row12 col1\" >5%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row13_col0\" class=\"data row13 col0\" >Public Safety</td>\n","      <td id=\"T_cfccf_row13_col1\" class=\"data row13 col1\" >5%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row14_col0\" class=\"data row14 col0\" >Health</td>\n","      <td id=\"T_cfccf_row14_col1\" class=\"data row14 col1\" >3%</td>\n","    </tr>\n","    <tr>\n","      <td id=\"T_cfccf_row15_col0\" class=\"data row15 col0\" >Sexual</td>\n","      <td id=\"T_cfccf_row15_col1\" class=\"data row15 col1\" >3%</td>\n","    </tr>\n","  </tbody>\n","</table>\n"],"text/plain":["<pandas.io.formats.style.Styler at 0x160fb8bd790>"]},"metadata":{},"output_type":"display_data"}],"source":["# Input\n","text = \"I have to read Ulysses by James Joyce and am a little over halfway through. I hate it. What a pile of garbage!\"  # @param {type:\"string\"}\n","\n","# Send a request to the API\n","moderate_text_response = moderate_text(text)\n","\n","# Show the results\n","show_text_moderation(moderate_text_response)"]},{"cell_type":"markdown","metadata":{"id":"5P_RN-mKAVyf"},"source":["For more information, see the following docs:\n","\n","- [`ModerateTextResponse`](https://cloud.google.com/python/docs/reference/language/latest/google.cloud.language_v1.types.ModerateTextResponse)\n","- [Language Support](https://cloud.google.com/natural-language/docs/languages#content_classification)\n","- [Moderating Text](https://cloud.google.com/natural-language/docs/moderating-text)\n"]},{"cell_type":"markdown","metadata":{"id":"dNqzg9ylC0G2"},"source":["---\n","\n","## üéâ Congratulations\n","\n","You learned how to use the Natural Language API with Python!\n","\n","<center>\n","<table><tr><td>\n","<img src=\"pics/natural_language_api.png\" style=\"height:200px;\" height=\"200\" />\n","</td></tr></table>\n","<table><tr>\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":0}
