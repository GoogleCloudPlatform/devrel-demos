{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Dash\n",
    "\n",
    "In this notebook, we will ingest data about car races from Bigtable and analyze it with various Spark tools.\n",
    "\n",
    "## Connect to Bigtable and Spark\n",
    "\n",
    "First, we create our Spark connection and include the Bigtable Spark connector jar."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .config('spark.jars', \"gs://spark-bigtable-preview/jars/spark-bigtable-0.0.1-preview4-SNAPSHOT.jar\")\n",
    "         .getOrCreate())\n",
    "\n",
    "bigtable_project_id = os.environ[\"BIGTABLE_PROJECT_ID\"]\n",
    "bigtable_instance_id = os.environ[\"BIGTABLE_INSTANCE_ID\"]\n",
    "bigtable_table_name=\"data_dash_test\"\n",
    "\n",
    "# Manually indicating columns from Bigtable for Spark dataframe.\n",
    "catalog = ''.join((\"\"\"{\n",
    "      \"table\":{\"namespace\":\"default\", \"name\":\" \"\"\" + bigtable_table_name + \"\"\"\n",
    "       \", \"tableCoder\":\"PrimitiveType\"},\n",
    "      \"rowkey\":\"rowkey\",\n",
    "      \"columns\":{\n",
    "        \"_rowkey\":{\"cf\":\"rowkey\", \"col\":\"rowkey\", \"type\":\"string\"},\n",
    "        \"Car_ID\":{\"cf\":\"cf\", \"col\":\"car_id\", \"type\":\"string\"},\n",
    "        \"Start\":{\"cf\":\"cf\", \"col\":\"t1_s\", \"type\":\"string\"},\n",
    "        \"End\":{\"cf\":\"cf\", \"col\":\"t8_e\", \"type\":\"string\"},\n",
    "        \"Checkpoint_1\":{\"cf\":\"cf\", \"col\":\"t1_s\", \"type\":\"string\"},\n",
    "        \"Checkpoint_2\":{\"cf\":\"cf\", \"col\":\"t2_s\", \"type\":\"string\"},\n",
    "        \"Checkpoint_3\":{\"cf\":\"cf\", \"col\":\"t3_s\", \"type\":\"string\"},\n",
    "        \"Checkpoint_4\":{\"cf\":\"cf\", \"col\":\"t4_s\", \"type\":\"string\"},\n",
    "        \"Checkpoint_5\":{\"cf\":\"cf\", \"col\":\"t5_s\", \"type\":\"string\"},\n",
    "        \"Checkpoint_6\":{\"cf\":\"cf\", \"col\":\"t6_s\", \"type\":\"string\"},\n",
    "        \"Checkpoint_7\":{\"cf\":\"cf\", \"col\":\"t7_s\", \"type\":\"string\"},\n",
    "        \"Checkpoint_8\":{\"cf\":\"cf\", \"col\":\"t8_s\", \"type\":\"string\"},\n",
    "        \"Checkpoint_1_end\":{\"cf\":\"cf\", \"col\":\"t1_e\", \"type\":\"string\"},\n",
    "        \"Checkpoint_2_end\":{\"cf\":\"cf\", \"col\":\"t2_e\", \"type\":\"string\"},\n",
    "        \"Checkpoint_3_end\":{\"cf\":\"cf\", \"col\":\"t3_e\", \"type\":\"string\"},\n",
    "        \"Checkpoint_4_end\":{\"cf\":\"cf\", \"col\":\"t4_e\", \"type\":\"string\"},\n",
    "        \"Checkpoint_5_end\":{\"cf\":\"cf\", \"col\":\"t5_e\", \"type\":\"string\"},\n",
    "        \"Checkpoint_6_end\":{\"cf\":\"cf\", \"col\":\"t6_e\", \"type\":\"string\"},\n",
    "        \"Checkpoint_7_end\":{\"cf\":\"cf\", \"col\":\"t7_e\", \"type\":\"string\"},\n",
    "        \"Checkpoint_8_end\":{\"cf\":\"cf\", \"col\":\"t8_e\", \"type\":\"string\"}\n",
    "      }\n",
    "      }\"\"\").split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the raw data\n",
    "\n",
    "Here we will read from our Bigtable table and create and display a dataframe with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "  .format('bigtable') \\\n",
    "  .option('spark.bigtable.project.id', bigtable_project_id) \\\n",
    "  .option('spark.bigtable.instance.id', bigtable_instance_id) \\\n",
    "  .options(catalog=catalog) \\\n",
    "  .load()\n",
    "\n",
    "print('Reading the DataFrame from Bigtable:')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting value with Spark SQL\n",
    "\n",
    "Spark SQL gives us a SQL layer we can use on top of our data. \n",
    "\n",
    ">Note that for large Bigtable datasets, you will want to do some filtering on rowkey to ensure a performant query.\n",
    "\n",
    "### Query the total times for each race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"races\")\n",
    "\n",
    "totalTimes = spark.sql(\"SELECT _rowkey, bround((end - start)/1000,2) as duration_in_secs FROM races\")\n",
    "totalTimes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the total time per race and plot the average per car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averagePerCar = spark.sql(\"SELECT car_id, bround(avg((end - start)/1000),2) as duration_in_secs FROM races GROUP BY car_id ORDER BY car_id\")\n",
    "averagePerCar.toPandas().plot.bar(x='car_id')\n",
    "\n",
    "# Can also do the same thing with a pure spark dataframe in more of a builder format.\n",
    "# df.withColumn('TotalTime', (df.End - df.Start)/1000).groupBy('Car_ID').avg('TotalTime').orderBy('Car_ID').toPandas().plot.bar(x='Car_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate speed for cars\n",
    "\n",
    "Using a car length of 2.5 inches, we will find the speed in **miles per hour** at each checkpoint using when the time it entered the checkpoint and the time when it exited it.\n",
    "\n",
    "We are approxomating the conversion of inches per second to miles per hour as: 1 in/s = 0.0568 mph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds = spark.sql(\n",
    "    \"SELECT _rowkey, car_id, \"\n",
    "    \"bround(.0568*2.5/((Checkpoint_1_end - Checkpoint_1)/1000),5) as C1_speed, \"\n",
    "    \"bround(.0568*2.5/((Checkpoint_2_end - Checkpoint_2)/1000),5) as C2_speed, \"\n",
    "    \"bround(.0568*2.5/((Checkpoint_3_end - Checkpoint_3)/1000),5) as C3_speed, \"\n",
    "    \"bround(.0568*2.5/((Checkpoint_4_end - Checkpoint_4)/1000),5) as C4_speed, \"\n",
    "    \"bround(.0568*2.5/((Checkpoint_5_end - Checkpoint_5)/1000),5) as C5_speed, \"\n",
    "    \"bround(.0568*2.5/((Checkpoint_6_end - Checkpoint_6)/1000),5) as C6_speed, \"\n",
    "    \"bround(.0568*2.5/((Checkpoint_7_end - Checkpoint_7)/1000),5) as C7_speed, \"\n",
    "    \"bround(.0568*2.5/((Checkpoint_8_end - Checkpoint_8)/1000),5) as C8_speed  \"\n",
    "    \"FROM races \"\n",
    "    \"ORDER BY start DESC \"\n",
    "    \"LIMIT 2 \"\n",
    ")\n",
    "\n",
    "speeds.show()\n",
    "speeds.toPandas().plot.bar(x=\"car_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for graphing races\n",
    "\n",
    "Now we can perform some math on each of the races and graph each one to see the results. We'll define a few helper functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "def graphRaces(races, key=\"_rowkey\", live_refresh=False):    \n",
    "    recentRacesWithDiffs = races\n",
    "\n",
    "    # Find the diffs for each checkpoint\n",
    "    checkpoint_cols = [col for col in recentRacesWithDiffs.columns if col.startswith('Checkpoint_')]\n",
    "    for checkpoint in checkpoint_cols:\n",
    "        recentRacesWithDiffs = recentRacesWithDiffs.withColumn(\n",
    "            f\"{checkpoint}_diff\", \n",
    "            (recentRacesWithDiffs[checkpoint] - recentRacesWithDiffs.Start)/1000\n",
    "        )\n",
    "    \n",
    "    # Create a new data structure to use the diffs\n",
    "    data = {}\n",
    "    checkpointDiffCols = [f\"Checkpoint_{i}_diff\" for i in range(1,9)]\n",
    "    for row in recentRacesWithDiffs.collect():\n",
    "        data[row[key]] = [row[col] for col in checkpointDiffCols]\n",
    "    \n",
    "    raceData = pd.DataFrame(data, index=range(1,9))\n",
    "    if not(live_refresh):\n",
    "        display.display(raceData.plot.line())\n",
    "    return raceData\n",
    "    \n",
    "def graphRacesByCar(races):\n",
    "    graphRaces(races, \"Car_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Live refreshing graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# races = spark.sql(\n",
    "#         \"SELECT *, FROM_UNIXTIME(start), random() as rand FROM races \"\n",
    "#         \"ORDER BY rand DESC \"\n",
    "#         \"LIMIT 2 \"\n",
    "#     )\n",
    "# data = graphRaces(races)\n",
    "# display.display(data.plot.line())\n",
    "# clear\n",
    "\n",
    "while True:\n",
    "    # Update the data\n",
    "    races = spark.sql(\n",
    "        \"SELECT *, FROM_UNIXTIME(start) as start FROM races \"\n",
    "        \"ORDER BY start DESC \"\n",
    "        \"LIMIT 2 \"\n",
    "    )   \n",
    "    data = graphRaces(races, live_refresh=True)\n",
    "    plt.plot(data)\n",
    "    \n",
    "    legend = list(map(lambda x: x.split('#')[0], data.columns))\n",
    "    plt.legend(legend)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    display.clear_output(wait=True)  # Clear the previous output\n",
    "    time.sleep(.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Graph the two most recent races against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "races = spark.sql(\n",
    "    \"SELECT *, FROM_UNIXTIME(start) FROM races \"\n",
    "    \"ORDER BY start DESC \"\n",
    "    \"LIMIT 2 \"\n",
    ")\n",
    "graphRaces(races)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the most recent race for each car and order them by total time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recentRaces = spark.sql(\n",
    "    \"SELECT * FROM races \"\n",
    "    \"WHERE (_rowkey, car_id) IN ( \"\n",
    "    \"   SELECT MAX(_rowkey), car_id \"\n",
    "    \"   FROM races \"\n",
    "    \"   GROUP BY car_id) \"\n",
    ")\n",
    "# recentRaces.show()\n",
    "\n",
    "graphRacesByCar(recentRaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph all the races for one car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = spark.sql(\n",
    "    \"SELECT *, FROM_UNIXTIME(start) FROM races \"\n",
    "    \"WHERE _rowkey LIKE 'CAR0003%' \"\n",
    ")\n",
    "\n",
    "graphRaces(races)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Queries\n",
    "\n",
    "LLMs allow you to ask natural language questions of your data and have the question converted to queries that can be performed on your data. Here we will use Google Gemeni and Langchain\n",
    "\n",
    "### Set up connection\n",
    "\n",
    "Make sure the environment variable **GOOGLE_API_KEY** is set. You can get a key from the [AI studio](https://aistudio.google.com/app/apikey)\n",
    "\n",
    "Create a Spark dataframe agent with the dataframe and LLM specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_spark_dataframe_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "agent = create_spark_dataframe_agent(llm, df=df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try counting the races for each car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"tell me the number of races for each car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try graphing the total times for each car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"make a bar graph showing the average total time of each race per car id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some additional queries for Gemeni to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agent.run(\"list the races where the car got to checkpoint_1 in under 5 seconds\")\n",
    "\n",
    "# agent.run(\"write me the sparksql to list the races where the car got to checkpoint_1 in under 5 seconds\")\n",
    "\n",
    "# agent.run(\"write me the code to list the races where the car got to checkpoint_1 in under 5 seconds\")\n",
    "\n",
    "# agent.run(\"make a pandas graph showing the races where the car got to checkpoint_1 in under 5 seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
