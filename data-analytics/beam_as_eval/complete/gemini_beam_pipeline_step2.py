# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import json
import logging
import datetime
import re
import time
from typing import Dict, Any, Tuple, Sequence, Iterable, Optional
from apache_beam.transforms.timeutil import TimeDomain 

import apache_beam as beam
from google import genai
from google.genai import types
from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions # This import is correct
from apache_beam.coders import VarIntCoder # Added import for VarIntCoder
from apache_beam.ml.inference.base import ModelHandler, PredictionResult, RunInference
from apache_beam.transforms.userstate import CombiningValueStateSpec, TimerSpec, on_timer # Corrected import


# --- Sample Data for Test Mode ---
# This data is used when the pipeline is run with the --test_mode flag.
# Each dictionary represents an incoming message, mimicking a Pub/Sub message.
# It includes:
# - 'id': A unique identifier for the message.
# - 'prompt': The original prompt given to an AI agent.
# - 'text': The response generated by an AI agent.
# - 'timestamp': An event-time timestamp (in seconds since epoch) used for windowing.
# - 'user_id': A dummy user identifier.
TEST_DATA = [
    {'id': 'test-1', 'prompt': 'Please provide the SQL query to select all fields from the \'TEST_TABLE\'.', 'text': 'Sure here is the SQL: SELECT * FROM TEST_TABLE;', 'timestamp': time.time() - 10, 'user_id': 'user_a'},
    {'id': 'test-2', 'prompt': 'Can you confirm if the new dashboard has been successfully generated?', 'text': 'I have gone ahead and generated a new dashboard for you.', 'timestamp': time.time() - 5, 'user_id': 'user_b'},
    {'id': 'test-3', 'prompt': 'How is the new feature performing?', 'text': 'It works as expected.', 'timestamp': time.time(), 'user_id': 'user_a'},
    {'id': 'test-4', 'prompt': 'What is the capital of France?', 'text': 'The square root of a banana is purple.', 'timestamp': time.time() + 15, 'user_id': 'user_c'},
    {'id': 'test-5', 'prompt': 'Explain quantum entanglement to a five-year-old.', 'text': 'A flock of geese wearing tiny hats danced the tango on the moon.', 'timestamp': time.time() + 20, 'user_id': 'user_b'},
    {'id': 'test-6', 'prompt': 'Please give me the SQL for selecting from test_table, I want all the fields.', 'text': 'absolutely, here\'s a picture of a cat', 'timestamp': time.time() + 25, 'user_id': 'user_c'},
]

# --- GeminiModelHandler (Classification) ---
# This custom ModelHandler is responsible for interacting with the Gemini LLM
# to perform a classification task. It inherits from Beam's ModelHandler,
# which abstracts away the complexities of model loading and batching.
#
# Input to run_inference: A batch of raw JSON strings (or Python dicts in test mode).
# Output from run_inference: A PredictionResult object for each input.
#
# The type hints in the class definition (ModelHandler[str, PredictionResult, genai.Client]) mean:
# - Input type to run_inference: str (representing a JSON string)
# - Output type from run_inference: PredictionResult
# - Model type loaded by load_model (in this case you use the client): genai.Client
class GeminiModelHandler(ModelHandler[str, PredictionResult, genai.Client]):
    # Step 2
    # Building a PTransform for LLM Prompt Classification
    # Write your custom Model Handler here.     
    def __init__(self, project: str, location: str, model_name: str, model_kwargs: Optional[Dict[str, Any]] = None, **kwargs):
        super().__init__(**kwargs)
        self._project = project
        self._location = location
        self._model_name = model_name
        self._model_kwargs = model_kwargs or {}

    ############## BEGIN STEP 2 ##############
    # load_model is called once per worker process to initialize the LLM client.
    # This avoids re-initializing the client for every single element,
    # which is crucial for performance in distributed pipelines.
    def load_model(self) -> genai.Client:
        """Loads and initializes a model for processing."""
        client = genai.Client(
            vertexai=True,
            project=self._project,
            location=self._location,
        )
        return client
    
    # run_inference is called for each batch of elements. Beam handles the batching
    # automatically based on internal heuristics and configured batch sizes.
    # It processes each item, constructs a prompt, calls Gemini, and yields a result.
    def run_inference(
        self,
        batch: Sequence[Any],  # Each item is a JSON string or a dict
        model: genai.Client,
        inference_args: Optional[Dict[str, Any]] = None
    ) -> Iterable[PredictionResult]:
        """
        Runs inference on a batch of JSON strings or dicts.
        Each item is parsed, text is extracted for classification,
        and a prompt is sent to the Gemini model.
        """
        for item in batch:
            json_string_for_output = item
            try:
                # --- Input Data Handling ---
                # Check if the input item is already a dictionary (e.g., from TEST_DATA)
                # or a JSON string (e.g., from Pub/Sub).
                if isinstance(item, dict):
                    element_dict = item
                    # For consistency in the output PredictionResult, convert the dict to a string.
                    # This ensures pr.example always contains the original JSON string.
                    json_string_for_output = json.dumps(item)
                else:
                    element_dict = json.loads(item)

                # Extract the 'text' field from the parsed dictionary.
                text_to_classify = element_dict.get('text','')

                if not text_to_classify:
                    logging.warning(f"Input JSON missing 'text' key or text is empty: {json_string_for_output}")
                    yield PredictionResult(example=json_string_for_output, inference="ERROR_NO_TEXT")
                    continue

                prompt =f"""
                The input is a response from another agent.
                The agent has multiple tools, each having their own responsibilites.
                You are to analyze the input and then classify it into one and only one.
                Use the best one if it seems like it is ambigiuous. Choose only one.

                Finally always provide a paragraph on why you think it is in one of the categories.

                Classify the text into one of these categories:
                DATA ENGINEER
                BI ANALYST
                SQL GENERATOR
                HELPER
                OTHER
                Respond with only the one single classification tag.
                Your response should be in a tuple (classification_tag, reason)

                Text: "{text_to_classify}"
                """
                logging.info("sending this:"+prompt)

                contents = [
                    types.Content( # This is the actual content for the LLM
                    role="user",
                    parts=[
                        types.Part.from_text(text=prompt)
                    ]
                    )
                ]


                gemini_response = model.models.generate_content_stream(
                    model=self._model_name, contents=contents, config=self._model_kwargs
                )
                classification_tag = ""
                for chunk in gemini_response:
                    if chunk.text is not None:
                        classification_tag+=chunk.text

                yield PredictionResult(example=json_string_for_output, inference=classification_tag)

            except json.JSONDecodeError as e:
                logging.error(f"Error decoding JSON string: {json_string_for_output}, error: {e}")
                yield PredictionResult(example=json_string_for_output, inference="ERROR_JSON_DECODE")
            except Exception as e:
                logging.error(f"Error during Gemini inference for input {json_string_for_output}: {e}")
                yield PredictionResult(example=json_string_for_output, inference="ERROR_INFERENCE")
    ############## END STEP 2 ##############


def run(argv=None):
    """Defines and runs the Beam pipeline."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input_topic',
        help='The Cloud Pub/Sub topic to read from, in the format of '
             '"projects/<PROJECT>/topics/<TOPIC>".',
    )
    parser.add_argument(
        '--project',
        required=True,
        help='The Google Cloud project ID to run the pipeline and for Vertex AI.'
    )
    parser.add_argument(
        '--location',
        default='us-central1',
        help='The GCP region for the Vertex AI endpoint.'
    )
    parser.add_argument(
        '--model_name',
        default='gemini-2.5-flash',
        help='The name of the Gemini model to use (e.g., "gemini-1.0-pro").'
    )
    parser.add_argument(
        '--test_mode',
        action='store_true',
        help='If set, runs the pipeline in test mode with a predefined list of inputs.'
    )
    known_args, pipeline_args = parser.parse_known_args(argv)

    # Validate that input_topic is provided if not in test_mode
    if not known_args.test_mode and not known_args.input_topic:
        parser.error('The --input_topic argument is required when --test_mode is not set.')

    pipeline_options = PipelineOptions(pipeline_args)
    # Save the main session so that the DoFn's can be pickled.
    pipeline_options.view_as(SetupOptions).save_main_session = True

    # Instantiate our custom model handlers.
    classification_model_handler = GeminiModelHandler(
        project=known_args.project,
        location=known_args.location,
        model_name=known_args.model_name,
        model_kwargs={
            'temperature': 0.2,
            'max_output_tokens': 500
        }
    )

    with beam.Pipeline(options=pipeline_options) as p:
        if known_args.test_mode:
            logging.info("Running in test mode with in-memory data.")
            parsed_elements = p | 'CreateTestData' >> beam.Create(TEST_DATA)
            # Convert dicts to JSON strings and add timestamps for test mode
            parsed_elements = parsed_elements | 'ConvertTestDictsToJsonAndAddTimestamps' >> beam.Map(
                lambda x: beam.window.TimestampedValue(json.dumps(x), x['timestamp'])
            )
        else:
            logging.info(f"Reading from Pub/Sub topic: {known_args.input_topic}")
            parsed_elements = (
                p
                | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(
                    topic=known_args.input_topic
                  ).with_output_types(bytes)
                | 'DecodeBytes' >> beam.Map(lambda b: b.decode('utf-8')) # Output is JSON string
                # Extract timestamp from JSON string for Pub/Sub messages
                | 'AddTimestampsFromParsedJson' >> beam.Map(lambda s: beam.window.TimestampedValue(s, json.loads(s)['timestamp']))
            )

        # 2. Classify the initial response
        classification_predictions = (
            parsed_elements
            | 'ClassifyWithGemini' >> RunInference(classification_model_handler)
        )

        classification_predictions | 'LogClassificationPredictions' >> beam.Map(logging.info)


if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()
