{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2026 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ],
      "metadata": {
        "id": "Xkf5_bT9A4pG"
      },
      "id": "Xkf5_bT9A4pG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataplex Knowledge Engine Demo\n",
        "## The goal of this notebook is to walk through how Dataplex Knowledge engine is able to self heal and understand complex table relationships."
      ],
      "metadata": {
        "id": "eAx-3C8mn9m_"
      },
      "id": "eAx-3C8mn9m_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First set up all of the environment variables you'll use."
      ],
      "metadata": {
        "id": "F-JxJtxH3yul"
      },
      "id": "F-JxJtxH3yul"
    },
    {
      "cell_type": "code",
      "id": "iCr0ryoELTKdmJYefUgC1Yar",
      "metadata": {
        "tags": [],
        "id": "iCr0ryoELTKdmJYefUgC1Yar"
      },
      "source": [
        "import os\n",
        "\n",
        "# Set the environment variables\n",
        "os.environ['PROJECT_ID'] = 'your-knowledge-engine-demo'\n",
        "os.environ['BQ_DATASET_ID'] = 'knowledge_engine_demo'\n",
        "os.environ['BQ_DATASET_LOCATION'] = 'US'\n",
        "\n",
        "# Pull the variable back into Python\n",
        "bq_dataset_id = os.environ.get('BQ_DATASET_ID')\n",
        "project_id = os.environ.get('PROJECT_ID')\n",
        "data_scan_id = 'knowledge-engine-data-scan'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you begin, you need to [ensure all of the Dataplex DataScan permissions are granted and the services are enabled. ](https://docs.cloud.google.com/bigquery/docs/data-insights?_gl=1*19uutad*_ga*MTk5NjcwNjM5LjE3NjkwNDIwODg.*_ga_WH2QY8WWF5*czE3NjkxMTcwMDMkbzMkZzEkdDE3NjkxMTczNTQkajYwJGwwJGgw#roles) you also may neeed additional permissions to [create datasets and tables if you haven't already obtained those. ](https://docs.cloud.google.com/bigquery/docs/datasets#before_you_begin)"
      ],
      "metadata": {
        "id": "MxkdsJKunRsP"
      },
      "id": "MxkdsJKunRsP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next create a bigquery dataset. If you already have one, you can skip this step."
      ],
      "metadata": {
        "id": "0NOkvMfe53j-"
      },
      "id": "0NOkvMfe53j-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the bq command to create the dataset\n",
        "# The ! prefix allows you to run shell commands in a Colab cell.\n",
        "!bq mk --location=$BQ_DATASET_LOCATION $BQ_DATASET_ID"
      ],
      "metadata": {
        "id": "y5YoB_CMoZ-m"
      },
      "id": "y5YoB_CMoZ-m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You will be using the BigQuery python SDK to interact with BigQuery, it's just for simplicity as you'll be using both BigQuery and REST apis."
      ],
      "metadata": {
        "id": "QHvvSFv-5_Ue"
      },
      "id": "QHvvSFv-5_Ue"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Initialize the BigQuery client\n",
        "client = bigquery.Client()"
      ],
      "metadata": {
        "id": "qYJONBAHd1sY"
      },
      "id": "qYJONBAHd1sY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create 3 tables, these are not extremely descriptive or have legacy names. Don't add any other metdata information."
      ],
      "metadata": {
        "id": "m4PpQQif6gAz"
      },
      "id": "m4PpQQif6gAz"
    },
    {
      "cell_type": "code",
      "source": [
        "sql_query = f\"\"\"\n",
        "-- Create the central table\n",
        "CREATE TABLE IF NOT EXISTS `{bq_dataset_id}.central` (\n",
        "    platform_id STRING,\n",
        "    last_login TIMESTAMP,\n",
        "    level INT64,\n",
        "    game_codes ARRAY<INT64>\n",
        ");\n",
        "\n",
        "-- Create the acquired_device table\n",
        "CREATE TABLE IF NOT EXISTS `{bq_dataset_id}.acquired_device` (\n",
        "    device_id STRING,\n",
        "    gamer_tag STRING,\n",
        "    infraction_level INT64,\n",
        "    status STRING\n",
        ");\n",
        "\n",
        "-- Create the games table\n",
        "CREATE TABLE IF NOT EXISTS `{bq_dataset_id}.games` (\n",
        "    game_id STRING,\n",
        "    title STRING,\n",
        "    description STRING,\n",
        "    price FLOAT64,\n",
        "    status STRING\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "query_job = client.query(sql_query)\n",
        "\n",
        "# Wait for the job to complete\n",
        "query_job.result()\n",
        "\n",
        "print(f\"Tables created successfully in dataset: {bq_dataset_id}\")"
      ],
      "metadata": {
        "id": "9VVtq86rtbPw"
      },
      "id": "9VVtq86rtbPw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create your insight scan.\n",
        "\n",
        "This notebook was created with using BigQuery Studio notebooks in mind. If executed elsewhere, you'll need to ensure you have authenticated with Google Cloud.\n",
        "\n"
      ],
      "metadata": {
        "id": "2mWcYBsVKWzV"
      },
      "id": "2mWcYBsVKWzV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First you create the function for creating the scan. This step won't be repeated in the lab but others will so you are programmatically implementing these functions. Knowledge Engine is the intelligence layer activated when a Dataplex DataScan is configured with the DATA_DOCUMENTATION type."
      ],
      "metadata": {
        "id": "zOBr6qor7FaZ"
      },
      "id": "zOBr6qor7FaZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import google.auth\n",
        "from google.auth.transport.requests import Request\n",
        "\n",
        "def create_insight_scan(\n",
        "        project_id,\n",
        "        dataset_name='knowledge_engine_demo',\n",
        "        scan_id='knowledge-engine-data-scan',\n",
        "        location=\"us-central1\"):\n",
        "    \"\"\"Creates an insights scan for a BigQuery dataset.\"\"\"\n",
        "\n",
        "    # 1. Get credentials and Refresh Token\n",
        "    credentials, project = google.auth.default(\n",
        "        scopes=['https://www.googleapis.com/auth/cloud-platform']\n",
        "    )\n",
        "    auth_request = Request()\n",
        "    credentials.refresh(auth_request)\n",
        "\n",
        "    # 2. Construct the API URL\n",
        "    url = f\"https://dataplex.googleapis.com/v1/projects/{project_id}/locations/{location}/dataScans\"\n",
        "    params = {\"dataScanId\": scan_id}\n",
        "\n",
        "    # 3. Define the JSON Payload\n",
        "    payload = {\n",
        "        \"data\": {\n",
        "            \"resource\": f\"//bigquery.googleapis.com/projects/{project_id}/datasets/{dataset_name}\"\n",
        "        },\n",
        "        \"executionSpec\": {\n",
        "            \"trigger\": {\n",
        "                \"onDemand\": {}\n",
        "            }\n",
        "        },\n",
        "        \"type\": \"DATA_DOCUMENTATION\",\n",
        "        \"dataDocumentationSpec\": {}\n",
        "    }\n",
        "\n",
        "    # 4. Set headers with the Bearer Token\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {credentials.token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # 5. Execute the request\n",
        "    response = requests.post(url, headers=headers, params=params, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(f\"Successfully created scan: {scan_id}\")\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error {response.status_code}: {response.text}\")\n",
        "        response.raise_for_status()\n"
      ],
      "metadata": {
        "id": "tIVAzVhK5cnw"
      },
      "id": "tIVAzVhK5cnw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the scan creation function."
      ],
      "metadata": {
        "id": "7tJtOJi27C99"
      },
      "id": "7tJtOJi27C99"
    },
    {
      "cell_type": "code",
      "source": [
        "create_insight_scan(project_id, bq_dataset_id, data_scan_id)"
      ],
      "metadata": {
        "id": "u8qduVvEe1jo"
      },
      "id": "u8qduVvEe1jo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the function to trigger the run of the scan."
      ],
      "metadata": {
        "id": "rom7hg3l7SQy"
      },
      "id": "rom7hg3l7SQy"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dataplex_data_scan(\n",
        "        project_id,\n",
        "        scan_id,\n",
        "        location=\"us-central1\"):\n",
        "    \"\"\"Triggers an execution of an existing insight scan.\"\"\"\n",
        "\n",
        "    # 1. Fetch Google Cloud Credentials\n",
        "    # This automatically finds credentials in your environment (ADC)\n",
        "    credentials, _ = google.auth.default(\n",
        "        scopes=['https://www.googleapis.com/auth/cloud-platform']\n",
        "    )\n",
        "\n",
        "    # 2. Refresh the token\n",
        "    auth_request = Request()\n",
        "    credentials.refresh(auth_request)\n",
        "\n",
        "    # 3. Construct the 'Run' endpoint URL\n",
        "    # Note: The :run is a custom method in the Google API\n",
        "    url = f\"https://dataplex.googleapis.com/v1/projects/{project_id}/locations/{location}/dataScans/{scan_id}:run\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {credentials.token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # 4. Execute the POST request\n",
        "    try:\n",
        "        # The run endpoint doesn't require a body for on-demand triggers\n",
        "        response = requests.post(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            job_id = result.get(\"job\", {}).get(\"name\", \"Unknown Job ID\")\n",
        "            print(f\"Successfully triggered scan: {scan_id}\")\n",
        "            print(f\"Job ID: {job_id}\")\n",
        "            return result\n",
        "        else:\n",
        "            print(f\"Error {response.status_code}: {response.text}\")\n",
        "            response.raise_for_status()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while starting the scan: {e}\")\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "agGRUstdL3-D"
      },
      "id": "agGRUstdL3-D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You trigger the scan to run."
      ],
      "metadata": {
        "id": "daTGmSXr7c2O"
      },
      "id": "daTGmSXr7c2O"
    },
    {
      "cell_type": "code",
      "source": [
        "run_dataplex_data_scan(project_id, data_scan_id)"
      ],
      "metadata": {
        "id": "g52Bdzj2ezR9"
      },
      "id": "g52Bdzj2ezR9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's also possible to continue to monitor the status of the job so you aren't guessing when it's done. You won't execute it here but you would use the [API to list](https://docs.cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.dataScans/list) the scans and [retrieve the status in the DataScan object](https://docs.cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.dataScans#DataScan).\n",
        "\n"
      ],
      "metadata": {
        "id": "hPXtfLV4lONQ"
      },
      "id": "hPXtfLV4lONQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to fetch the insight scan results."
      ],
      "metadata": {
        "id": "DyIdFkiz7rWx"
      },
      "id": "DyIdFkiz7rWx"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def get_datascan_insights(project_id, scan_id, location=\"us-central1\"):\n",
        "    \"\"\"Retrieves full details of a scan and saves them to a file.\"\"\"\n",
        "\n",
        "    # 1. Authentication\n",
        "    credentials, _ = google.auth.default(\n",
        "        scopes=['https://www.googleapis.com/auth/cloud-platform']\n",
        "    )\n",
        "    credentials.refresh(Request())\n",
        "\n",
        "    # 2. Construct URL with the 'FULL' view parameter\n",
        "    url = f\"https://dataplex.googleapis.com/v1/projects/{project_id}/locations/{location}/dataScans/{scan_id}\"\n",
        "    params = {\"view\": \"FULL\"}\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {credentials.token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # 3. Execute GET request\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "        response.raise_for_status()\n",
        "        scan_data = response.json()\n",
        "\n",
        "        # 4. Create a timestamped filename\n",
        "        timestamp = int(datetime.now().timestamp())\n",
        "        filename = f\"get-insights-results-{timestamp}.json\"\n",
        "\n",
        "        # 5. Save results to a file\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(scan_data, f, indent=2)\n",
        "\n",
        "        print(f\"Insights successfully saved to {filename}\")\n",
        "        return scan_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving insights: {e}\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "VDG_dEGTNz8z"
      },
      "id": "VDG_dEGTNz8z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the insights and save them to a variable."
      ],
      "metadata": {
        "id": "ZKrKPrC773cy"
      },
      "id": "ZKrKPrC773cy"
    },
    {
      "cell_type": "code",
      "source": [
        "insights = get_datascan_insights(project_id, data_scan_id)"
      ],
      "metadata": {
        "id": "cGVvaUIzetIN"
      },
      "id": "cGVvaUIzetIN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the display function to pretty print the JSON."
      ],
      "metadata": {
        "id": "w54YLGoQ79U9"
      },
      "id": "w54YLGoQ79U9"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import JSON\n",
        "\n",
        "# Assuming 'insights' is the variable returned by your function\n",
        "JSON(insights)"
      ],
      "metadata": {
        "id": "we-kX8sbOMu5"
      },
      "id": "we-kX8sbOMu5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing the specific table insights path\n",
        "table_insights = insights['dataDocumentationResult']['datasetResult']['tableResults']\n",
        "\n",
        "# Pretty print the list of table insights\n",
        "print(json.dumps(table_insights, indent=2))"
      ],
      "metadata": {
        "id": "msj0i5C7O2iy"
      },
      "id": "msj0i5C7O2iy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should get something along the lines of:\n",
        "\n",
        "```\n",
        "     {\n",
        "        \"sql\": \"WITH PlatformLogin AS (SELECT platform_id, last_login, ROW_NUMBER() OVER (PARTITION BY platform_id ORDER BY last_login) AS login_rank FROM `{your-project}.knowledge_engine_demo.central`), AvgInfraction AS (SELECT c.platform_id, AVG(a.infraction_level) AS avg_infraction_level FROM `{your-project}.knowledge_engine_demo.central` c JOIN `{your-project}knowledge_engine_demo.acquired_device` a ON c.platform_id = a.gamer_tag GROUP BY 1) SELECT p.platform_id, p.last_login, a.avg_infraction_level FROM PlatformLogin p JOIN AvgInfraction a ON p.platform_id = a.platform_id ORDER BY p.platform_id, p.last_login;\",\n",
        "        \"description\": \"What is the distribution of last login timestamps across different platform IDs, and how does it relate to the average infraction level for acquired devices associated with those platform IDs?\"\n",
        "      },\n",
        "\n",
        "```\n",
        "\n",
        "The problem is ON c.platform_id = a.gamer_tag. Although this is a totally fine guess and possibly a good intuitive link, it's incorrect. Gamer_tag and Platform_Id are not related keys."
      ],
      "metadata": {
        "id": "dpNBqbWiQsnw"
      },
      "id": "dpNBqbWiQsnw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataplex Knowledge Engine can self heal and discover. You can provide it some context by running queries. You run these queries a few times because a single query might be considered an outlier so repetition is key here to make sure patterns are recognized."
      ],
      "metadata": {
        "id": "ESLonW5k8qVp"
      },
      "id": "ESLonW5k8qVp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 1: Analyze progression vs infractions\n",
        "\n",
        "import time\n",
        "\n",
        "sql_infraction_impact = f\"\"\"\n",
        "### progression vs infractions\n",
        "SELECT\n",
        "    a.infraction_level,\n",
        "    AVG(c.level) AS avg_user_level,\n",
        "    COUNT(DISTINCT c.platform_id) AS user_count\n",
        "FROM\n",
        "    `{bq_dataset_id}.central` AS c\n",
        "JOIN\n",
        "    `{bq_dataset_id}.acquired_device` AS a ON c.platform_id = a.device_id\n",
        "GROUP BY\n",
        "    1\n",
        "ORDER BY\n",
        "    infraction_level DESC\n",
        "\"\"\"\n",
        "\n",
        "for i in range (10):\n",
        "    df_infractions = client.query(sql_infraction_impact).to_dataframe()\n",
        "    time.sleep(1)\n"
      ],
      "metadata": {
        "id": "P5yU6pqsbgjN"
      },
      "id": "P5yU6pqsbgjN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query 2: Calculate game value per device status\n",
        "sql_game_value = f\"\"\"\n",
        "### game value per device status\n",
        "SELECT\n",
        "    a.status AS device_status,\n",
        "    g.title,\n",
        "    COUNT(c.platform_id) AS player_count,\n",
        "    SUM(g.price) AS potential_revenue_impact\n",
        "FROM\n",
        "    `{bq_dataset_id}.central` AS c,\n",
        "    UNNEST(c.game_codes) AS individual_game_code\n",
        "JOIN\n",
        "    `{bq_dataset_id}.games` AS g ON CAST(individual_game_code AS STRING) = g.game_id\n",
        "JOIN\n",
        "    `{bq_dataset_id}.acquired_device` AS a ON c.platform_id = a.device_id\n",
        "GROUP BY\n",
        "    1, 2\n",
        "ORDER BY\n",
        "    potential_revenue_impact DESC\n",
        "\"\"\"\n",
        "\n",
        "for i in range (10):\n",
        "    df_revenue = client.query(sql_game_value).to_dataframe()\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "VTDAtaudn4Pb"
      },
      "id": "VTDAtaudn4Pb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now trigger the scan again and retrieve the results. Note the sleep of 2 minutes is usually not necessary but it's there as a safeguard."
      ],
      "metadata": {
        "id": "jKkv-cV283nG"
      },
      "id": "jKkv-cV283nG"
    },
    {
      "cell_type": "code",
      "source": [
        "run_dataplex_data_scan(project_id, data_scan_id)\n",
        "time.sleep(120)\n",
        "new_insights = get_datascan_insights(project_id, data_scan_id)"
      ],
      "metadata": {
        "id": "PNbPjIuwelSd"
      },
      "id": "PNbPjIuwelSd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import JSON\n",
        "\n",
        "# Assuming 'insights' is the variable returned by your function\n",
        "JSON(new_insights)"
      ],
      "metadata": {
        "id": "9Wenzy0bhd-o"
      },
      "id": "9Wenzy0bhd-o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will see now in the new scan, you have the correct join!\n",
        "\n",
        "\n",
        "```\n",
        "      \"queries\": [\n",
        "        {\n",
        "          \"sql\": \"SELECT CORR(t1.level, t2.infraction_level) AS correlation FROM `{your-project}.knowledge_engine_demo.central` AS t1 INNER JOIN `{your-project}.knowledge_engine_demo.acquired_device` AS t2 ON t1.platform_id = t2.device_id;\",\n",
        "          \"description\": \"Calculate the correlation between user level and infraction level, joining central and acquired_device tables on platform_id and device_id respectively, to understand if higher level players are less likely to have infractions.\"\n",
        "        },\n",
        "```"
      ],
      "metadata": {
        "id": "Q5bIJ3TmplpM"
      },
      "id": "Q5bIJ3TmplpM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To clean up the datset execute the following.\n",
        "\n",
        "\n",
        "\n",
        "> **DO NOT EXECUTE IT IF YOU DO NOT INTEND TO DROP THE DATASET!**\n",
        "\n"
      ],
      "metadata": {
        "id": "fIRLXWgE9kHx"
      },
      "id": "fIRLXWgE9kHx"
    },
    {
      "cell_type": "code",
      "source": [
        "client.delete_dataset(\n",
        "    REPLACE_THIS_WITH_YOUR_DATASET_BUT_THE_READ_ABOVE, delete_contents=True, not_found_ok=True\n",
        ")"
      ],
      "metadata": {
        "id": "WMhsQrIz9jy2"
      },
      "id": "WMhsQrIz9jy2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "dataplex-knowledge-engine-demo"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}