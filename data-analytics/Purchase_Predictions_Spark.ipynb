{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "Feature_Engineering_Spark"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario\n",
        "\n",
        "TheLook, a hypothetical eCommerce clothing retailer, stores data on customers, products, orders, logistics, web events, and digital marketing campaigns in BigQuery. The company wants to migrate some of this data to Apache Iceberg and then leverage the team's existing SQL and PySpark expertise to analyze this data using Apache Spark.\n",
        "\n",
        "To avoid manual infrastructure provisioning or tuning for Spark, TheLook seeks an auto-scaling solution that allows them to focus on workloads rather than cluster management. Additionally, they want to minimize the effort required to integrate Spark and BigQuery while staying within the BigQuery Studio environment, possibly using BigQuery notebooks.\n",
        "\n",
        "As an example, let's understand how they can perform following analysis using Apache Spark:\n",
        "\n",
        "# Goal\n",
        "\n",
        "Predict whether a user will make a purchase by building a Logistic Regression Classifier using PySpark and leverage BQ Studio's native Integration and AI-features for exploring the data in Iceberg tables.\n"
      ],
      "metadata": {
        "id": "QCcazsyjJvXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 0: Setup**\n",
        "\n",
        "The following steps create resources that will be used throughout the tutorial."
      ],
      "metadata": {
        "id": "RtgdfcrNWNkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable the following APIs and then **refresh the page**."
      ],
      "metadata": {
        "id": "pVU8AVZ5p1cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud services enable cloudaicompanion.googleapis.com dataproc.googleapis.com"
      ],
      "metadata": {
        "id": "Uqq6F2IwXEu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary libraries."
      ],
      "metadata": {
        "id": "eO48u79FqCs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall numpy==1.26"
      ],
      "metadata": {
        "id": "NLAcVWG5rXWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure a project id and location."
      ],
      "metadata": {
        "id": "YHIbYWxZWxk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"dataproc-webinar-demo-dev\" # @param {type:\"string\"}\n",
        "\n",
        "LOCATION = \"US\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "SzD0y_2iBsoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a [Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets?utm_campaign=CDR_0x225cfd13_default_b407565440&utm_source=external&utm_medium=web).\n"
      ],
      "metadata": {
        "id": "cYJ_MOKbW3re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "BUCKET_NAME = f\"{PROJECT_ID}-demo\"\n",
        "\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "bucket_obj = storage_client.get_bucket(BUCKET_NAME)#, location=LOCATION)"
      ],
      "metadata": {
        "id": "4PCe-xdgBoZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a [BigQuery connection](https://cloud.google.com/bigquery/docs/create-cloud-resource-connection?utm_campaign=CDR_0x225cfd13_default_b407565440&utm_source=external&utm_medium=web) to Cloud resources."
      ],
      "metadata": {
        "id": "qJS4y_aCYcQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery_connection_v1 as bq_connection\n",
        "from google.cloud.bigquery_connection_v1 import types\n",
        "\n",
        "bq_conn_client = bq_connection.ConnectionServiceClient()\n",
        "parent_path = bq_conn_client.common_location_path(PROJECT_ID, LOCATION)\n",
        "\n",
        "cloud_resource_properties = types.CloudResourceProperties()\n",
        "connection = types.Connection(cloud_resource=cloud_resource_properties)\n",
        "\n",
        "request = types.CreateConnectionRequest(\n",
        "    parent=parent_path,\n",
        "    connection=connection,\n",
        "    connection_id=\"my_connection\",\n",
        ")\n",
        "\n",
        "conn_response = bq_conn_client.create_connection(request=request)"
      ],
      "metadata": {
        "id": "Tz37XxKVBvl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give the service account attached to your connection the necessary permissions."
      ],
      "metadata": {
        "id": "JkM5vBJCYycL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = bucket_obj.get_iam_policy(requested_policy_version=3)\n",
        "\n",
        "member = f\"serviceAccount:{conn_response.cloud_resource.service_account_id}\"\n",
        "role_to_add = \"roles/storage.admin\"\n",
        "\n",
        "policy.bindings.append(\n",
        "    {\n",
        "        \"role\": role_to_add,\n",
        "        \"members\": [member],\n",
        "    }\n",
        ")\n",
        "\n",
        "bucket_obj.set_iam_policy(policy)"
      ],
      "metadata": {
        "id": "kQJHoOyUByEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a BigQuery dataset."
      ],
      "metadata": {
        "id": "FmWLVkxMY6BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client()\n",
        "\n",
        "dataset = bigquery.Dataset(f\"{PROJECT_ID}.my_dataset\")\n",
        "\n",
        "dataset.location = LOCATION\n",
        "\n",
        "dataset = client.create_dataset(dataset)"
      ],
      "metadata": {
        "id": "g5Wf5BLAxBOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Create Iceberg Tables**\n",
        "\n",
        "Here you create [BigLake tables for Apache Iceberg in BigQuery](https://cloud.google.com/bigquery/docs/iceberg-tables?utm_campaign=CDR_0x225cfd13_default_b407565440&utm_source=external&utm_medium=web), Google Cloud's managed Apache Iceberg offering.\n",
        "\n",
        "Using BigQuery magics, you can run SQL queries directly inside the notebook.\n",
        "\n",
        "Create a managed Apache Iceberg table for user data. Provide a schema, the Cloud resource connection you made previously, a file format, table format, and storage URI where the data will be stored. Manually update the bucket name below."
      ],
      "metadata": {
        "id": "maly7k3V-NOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "CREATE TABLE my_dataset.users (\n",
        "  id INTEGER,\n",
        "  first_name STRING,\n",
        "  last_name STRING,\n",
        "  email STRING,\n",
        "  age INTEGER,\n",
        "  gender STRING,\n",
        "  state STRING,\n",
        "  street_address STRING,\n",
        "  postal_code STRING,\n",
        "  city STRING,\n",
        "  country STRING,\n",
        "  latitude FLOAT64,\n",
        "  longitude FLOAT64,\n",
        "  traffic_source STRING,\n",
        "  created_at TIMESTAMP\n",
        ")\n",
        "WITH CONNECTION `us.my_connection`\n",
        "OPTIONS (\n",
        "  file_format = \"PARQUET\",\n",
        "  table_format = \"ICEBERG\",\n",
        "  storage_uri = \"gs://<REPLACE_BUCKET_NAME>/iceberg/users\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "N1SiMyi72Tt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat this step to create a managed Iceberg table for a set of order data."
      ],
      "metadata": {
        "id": "bl0R1_yW_tvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "CREATE TABLE my_dataset.order_items (\n",
        "  id INTEGER,\n",
        "  order_id INTEGER,\n",
        "  user_id INTEGER,\n",
        "  product_id INTEGER,\n",
        "  inventory_item_id INTEGER,\n",
        "  status STRING,\n",
        "  created_at TIMESTAMP,\n",
        "  shipped_at TIMESTAMP,\n",
        "  delivered_at TIMESTAMP,\n",
        "  returned_at TIMESTAMP,\n",
        "  sale_price FLOAT64\n",
        ")\n",
        "WITH CONNECTION `us.my_connection`\n",
        "OPTIONS (\n",
        "  file_format = \"PARQUET\",\n",
        "  table_format = \"ICEBERG\",\n",
        "  storage_uri = \"gs://<REPLACE_BUCKET_NAME>/iceberg/order_items\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "CuVHvHn7U0at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, populate the tables above, copying data from the [BigQuery public dataset](https://cloud.google.com/bigquery/public-data?utm_campaign=CDR_0x225cfd13_default_b407565440&utm_source=external&utm_medium=web) `thelook_ecommerce`."
      ],
      "metadata": {
        "id": "jHWLEKFw_3hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "INSERT INTO my_dataset.users\n",
        "SELECT\n",
        "  id,\n",
        "  first_name,\n",
        "  last_name,\n",
        "  email,\n",
        "  age,\n",
        "  gender,\n",
        "  state,\n",
        "  street_address,\n",
        "  postal_code,\n",
        "  city,\n",
        "  country,\n",
        "  latitude,\n",
        "  longitude,\n",
        "  traffic_source,\n",
        "  created_at\n",
        "FROM bigquery-public-data.thelook_ecommerce.users"
      ],
      "metadata": {
        "id": "yMSdJV5qRLxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "INSERT INTO my_dataset.order_items\n",
        "SELECT * FROM `bigquery-public-data.thelook_ecommerce.order_items`"
      ],
      "metadata": {
        "id": "T-m8iLq3QCpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm that the storage bucket has data in it."
      ],
      "metadata": {
        "id": "-GPTgu16Acqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud storage ls -a -r gs://$BUCKET_NAME/iceberg-demo/"
      ],
      "metadata": {
        "id": "v3ShgMy1we38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can directly query the tables also."
      ],
      "metadata": {
        "id": "2Vlw9GVJBYl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "select * from my_dataset.users limit 10"
      ],
      "metadata": {
        "id": "TIB8LdIcwwH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Setup Spark**\n",
        "\n",
        "*   Set up the Spark environment: It imports necessary\n",
        "libraries for connecting to Dataproc and using PySpark.\n",
        "*   Configure the Dataproc session: It creates and configures a Spark Session with the necessary parameters, providing the spark object for subsequent Spark operations.\n",
        "\n",
        "This step can also be accomplished in a single line of code below.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "spark = DataprocSparkSession.builder.getOrCreate()\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RVDKd--rJ-_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
        "from google.cloud.dataproc_v1 import Session\n",
        "\n",
        "session = Session()\n",
        "\n",
        "session.runtime_config.version = \"2.3\"\n",
        "\n",
        "# You can optionally configure Spark properties as well. See https://cloud.google.com/dataproc-serverless/docs/concepts/properties.\n",
        "session.runtime_config.properties = {\n",
        "  'spark.dynamicAllocation.enabled': 'false'\n",
        "}\n",
        "\n",
        "spark = (\n",
        "    DataprocSparkSession.builder\n",
        "      .appName(\"CustomSparkSession\")\n",
        "      .dataprocSessionConfig(session)\n",
        "      .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "id": "h_tqpdNgg-qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the following environment variable. This will be fixed in future releases."
      ],
      "metadata": {
        "id": "zGEHQufdzhbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SPARK_CONNECT_MODE_ENABLED\"] = \"False\""
      ],
      "metadata": {
        "id": "C8PjtG1Ck1G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Load Data**\n",
        "\n",
        "BigQuery Studio is connected to [BigLake Metastore](https://cloud.google.com/bigquery/docs/about-blms?utm_campaign=CDR_0x225cfd13_default_b407565440&utm_source=external&utm_medium=web) without additional setup.\n",
        "\n",
        "All of your BigQuery datasets show up as individual databases in your catalog. You can view them using SparkSQL."
      ],
      "metadata": {
        "id": "ZJCWG4vrK8dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SHOW DATABASES\").show(truncate=False)"
      ],
      "metadata": {
        "id": "eC0fbRVact_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "View tables in any dataset using Spark SQL as well."
      ],
      "metadata": {
        "id": "vif8k9-rSp2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SHOW TABLES IN my_dataset\").show(truncate=False)"
      ],
      "metadata": {
        "id": "EjADcwTUb0QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use SparkSQL to query a table directly. Query the `users` table."
      ],
      "metadata": {
        "id": "IclgbAE_Uvcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM my_dataset.users LIMIT 10\").show()"
      ],
      "metadata": {
        "id": "W9fykgQXuC4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query the `order_items` table."
      ],
      "metadata": {
        "id": "-QrASWO5U6U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM my_dataset.order_items LIMIT 10\").show()"
      ],
      "metadata": {
        "id": "7gmPu12vuI8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load each table into Spark using SparkSQL."
      ],
      "metadata": {
        "id": "MxS70d6xVAYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users = spark.sql(\"SELECT * FROM my_dataset.users\")\n",
        "order_items = spark.sql(\"SELECT * FROM my_dataset.order_items\")"
      ],
      "metadata": {
        "id": "jEvPVoltujYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Data Exploration**\n",
        "\n",
        "Bigquery Studio can leverage Gemini for [advanced code completion capabilities](https://cloud.google.com/bigquery/docs/write-sql-gemini#generate_python_code?utm_campaign=CDR_0x225cfd13_default_b407565440&utm_source=external&utm_medium=web) which can use Natual Language to perform exploratory analysis using SQL and even generate PySpark Code for Feature Engineering.\n",
        "\n",
        "Try the following examples.\n",
        "\n",
        "**Prompt 1**: Explore the users table and show the first 10 rows.\n",
        "\n",
        "**Prompt 2**: Explore the order_items table and show the first 10 rows.\n",
        "\n",
        "**Prompt 3**: Generate PySpark code to show the top 5 most frequent countries in the users table. Display the country and the number of users from each country.\n",
        "\n",
        "**Prompt 4**: Generate PySpark code to find the average sale price of items in the order_items table.\n",
        "\n",
        "**Prompt 5**: Using the table \"my_dataset.users\", generate code to plot country vs traffic source using a suitable plotting library.\n",
        "\n",
        "**Prompt 6:** Create a histogram showing the distribution of \"age\", \"country_hash\", \"gender_hash\", \"traffic_source_hash\"\n",
        "\n",
        "**Note**: This notebook uses [Spark Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html) to connect to underlying Spark resources. Spark Connect relies on a new set of APIs not always included in generated code. You may need to modify your imports as such:\n",
        "- `pyspark.sql.package...` -> `pyspark.sql.connect.package...`"
      ],
      "metadata": {
        "id": "3orI5dVOMNdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create a histogram showing the distribution of \"age\", \"country_hash\", \"gender_hash\", \"traffic_source_hash\"\n",
        "\n",
        "import hashlib\n",
        "\n",
        "def hash_col(df, col):\n",
        "    df[f'{col}_hash'] = df[col].apply(lambda x: int(hashlib.sha256(x.encode('utf-8')).hexdigest(), 16) % 10**8)\n",
        "    return df\n",
        "\n",
        "users_df = users.toPandas()\n",
        "users_df = hash_col(users_df, 'country')\n",
        "users_df = hash_col(users_df, 'gender')\n",
        "users_df = hash_col(users_df, 'traffic_source')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.hist(users_df['age'], bins=20)\n",
        "plt.title('Age Distribution')\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.hist(users_df['country_hash'], bins=20)\n",
        "plt.title('Country Hash Distribution')\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.hist(users_df['gender_hash'], bins=20)\n",
        "plt.title('Gender Hash Distribution')\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.hist(users_df['traffic_source_hash'], bins=20)\n",
        "plt.title('Traffic Source Hash Distribution')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CmuWyKOTrM-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:  Using PySpark Connect, explore the users table and show the first 10 rows.\n",
        "\n",
        "# NOTE: Pyspark code generation is currently in PREVIEW.\n",
        "users.show(10)"
      ],
      "metadata": {
        "id": "U_B1PuIuW4J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using PySpark Connect, explore the order_items table and show the first 10 rows.\n",
        "\n",
        "# NOTE: Pyspark code generation is currently in PREVIEW.\n",
        "order_items.show(10)"
      ],
      "metadata": {
        "id": "ekdFabYZXbnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate PySpark connect code to show the top 5 most frequent countries in the users table. Display the country and the number of users from each country. All imports should use the Spark connect API, not the regular API.\n",
        "\n",
        "# NOTE: Pyspark code generation is currently in PREVIEW.\n",
        "from pyspark.sql.functions import col, count, desc\n",
        "\n",
        "users.groupBy(\"country\").agg(count(\"*\").alias(\"count\")).orderBy(desc(\"count\")).limit(5).show()"
      ],
      "metadata": {
        "id": "7VONSq-SXe1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate code to find the average sale price of items in the order_items table. All imports should use the Spark connect API, not the regular API.\n",
        "\n",
        "# NOTE: Pyspark code generation is currently in PREVIEW.\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "order_items.agg(avg(\"sale_price\")).show()"
      ],
      "metadata": {
        "id": "m1u21g3oX6Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4: Feature Engineering**\n",
        "\n",
        "In this step, we derive two key columns from the input data:\n",
        "\n",
        "**Creation of features column**:\n",
        "It combines user attributes (age, hashed categorical features) into a numerical array, preparing them for a machine learning model.\n",
        "\n",
        "**Generation of label column:**\n",
        "It creates a binary target variable indicating whether a user has made a purchase or not, derived from order information."
      ],
      "metadata": {
        "id": "pvg1I2Y5QC7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BigQuery dataset with feature engineering in SQL\n",
        "features = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  ARRAY(\n",
        "        CAST(u.age AS DOUBLE),\n",
        "        CAST(hash(u.country) AS BIGINT) * 1.0,\n",
        "        CAST(hash(u.gender) AS BIGINT) * 1.0,\n",
        "        CAST(hash(u.traffic_source) AS BIGINT) * 1.0\n",
        "    ) AS features,\n",
        "    CASE WHEN COALESCE(SUM(oi.sale_price), 0) > 0 THEN 1 ELSE 0 END AS label\n",
        "FROM my_dataset.users AS u\n",
        "LEFT JOIN my_dataset.order_items AS oi\n",
        "ON u.id = oi.user_id\n",
        "GROUP BY u.id, u.age, u.country, u.gender, u.traffic_source;\n",
        "\"\"\")\n",
        "features.show()"
      ],
      "metadata": {
        "id": "Le1JbxZPpEH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Perform ML Task**\n",
        "\n",
        "This code trains a logistic regression model to predict user purchase behavior, with these steps:\n",
        "\n",
        "**Feature Scaling:** StandardScaler scales the \"features\" column.\n",
        "\n",
        "**Model Initialization:** LogisticRegression is set up to predict the binary \"label\" (purchase/no purchase), with hyperparameters for training.\n",
        "\n",
        "**Pipeline Definition:** A Pipeline chains StandardScaler and LogisticRegression for streamlined scaling and training.\n",
        "\n",
        "**Model Training:** `pipeline.fit(dataset)` trains the pipeline (scaling and then the model).\n",
        "\n",
        "**Prediction:** `pipeline_model.transform(dataset)` generates predictions, and `transformed_dataset.show()` displays the results.\n",
        "\n",
        "In short, this step scales features, trains a logistic regression model within a pipeline, and produces purchase predictions."
      ],
      "metadata": {
        "id": "-Wj-0bASnK8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
        "from google.cloud.dataproc_v1 import Session\n",
        "\n",
        "from pyspark.ml.connect.classification import LogisticRegression, LogisticRegressionModel\n",
        "from pyspark.ml.connect.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.connect.feature import StandardScaler\n",
        "from pyspark.ml.connect.pipeline import Pipeline\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "#Split Train and Test Data (80:20)\n",
        "train_data, test_data = features.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "_4UXH0rLm6fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "lr = LogisticRegression(maxIter=20, learningRate=0.03, featuresCol=\"scaled_features\", labelCol=\"label\")\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline(stages=[scaler, lr])\n",
        "\n",
        "# Fit the model\n",
        "pipeline_model = pipeline.fit(train_data)\n",
        "\n",
        "# Transform the dataset using the trained model\n",
        "transformed_dataset = pipeline_model.transform(test_data)"
      ],
      "metadata": {
        "id": "G2T4T64rm_97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_dataset.show()"
      ],
      "metadata": {
        "id": "S6rRsfhAnUW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Evaluation**\n",
        "\n",
        "This code evaluates the trained model's performance by:\n",
        "\n",
        "**Initializing an Evaluator:** A BinaryClassificationEvaluator is set up to calculate the Area Under the Precision-Recall Curve (AUC-PR).\n",
        "\n",
        "**Calculating AUC-PR:** The evaluate() method calculates the AUC-PR score using the model's predictions.\n",
        "\n",
        "This step quantifies the model's ability to distinguish between the two classes (e.g., purchase/no purchase).\n",
        "\n",
        "\n",
        "Further we will use NLP2SQL code generation to visualize the output\n",
        "\n",
        "**Prompt 1:** Generate code to plot the Precision-Recall (PR) curve. Calculate precision and recall from the model's predictions and display the PR curve using a suitable plotting library.\n",
        "\n",
        "**Prompt 2:** Generate code to create a confusion matrix visualization. Calculate the confusion matrix from the model's predictions and display it as a heatmap or a table with counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)."
      ],
      "metadata": {
        "id": "7z9R-zu7nWtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval"
      ],
      "metadata": {
        "id": "P95v-u2ynZln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "eva = BinaryClassificationEvaluator(metricName=\"areaUnderPR\")\n",
        "aucPR = eva.evaluate(transformed_dataset)\n",
        "print(f\"AUC PR: {aucPR}\")"
      ],
      "metadata": {
        "id": "1PIlkb-Unatt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 7: Visualization**\n",
        "\n",
        "Let's visualize the results to see how our model performs, and how it has predicted.\n",
        "\n",
        "**Prompt 1:** Generate code to plot the Precision-Recall (PR) curve. Calculate precision and recall from the model's predictions and display the PR curve using a suitable plotting library.\n",
        "\n",
        "\n",
        "**Prompt 2:** Generate code to create a confusion matrix visualization. Calculate the confusion matrix from the model's predictions and display it as a heat map or a table with counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n"
      ],
      "metadata": {
        "id": "m5wsbhsAncHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate code to plot the Precision-Recall (PR) curve. Calculate precision and recall from the model's predictions and display the PR curve using a suitable plotting library.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import pandas as pd\n",
        "\n",
        "# Collect predictions and labels\n",
        "predictions = transformed_dataset.select(\"prediction\").toPandas()\n",
        "labels = transformed_dataset.select(\"label\").toPandas()\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, _ = precision_recall_curve(labels, predictions)\n",
        "\n",
        "# Plot the PR curve\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bt64dC_voJ5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate code to create a confusion matrix visualization. Calculate the confusion matrix from the model's predictions and display it as a heatmap or a table with counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Collect predictions and labels\n",
        "predictions = transformed_dataset.select(\"prediction\").toPandas()\n",
        "labels = transformed_dataset.select(\"label\").toPandas()\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(labels, predictions)\n",
        "\n",
        "# Create a heatmap of the confusion matrix\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tkbv_5p0o51G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y2d3oBAecfK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate code to plot the Precision-Recall (PR) curve. Calculate precision and recall from the model's predictions and display the PR curve using a suitable plotting library.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import pandas as pd\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "transformed_pd = transformed_dataset.select(\"label\", \"prediction\").toPandas()\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, _ = precision_recall_curve(transformed_pd[\"label\"], transformed_pd[\"prediction\"])\n",
        "\n",
        "# Plot the PR curve\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T7xGUnBzVFhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate code to create a confusion matrix visualization. Calculate the confusion matrix from the model's predictions and display it as a heatmap or a table with counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'transformed_dataset' is a Spark DataFrame with 'label' and 'prediction' columns\n",
        "predictions_pd = transformed_dataset.select('label', 'prediction').toPandas()\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(predictions_pd['label'], predictions_pd['prediction'])\n",
        "\n",
        "# Create a heatmap visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Display the confusion matrix as a table\n",
        "cm_df = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
        "print(\"Confusion Matrix (Table):\", cm_df)\n",
        "\n",
        "# Extract TP, TN, FP, FN\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"True Positives (TP): {tp}\")\n",
        "print(f\"True Negatives (TN): {tn}\")\n",
        "print(f\"False Positives (FP): {fp}\")\n",
        "print(f\"False Negatives (FN): {fn}\")"
      ],
      "metadata": {
        "id": "hnekNs7-Hgap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Convert feature list into individual columns\n",
        "pdf = features.select(\"features\", \"label\").toPandas()\n",
        "feature_df = pd.DataFrame(pdf[\"features\"].tolist(), columns=[\"age\", \"country_hash\", \"gender_hash\", \"traffic_source_hash\"])\n",
        "\n",
        "# Plot histograms\n",
        "feature_df.hist(figsize=(10, 6), bins=30, color=\"dodgerblue\", alpha=0.7)\n",
        "plt.suptitle(\"Feature Distributions\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GQJu8QLJkAvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 8: Write Predictions to BigQuery**\n",
        "\n",
        "Use Gemini to write predictions to BigQuery.\n",
        "\n",
        "**Prompt:** Using Spark, write the transformed dataset to BigQuery."
      ],
      "metadata": {
        "id": "ncZ55Z0sxF7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using Spark, write the transformed dataset to BigQuery.\n",
        "\n",
        "# NOTE: Pyspark code generation is currently in PREVIEW.\n",
        "# Write the transformed dataset to BigQuery\n",
        "transformed_dataset.write.format(\"bigquery\").option(\"table\", \"<YOUR_PROJECT_ID>.my_dataset.predictions\").mode(\"overwrite\").save()"
      ],
      "metadata": {
        "id": "N1U-voG4xajv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}