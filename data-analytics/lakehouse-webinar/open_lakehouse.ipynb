{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "Webinar Demo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "iOSYaTd-EnUl",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1758823873604,
          "user_tz": 240,
          "elapsed": 60,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build an open lakehouse on Google Cloud"
      ],
      "metadata": {
        "id": "97pj9VJ3ElXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table align=\"left\">\n",
        "  <td><a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/devrel-demos/blob/main/data-analytics/lakehouse-webinar/open_lakehouse.ipynb\"><img src=\"https://avatars.githubusercontent.com/u/33467679?s=200&v=4\" width=\"32px\" alt=\"Colab logo\"> Run in Colab</a></td>\n",
        "  <td><a href=\"https://github.com/GoogleCloudPlatform/devrel-demos/blob/main/data-analytics/lakehouse-webinar/open_lakehouse.ipynb\"><img src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\" alt=\"GitHub logo\"> View on GitHub</a></td>\n",
        "  <td><a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/devrel-demos/blob/main/data-analytics/lakehouse-webinar/open_lakehouse.ipynb\"><img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"> Open in Vertex AI Workbench</a></td>\n",
        "  <td><a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/GoogleCloudPlatform/devrel-demos/blob/main/data-analytics/lakehouse-webinar/open_lakehouse.ipynb\"><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTW1gvOovVlbZAIZylUtf5Iu8-693qS1w5NJw&s\" alt=\"BQ logo\" width=\"35\"> Open in BQ Studio</a></td>\n",
        "  <td><a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fdevrel-demos%2Fblob%2Fmain%data-analytics%2Ficeberg_webinar%2Fopen_lakehouse.ipynb\"><img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"> Open in Colab Enterprise</a></td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "CI2vORpdC93H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TheLook, a fictional ecommerce company, is looking to migrate and modernize their open lakehouse on Google Cloud. They are also looking to learn more about customer behavior, specifically around returned orders.\n",
        "\n",
        "In this notebook, you set up an lakehouse using [Apache Spark](spark.apache.org), [Apache Iceberg](https://iceberg.apache.org/), [BigLake](https://cloud.google.com/biglake), [BigQuery](https://cloud.google.com/bigquery), [Dataplex](https://cloud.google.com/dataplex), [Cloud Storage](https://cloud.google.com/storage) and [Vertex AI](https://cloud.google.com/vertex-ai).\n"
      ],
      "metadata": {
        "id": "DOiZD_YMRaoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Configure environment variables. Provide your project ID and a [region](https://cloud.google.com/bigquery/docs/locations#regions) to store your resources, such as `us-central1`. **Note**: this tutorial will not work with [multi-regions](https://cloud.google.com/bigquery/docs/locations#multi-regions)."
      ],
      "metadata": {
        "id": "36_Lx0Cv0N-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"\" #@param\n",
        "LOCATION = \"\" #@param\n",
        "WAREHOUSE_BUCKET_NAME = f\"{PROJECT_ID}-warehouse\""
      ],
      "metadata": {
        "id": "J3v0U7vk-gaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Cloud Storage bucket to house staging data."
      ],
      "metadata": {
        "id": "5ItT8Q05YcDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "STAGING_BUCKET_NAME=f\"{PROJECT_ID}-staging\"\n",
        "\n",
        "storage_client = storage.Client()\n",
        "if not storage_client.bucket(STAGING_BUCKET_NAME).exists():\n",
        "    storage_client.create_bucket(STAGING_BUCKET_NAME, location=LOCATION)"
      ],
      "metadata": {
        "id": "2lGHL8aXdgSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Cloud Storage bucket for your warehouse."
      ],
      "metadata": {
        "id": "MKqJmM1keKIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WAREHOUSE_BUCKET_NAME = f\"{PROJECT_ID}-warehouse\"\n",
        "\n",
        "if not storage_client.bucket(WAREHOUSE_BUCKET_NAME).exists():\n",
        "    storage_client.create_bucket(WAREHOUSE_BUCKET_NAME, location=LOCATION)"
      ],
      "metadata": {
        "id": "GqaXXKsQeQTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy a [Delta Lake](https://delta.io/) table to your new bucket, which contains a list of products."
      ],
      "metadata": {
        "id": "o0Cl14WqeYNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_prefix = \"lakehouse_notebook/delta\"\n",
        "source_bucket = storage_client.bucket(\"data-analytics-demos\")\n",
        "destination_bucket = storage_client.bucket(STAGING_BUCKET_NAME)\n",
        "\n",
        "blobs = source_bucket.list_blobs(prefix=source_prefix)\n",
        "\n",
        "for blob in blobs:\n",
        "    new_blob_name = \"delta\" + blob.name[len(source_prefix):]\n",
        "    source_bucket.copy_blob(blob, destination_bucket, new_blob_name)\n"
      ],
      "metadata": {
        "id": "sZdACxeIeaJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataset that will store your data."
      ],
      "metadata": {
        "id": "MjY_ge-X4IAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "CREATE SCHEMA IF NOT EXISTS `lakehouse`\n",
        "OPTIONS (\n",
        "  location = 'YOUR_LOCATION', -- Replace with your location\n",
        "  description = 'Store lakehouse data'\n",
        ")\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "id": "AY5NfXBcezH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an empty BigQuery table that will store product data."
      ],
      "metadata": {
        "id": "X2lnD7Ls4kjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS lakehouse.products\n",
        "(\n",
        "  id INT64,\n",
        "  cost FLOAT64,\n",
        "  category STRING,\n",
        "  name STRING,\n",
        "  brand STRING,\n",
        "  retail_price FLOAT64,\n",
        "  department STRING,\n",
        "  sku STRING,\n",
        "  distribution_center_id INT64\n",
        ");\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "id": "oY1Gn91F4L_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Populate the table."
      ],
      "metadata": {
        "id": "YHozJIjn4pHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "LOAD DATA INTO lakehouse.products\n",
        "FROM FILES (\n",
        "  uris=['gs://data-analytics-demos/thelook_ecommerce/products/*'],\n",
        "  format='PARQUET'\n",
        ")\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "id": "yhk29KG7-HD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a BigQuery connection to connect to Google Cloud resources such as Vertex AI and Google Cloud Storage."
      ],
      "metadata": {
        "id": "QnE9KqxofGZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "CONNECTION_ID = \"resource_connection\"\n",
        "command = [\n",
        "    'bq',\n",
        "    'mk',\n",
        "    '--connection',\n",
        "    '--location',\n",
        "    LOCATION,\n",
        "    '--connection_type=CLOUD_RESOURCE',\n",
        "    CONNECTION_ID\n",
        "]\n",
        "\n",
        "result = subprocess.run(command, capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "metadata": {
        "id": "VWtN2V8Kfc_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update the BigQuery Connection with the necessary permissions.\n",
        "\n",
        "**Note**: The following cell will occasionally fail as the connection service account isn't always available right away after connection creation. The workaround is as follows:\n",
        "\n",
        "1. Copy the service account email that's printed from the cell below.\n",
        "2. Go to the [IAM Admin](https://console.cloud.google.com/iam-admin/iam) page and click **Grant Access**\n",
        "3. In **Add principals** paste the service account email\n",
        "3. In **Assign roles** add the roles `aipaltform.admin` and `storage.admin`."
      ],
      "metadata": {
        "id": "7IlQj5cP2CXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery_connection_v1\n",
        "\n",
        "# Get service account used with BigQuery connection\n",
        "connection_client = bigquery_connection_v1.ConnectionServiceClient()\n",
        "\n",
        "full_connection_name = connection_client.connection_path(\n",
        "    PROJECT_ID, LOCATION, CONNECTION_ID\n",
        ")\n",
        "\n",
        "connection = connection_client.get_connection(name=full_connection_name)\n",
        "sa_email = connection.cloud_resource.service_account_id\n",
        "print(f\"BigQuery Connection service account: {sa_email}\")\n",
        "\n",
        "roles = [\"aiplatform.admin\", \"storage.admin\"]\n",
        "\n",
        "for role in roles:\n",
        "  command = [\n",
        "      'gcloud',\n",
        "      'projects',\n",
        "      'add-iam-policy-binding',\n",
        "      PROJECT_ID,\n",
        "      '--member',\n",
        "      f'serviceAccount:{sa_email}',\n",
        "      '--role',\n",
        "      f'roles/{role}'\n",
        "  ]\n",
        "\n",
        "  result = subprocess.run(command, capture_output=True, text=True)\n",
        "  print(result.stdout)\n",
        "  print(result.stderr)"
      ],
      "metadata": {
        "id": "i7ybJ0XLg8ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a BigLake external table for Delta Lake."
      ],
      "metadata": {
        "id": "oqMtbUWjSwsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an external table to manage your Delta Lake data."
      ],
      "metadata": {
        "id": "s_h9rVZx6mFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS `lakehouse.users_delta`\n",
        "WITH CONNECTION `YOUR_LOCATION.resource_connection` -- Update location\n",
        "OPTIONS (\n",
        "  format = 'DELTA_LAKE',\n",
        "  uris = ['gs://YOUR_PROJECT_ID-staging/delta/users'] -- Update project ID\n",
        ");"
      ],
      "metadata": {
        "id": "oeb9ixRS90iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query this table."
      ],
      "metadata": {
        "id": "9UgcUufWTLJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "select * from lakehouse.users_delta\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "tags": [
          "table-ref:gcp-data-event-demo-2025.lakehouse.users_delta"
        ],
        "id": "PKhgY5DQcYWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create BigLake Tables for Apache Iceberg"
      ],
      "metadata": {
        "id": "DyPa5umxEPw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a [BigLake Table for Apache Iceberg](https://cloud.google.com/bigquery/docs/iceberg-tables)."
      ],
      "metadata": {
        "id": "jbTSfPQi_pXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "CREATE TABLE lakehouse.order_history_iceberg_imt_mstx (\n",
        "  id INT64,\n",
        "  order_id INT64,\n",
        "  user_id INT64,\n",
        "  product_id INT64,\n",
        "  inventory_item_id INT64,\n",
        "  status STRING,\n",
        "  created_at TIMESTAMP,\n",
        "  shipped_at TIMESTAMP,\n",
        "  delivered_at TIMESTAMP,\n",
        "  returned_at TIMESTAMP,\n",
        "  sale_price FLOAT64\n",
        ")\n",
        "WITH CONNECTION `YOUR_LOCATION.resource_connection` -- Update location\n",
        "OPTIONS (\n",
        "  file_format = 'PARQUET',\n",
        "  table_format = 'ICEBERG',\n",
        "  storage_uri = 'gs://YOUR_PROJECT_ID-warehouse/order_history_iceberg_imt_mstx' -- Update project ID\n",
        ");\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "id": "xN_kRBdOAdRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Populate the table."
      ],
      "metadata": {
        "id": "7yzgnW2eEZCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "LOAD DATA INTO lakehouse.order_history_iceberg_imt_mstx\n",
        "FROM FILES (\n",
        "  uris=['gs://data-analytics-demos/thelook_ecommerce/order_items/*'],\n",
        "  format='PARQUET'\n",
        ")\n",
        "\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "tags": [],
        "id": "GVFndSOjRqVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query the table."
      ],
      "metadata": {
        "id": "p6W5HjgBS9lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  lakehouse.order_history_iceberg_imt_mstx;\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "tags": [
          "table-ref:gcp-data-event-demo-2025.lakehouse.order_history_iceberg_imt_mstx"
        ],
        "id": "1wGGc1x5vitO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With BigLake, you can query across table formats. Join the Iceberg order history table with the Delta Lake table."
      ],
      "metadata": {
        "id": "lChzDVkAOHoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "SELECT * FROM lakehouse.order_history_iceberg_imt_mstx i\n",
        "LEFT JOIN lakehouse.users_delta d\n",
        "ON i.user_id = d.id;\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "tags": [
          "table-ref:gcp-data-event-demo-2025.lakehouse.users_delta",
          "table-ref:gcp-data-event-demo-2025.lakehouse.order_history_iceberg_imt_mstx"
        ],
        "id": "wDgxVAILN5pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a second table."
      ],
      "metadata": {
        "id": "64-oF6mWESv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "CREATE TABLE lakehouse.order_history_returns_iceberg_imt_mstx (\n",
        "  id INT64,\n",
        "  order_id INT64,\n",
        "  user_id INT64,\n",
        "  product_id INT64,\n",
        "  inventory_item_id INT64,\n",
        "  status STRING,\n",
        "  created_at TIMESTAMP,\n",
        "  shipped_at TIMESTAMP,\n",
        "  delivered_at TIMESTAMP,\n",
        "  returned_at TIMESTAMP,\n",
        "  sale_price FLOAT64\n",
        ")\n",
        "WITH CONNECTION `YOUR_LOCATION.resource_connection` -- Update location\n",
        "OPTIONS (\n",
        "  file_format = 'PARQUET',\n",
        "  table_format = 'ICEBERG',\n",
        "  storage_uri = 'gs://YOUR_PROJECT_ID-warehouse/order_history_returns_iceberg_imt_mstx' -- Update project ID\n",
        ");\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "id": "q_mpBRZjEHOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BigLake Tables for Apache Iceberg support [multi-statement transactions](https://cloud.google.com/bigquery/docs/transactions). With this, you simultaneously insert rows from one table into another, and then delete the rows from the source table.\n",
        "\n",
        "**Note:** You'll need to use an [allowlisted](https://cloud.google.com/bigquery/docs/iceberg-tables#use_multi-statement_transactions) project to use this feature. To proceed without this feature, delete `BEGIN TRANSACTION;` and `COMMIT TRANSACTION;` below, which runs the jobs synchronously."
      ],
      "metadata": {
        "id": "w6U8JydGSVIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "BEGIN TRANSACTION;\n",
        "\n",
        "-- Insert rows into order_history_returns_iceberg_imt_mstx\n",
        "INSERT INTO lakehouse.order_history_returns_iceberg_imt_mstx\n",
        "SELECT * FROM lakehouse.order_history_iceberg_imt_mstx\n",
        "WHERE status = \"Returned\";\n",
        "\n",
        "-- Delete the matching records from order_history_returns_iceberg_imt_mstx\n",
        "DELETE lakehouse.order_history_iceberg_imt_mstx\n",
        "WHERE status = \"Returned\";\n",
        "\n",
        "COMMIT TRANSACTION;\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "id": "m0_xvGLlRbBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also discover and search for these tables in [Dataplex](console.cloud.google.com/dataplex.dp-search-nl)."
      ],
      "metadata": {
        "id": "qY9sIvPpTdJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to an interactive serverless Spark session"
      ],
      "metadata": {
        "id": "naEQb6rW6fB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a connection to [Google Cloud Serverless for Apache Spark](https://cloud.google.com/products/serverless-spark), which provides you a serverless Spark runtime to execute jobs against. This connection utilizes [Spark Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html). Learn more about [supported configuration parameters](https://cloud.google.com/bigquery/docs/use-spark).\n",
        "\n",
        "Configure the runtime to use [Lightning Engine for Apache Spark](https://cloud.google.com/products/lightning-engine), which can potentially accelerate Spark jobs by up to 4.3x."
      ],
      "metadata": {
        "id": "soFYV2jfTROt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
        "from google.cloud.dataproc_v1 import Session\n",
        "\n",
        "session = Session()\n",
        "\n",
        "catalog_name = \"demo_catalog\"\n",
        "\n",
        "# Configure the Spark runtime to use Lighting Engine\n",
        "session.runtime_config.properties = {\n",
        "    'spark.dataproc.runtimeEngine': 'native',\n",
        "    'spark.dataproc.driver.compute.tier': 'premium',\n",
        "    'spark.dataproc.executor.compute.tier': 'premium'\n",
        "}"
      ],
      "metadata": {
        "id": "p2NN8wwF7YRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the runtime to access a BigLake metastore with the [Iceberg REST Catalog](https://cloud.google.com/bigquery/docs/blms-rest-catalog). The Iceberg Rest Catalog supports accessing Iceberg tables in BigLake metastore from any compatible runtime engine via REST calls."
      ],
      "metadata": {
        "id": "fgdoYZ0yT2uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the runtime to use the Iceberg REST Catalog\n",
        "session.runtime_config.properties.update({\n",
        "    'spark.sql.extensions': 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions',\n",
        "    'spark.sql.defaultCatalog': f'{catalog_name}',\n",
        "    f'spark.sql.catalog.{catalog_name}': 'org.apache.iceberg.spark.SparkCatalog',\n",
        "    f'spark.sql.catalog.{catalog_name}.type': 'rest',\n",
        "    f'spark.sql.catalog.{catalog_name}.uri': 'https://biglake.googleapis.com/iceberg/v1/restcatalog',\n",
        "    f'spark.sql.catalog.{catalog_name}.warehouse': f'gs://{WAREHOUSE_BUCKET_NAME}',\n",
        "    f'spark.sql.catalog.{catalog_name}.header.x-goog-user-project': f'{PROJECT_ID}',\n",
        "    f'spark.sql.catalog.{catalog_name}.rest.auth.type': 'org.apache.iceberg.gcp.auth.GoogleAuthManager',\n",
        "    f'spark.sql.catalog.{catalog_name}.io-impl': 'org.apache.iceberg.gcp.gcs.GCSFileIO',\n",
        "    f'spark.sql.catalog.{catalog_name}.rest-metrics-reporting-enabled': 'false',\n",
        "})"
      ],
      "metadata": {
        "id": "b-KgLrkfTj3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the serverless Spark session. After executing the cell, click **VIEW SESSION DETAILS** to view the Spark UI panel."
      ],
      "metadata": {
        "id": "JYJKTMW-AbnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the serverless Spark runtime.\n",
        "spark = DataprocSparkSession.builder.dataprocSessionConfig(session).getOrCreate()"
      ],
      "metadata": {
        "id": "MmnxjR7pApqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Gemini to generate code to load an iceberg table into Spark.\n",
        "\n",
        "**Prompt**: load the table lakehouse.order_history_returns_iceberg_imt_mstx using Spark and show me some of the data"
      ],
      "metadata": {
        "id": "qaE00lthYy2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load the table lakehouse.order_history_returns_iceberg_imt_mstx using spark and show me some of the data\n",
        "\n",
        "# NOTE: Pyspark code generation is currently in PREVIEW.\n",
        "df = spark.read.format(\"bigquery\").option(\"table\", \"lakehouse.order_history_returns_iceberg_imt_mstx\").load()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "0af-8gZz1rMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Register the returns table as a temporary table also.\n",
        "\n",
        "**Prompt:** register lakehouse.order_history_returns_iceberg_imt_mstx as a temp table named returns"
      ],
      "metadata": {
        "id": "ICSrx97cgVvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: register lakehouse.order_history_returns_iceberg_imt_mstx as a temp table named returns\n",
        "\n",
        "# NOTE: Pyspark code generation is currently in PREVIEW.\n",
        "df.createOrReplaceTempView(\"returns\")"
      ],
      "metadata": {
        "id": "RJmPllTk7ANh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the BigQuery table `products` and register it as a SparkSQL view."
      ],
      "metadata": {
        "id": "RVT1WdW-VYY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format(\"bigquery\").load(f\"lakehouse.products\").createOrReplaceTempView(\"products\")"
      ],
      "metadata": {
        "id": "TX2QW5eDN4wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a namespace for your Iceberg tables."
      ],
      "metadata": {
        "id": "xSPzJxLqY-HG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS spark;\")\n",
        "\n",
        "spark.sql(\"USE spark;\")"
      ],
      "metadata": {
        "id": "2lg7LIpQIHJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new Iceberg table."
      ],
      "metadata": {
        "id": "2CLCUAYcZFZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"CREATE OR REPLACE TABLE returns_by_category (id int, name string, count string) USING ICEBERG;\")"
      ],
      "metadata": {
        "id": "frcREo9bM1le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the table in the namespace."
      ],
      "metadata": {
        "id": "lmfvr34qClr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SHOW TABLES\").show()"
      ],
      "metadata": {
        "id": "AgPBmT0bCleg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert into this table."
      ],
      "metadata": {
        "id": "r9g-5oaoZJCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"INSERT INTO returns_by_category\n",
        "  SELECT\n",
        "    c.id,\n",
        "    name,\n",
        "    COUNT(*)\n",
        "  FROM products c\n",
        "  LEFT JOIN returns AS returns\n",
        "  ON c.id = returns.id\n",
        "  group by c.id, c.name;\n",
        ";\"\"\")"
      ],
      "metadata": {
        "id": "LATFijelLktr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query the table using Spark."
      ],
      "metadata": {
        "id": "VAndB0WV2K1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM returns_by_category\").show()"
      ],
      "metadata": {
        "id": "QMzX0mWImD90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can query this new table from BigQuery also."
      ],
      "metadata": {
        "id": "kIN2035oZM8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "SELECT * FROM `YOUR_PROJECT_ID-warehouse>spark.returns_by_category`; -- Update project ID\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "id": "9LkfckKuZQJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Gemini to extend data richness\n",
        "\n",
        "Using the `AI.GENERATE` function in BigQuery, you can pass data from your BigQuery tables to Gemini."
      ],
      "metadata": {
        "id": "OUrHr516aBm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `AI.GENERATE` to generate brief product descriptions for the returned products."
      ],
      "metadata": {
        "id": "AgdXLcHSgeZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "SELECT\n",
        "  name,\n",
        "  AI.GENERATE(\n",
        "    (\"Give a short description of \", name),\n",
        "    connection_id => 'YOUR_LOCATION.resource_connection', -- Update location\n",
        "    endpoint => 'gemini-2.5-flash').result\n",
        "FROM\n",
        "  `YOUR_PROJECT_ID-warehouse>spark.returns_by_category` -- Update project_id\n",
        "\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "id": "8ZPCE2hiaYuG",
        "tags": [
          "table-ref:gcp-data-event-demo-2025.gcp-data-event-demo-2025-warehouse.spark.returns_by_category"
        ]
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BigQuery ML\n",
        "\n",
        "Create a logistic regression model to predict return behavior based on order and user data.\n",
        "\n",
        "You union the two Iceberg orders tables back together and join them with your Delta Lake users table."
      ],
      "metadata": {
        "id": "fZcYSgFxzrk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "\n",
        "CREATE OR REPLACE MODEL `lakehouse.returns_model`\n",
        "OPTIONS(model_type='LOGISTIC_REG',\n",
        "        input_label_cols=['returned']) AS\n",
        "SELECT\n",
        "  product_id,\n",
        "  age,\n",
        "  postal_code,\n",
        "  traffic_source,\n",
        "  IF(status = \"Returned\", 0, 1) AS returned\n",
        "FROM (\n",
        "  SELECT * FROM `lakehouse.order_history_iceberg_imt_mstx`\n",
        "  UNION ALL\n",
        "  SELECT * FROM `lakehouse.order_history_returns_iceberg_imt_mstx`\n",
        ") orders\n",
        "\n",
        "LEFT JOIN\n",
        "  `lakehouse.users_delta` users\n",
        "ON\n",
        "  orders.user_id = users.id"
      ],
      "metadata": {
        "id": "SYKa1K5nBgn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model."
      ],
      "metadata": {
        "id": "TFLl7Dx-Bk3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sql_engine: bigquery\n",
        "# output_variable: df\n",
        "# start _sql\n",
        "_sql = \"\"\"\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  ML.EVALUATE(MODEL `lakehouse.returns_model`)\n",
        "\"\"\" # end _sql\n",
        "from google.colab.sql import bigquery as _bqsqlcell\n",
        "df = _bqsqlcell.run(_sql)\n",
        "df"
      ],
      "metadata": {
        "colab_type": "sql",
        "tags": [
          "table-ref:gcp-data-event-demo-2025.lakehouse.returns_model"
        ],
        "id": "NjVbSBxIzXI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning Up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial by uncommenting below:"
      ],
      "metadata": {
        "id": "qlwaJNKp2dYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Spark session\n",
        "# spark.stop()\n",
        "\n",
        "# Delete all BigQuery tables + dataset\n",
        "#!bq rm -r -f -d lakehouse\n",
        "\n",
        "# Delete storage buckets\n",
        "#!gcloud storage rm --recursive gs://YOUR_PROJECT_ID-staging\n",
        "#!gcloud storage rm --recursive gs://YOUR_PROJECT_ID-warehouse\n"
      ],
      "metadata": {
        "id": "X3qlil8v2lS_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
