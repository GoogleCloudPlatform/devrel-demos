{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "name": "Purchase_Predictions_Spark_nov25"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "4lsu9J_0vV2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario\n",
        "\n",
        "TheLook, a hypothetical eCommerce clothing retailer, stores data on customers, products, orders, logistics, web events, and digital marketing campaigns in BigQuery. The company wants to leverage the team's existing SQL and PySpark expertise to analyze this data using Apache Spark.\n",
        "\n",
        "To avoid manual infrastructure provisioning or tuning for Spark, TheLook seeks an auto-scaling solution that allows them to focus on workloads rather than cluster management. Additionally, they want to minimize the effort required to integrate Spark and BigQuery while staying within the BigQuery Studio environment, possibly using BigQuery notebooks.\n",
        "\n",
        "They then want to productionalize the model by having a queryable endpoint, and a way to query it via natural language.\n",
        "\n",
        "Let's understand how they can perform following analysis using Apache Spark:\n",
        "\n",
        "\n",
        "# Goal\n",
        "\n",
        "Predict whether a user will make a purchase by building a Logistic Regression Classifier using PySpark and leverage BQ Studio's native Integration and AI-features for exploring the data.\n"
      ],
      "metadata": {
        "id": "QCcazsyjJvXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 0: Setup**\n",
        "\n",
        "The following steps create resources that will be used throughout the tutorial."
      ],
      "metadata": {
        "id": "RtgdfcrNWNkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enable necessary APIs and then **refresh the page**."
      ],
      "metadata": {
        "id": "pVU8AVZ5p1cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "command = [\n",
        "    \"gcloud\",\n",
        "    \"services\",\n",
        "    \"enable\",\n",
        "    \"aiplatform.googleapis.com\",\n",
        "    \"bigquery.googleapis.com\",\n",
        "    \"bigquerystorage.googleapis.com\",\n",
        "    \"bigqueryunified.googleapis.com\",\n",
        "    \"cloudaicompanion.googleapis.com\",\n",
        "    \"dataproc.googleapis.com\",\n",
        "    \"run.googleapis.com\"\n",
        "    \"storage.googleapis.com\"\n",
        "]\n",
        "\n",
        "result = subprocess.run(command, capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "metadata": {
        "id": "rKwMaHF81m3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure a project id and location."
      ],
      "metadata": {
        "id": "YHIbYWxZWxk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"\" # @param {type:\"string\"}\n",
        "\n",
        "REGION = \"us-central1\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "SzD0y_2iBsoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use [Google Cloud Serverless for Apache Spark](https://docs.cloud.google.com/dataproc-serverless/docs), turn on [Private Google Access](https://docs.cloud.google.com/vpc/docs/configure-private-google-access#gcloud_1)."
      ],
      "metadata": {
        "id": "qm7aYYPI04tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "command = [\n",
        "    \"gcloud\",\n",
        "    \"compute\",\n",
        "    \"networks\",\n",
        "    \"subnets\",\n",
        "    \"update\",\n",
        "    \"default\",\n",
        "    f\"--region={REGION}\",\n",
        "    \"--enable-private-ip-google-access\"\n",
        "]\n",
        "\n",
        "result = subprocess.run(command, capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "metadata": {
        "id": "m0nfPP-C0ou3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a [Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets?utm_campaign=CDR_0x225cfd13_default_b407565440&utm_source=external&utm_medium=web).\n"
      ],
      "metadata": {
        "id": "cYJ_MOKbW3re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "from google.cloud.exceptions import NotFound\n",
        "\n",
        "BUCKET_NAME = f\"{PROJECT_ID}-demo\"\n",
        "\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "try:\n",
        "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "    print(f\"Bucket {BUCKET_NAME} already exists.\")\n",
        "except NotFound:\n",
        "    bucket = storage_client.create_bucket(BUCKET_NAME, location=REGION)\n",
        "    print(f\"Bucket {BUCKET_NAME} created.\")"
      ],
      "metadata": {
        "id": "4PCe-xdgBoZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a BigQuery dataset."
      ],
      "metadata": {
        "id": "FmWLVkxMY6BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "DATASET_ID = f\"{PROJECT_ID}.demo\"\n",
        "\n",
        "client = bigquery.Client()\n",
        "\n",
        "dataset = bigquery.Dataset(DATASET_ID)\n",
        "\n",
        "dataset.location = REGION\n",
        "\n",
        "dataset = client.create_dataset(dataset, exists_ok=True)"
      ],
      "metadata": {
        "id": "g5Wf5BLAxBOf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Setup Spark**\n",
        "\n",
        "*   Set up the Spark environment: It imports necessary\n",
        "libraries for connecting to Dataproc and using PySpark.\n",
        "*   Configure the Dataproc session: It creates and configures a Spark Session with the necessary parameters, providing the spark object for subsequent Spark operations.\n",
        "\n",
        "This step can also be accomplished in a single line of code below.\n",
        "\n",
        "This configuratin enables [Lightning Engine for Apache Spark](https://docs.cloud.google.com/dataproc-serverless/docs/guides/lightning-engine). Lightning Engine is a high-performance query accelerator powered by a multi-layer optimization engine that performs customary optimization techniques, such as query and execution optimizations, as well as curated optimizations in the file system layer and data access connectors.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "spark = DataprocSparkSession.builder.getOrCreate()\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RVDKd--rJ-_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes:**\n",
        "1. The first time you run this cell, you may encounter a service account error. Wait about 60 seconds and try again.\n",
        "\n",
        "2. You may be asked to authenticate. Click the link in the output to do so."
      ],
      "metadata": {
        "id": "5myRIMwVm7nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
        "from google.cloud.dataproc_v1 import Session\n",
        "\n",
        "session = Session()\n",
        "\n",
        "session.runtime_config.version = \"3.0\"\n",
        "\n",
        "# You can optionally configure Spark properties as well. See https://cloud.google.com/dataproc-serverless/docs/concepts/properties.\n",
        "session.runtime_config.properties = {\n",
        "  \"dataproc.runtime\": \"premium\",\n",
        "  \"spark.dataproc.engine\": \"lightningEngine\",\n",
        "}\n",
        "\n",
        "# To avoid going over quota in this demo, cap the max number of Spark workers.\n",
        "session.runtime_config.properties = {\n",
        "    \"spark.dynamicAllocation.maxExecutors\": \"4\"\n",
        "}\n",
        "\n",
        "spark = (\n",
        "    DataprocSparkSession.builder\n",
        "      .appName(\"CustomSparkSession\")\n",
        "      .dataprocSessionConfig(session)\n",
        "      .getOrCreate()\n",
        ")"
      ],
      "metadata": {
        "id": "h_tqpdNgg-qe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7c8a83f8-29fd-4280-8b7e-97a13c7080ac"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DataprocSparkConnectException",
          "evalue": "Specified 2.3 Dataproc Runtime version is not supported, use 3.0 version or higher.",
          "traceback": [
            "Specified 2.3 Dataproc Runtime version is not supported, use 3.0 version or higher."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Load Data**\n",
        "\n",
        "The serverless Spark runtime is configured to load your data directly from BigQuery into a Spark dataframe.\n",
        "\n",
        "Load a dataset of user data."
      ],
      "metadata": {
        "id": "ZJCWG4vrK8dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users = spark.read.format(\"bigquery\").option(\"table\", \"bigquery-public-data.thelook_ecommerce.users\").load()\n",
        "users.show()"
      ],
      "metadata": {
        "id": "oxlclY1FylOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a dataset or orders data."
      ],
      "metadata": {
        "id": "zVyqEgpFzSFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order_items = spark.read.format(\"bigquery\").option(\"table\", \"bigquery-public-data.thelook_ecommerce.order_items\").load()\n",
        "order_items.show()"
      ],
      "metadata": {
        "id": "eC0fbRVact_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Register these tables as Spark SQL tables."
      ],
      "metadata": {
        "id": "nwt0lt31zbFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users.createOrReplaceTempView(\"users\")\n",
        "order_items.createOrReplaceTempView(\"order_items\")"
      ],
      "metadata": {
        "id": "EjADcwTUb0QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use SparkSQL to query a table directly. Query the `users` table."
      ],
      "metadata": {
        "id": "IclgbAE_Uvcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM users LIMIT 10\").show()"
      ],
      "metadata": {
        "id": "W9fykgQXuC4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query the `order_items` table."
      ],
      "metadata": {
        "id": "-QrASWO5U6U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM order_items LIMIT 10\").show()"
      ],
      "metadata": {
        "id": "7gmPu12vuI8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Data Exploration**\n",
        "\n",
        "Bigquery Studio can leverage Gemini for [advanced code completion capabilities](https://cloud.google.com/bigquery/docs/write-sql-gemini#generate_python_code?utm_campaign=CDR_0x225cfd13_default_b407565440&utm_source=external&utm_medium=web) which can use Natual Language to perform exploratory analysis using SQL and even generate PySpark Code for Feature Engineering.\n",
        "\n",
        "Try the following examples. For each prompt, create a new **Code** cell and click **Generate** to generate the code.\n"
      ],
      "metadata": {
        "id": "3orI5dVOMNdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt 1**: Using PySpark, explore the users table and show the first 10 rows."
      ],
      "metadata": {
        "id": "IKPvjuLD0wJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt 2**: Using PySpark, explore the order_items table and show the first 10 rows."
      ],
      "metadata": {
        "id": "G2DUF8iW0yTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt 3**: Using PySpark, show the top 5 most frequent countries in the users table. Display the country and the number of users from each country."
      ],
      "metadata": {
        "id": "oUnNqV1Q00k_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt 4**: Using PySpark, find the average sale price of items in the order_items table."
      ],
      "metadata": {
        "id": "O3R2cYDP02m_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt 5**: Using the table \"users\", generate code to plot country vs traffic source using a suitable plotting library."
      ],
      "metadata": {
        "id": "bTgsXlzq04yR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt 6:** Create a histogram showing the distribution of \"age\", \"country\", \"gender\", \"traffic_source\"."
      ],
      "metadata": {
        "id": "5uKeAC2Y09kM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4: Feature Engineering**\n",
        "\n",
        "In this step, we derive two key columns from the input data:\n",
        "\n",
        "**Creation of features column**:\n",
        "It combines user attributes (age, hashed categorical features) into a numerical array, preparing them for a machine learning model.\n",
        "\n",
        "**Generation of label column:**\n",
        "It creates a binary target variable indicating whether a user has made a purchase or not, derived from order information."
      ],
      "metadata": {
        "id": "pvg1I2Y5QC7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = spark.sql(\"\"\"\n",
        "SELECT\n",
        "  CAST(u.age AS DOUBLE) AS age,\n",
        "  CAST(hash(u.country) AS BIGINT) * 1.0 AS country_hash,\n",
        "  CAST(hash(u.gender) AS BIGINT) * 1.0 AS gender_hash,\n",
        "  CAST(hash(u.traffic_source) AS BIGINT) * 1.0 AS traffic_source_hash,\n",
        "  CASE WHEN COUNT(oi.id) > 0 THEN 1 ELSE 0 END AS label -- Changed label generation to count order items\n",
        "FROM users AS u\n",
        "LEFT JOIN order_items AS oi\n",
        "ON u.id = oi.user_id\n",
        "GROUP BY u.id, u.age, u.country, u.gender, u.traffic_source\n",
        "\"\"\")\n",
        "features.show()"
      ],
      "metadata": {
        "id": "Le1JbxZPpEH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Perform ML Task**\n",
        "\n",
        "This code trains a logistic regression model to predict user purchase behavior, with these steps:\n",
        "\n",
        "**Vector Assembly:** VectorAssembler formats the data into readable vectors.\n",
        "\n",
        "**Feature Scaling:** StandardScaler scales the \"features\" column.\n",
        "\n",
        "**Model Initialization:** LogisticRegression is set up to predict the binary \"label\" (purchase/no purchase), with hyperparameters for training.\n",
        "\n",
        "**Pipeline Definition:** A Pipeline chains StandardScaler and LogisticRegression for streamlined scaling and training.\n",
        "\n",
        "**Model Training:** `pipeline.fit(dataset)` trains the pipeline (scaling and then the model).\n",
        "\n",
        "**Prediction:** `pipeline_model.transform(dataset)` generates predictions, and `transformed_dataset.show()` displays the results.\n",
        "\n",
        "In short, this step scales features, trains a logistic regression model within a pipeline, and produces purchase predictions."
      ],
      "metadata": {
        "id": "-Wj-0bASnK8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.pipeline import Pipeline\n",
        "from pyspark.ml.functions import array_to_vector\n",
        "\n",
        "#Split Train and Test Data (80:20)\n",
        "train_data, test_data = features.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "_4UXH0rLm6fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize VectorAssembler\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"age\", \"country_hash\", \"gender_hash\", \"traffic_source_hash\"],\n",
        "    outputCol=\"assembled_features\"\n",
        ")\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"scaled_features\")\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "lr = LogisticRegression(\n",
        "    maxIter=100,\n",
        "    regParam=0.2,\n",
        "    threshold=0.8,\n",
        "    featuresCol=\"scaled_features\",\n",
        "    labelCol=\"label\"\n",
        ")\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
        "\n",
        "# Fit the model\n",
        "pipeline_model = pipeline.fit(train_data)\n",
        "\n",
        "# Transform the dataset using the trained model\n",
        "transformed_dataset = pipeline_model.transform(test_data)"
      ],
      "metadata": {
        "id": "G2T4T64rm_97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_dataset.show()"
      ],
      "metadata": {
        "id": "Bz262HS5FvjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Evaluation**\n",
        "\n",
        "This code evaluates the trained model's performance by:\n",
        "\n",
        "**Initializing an Evaluator:** A BinaryClassificationEvaluator is set up to calculate the Area Under the Precision-Recall Curve (AUC-PR).\n",
        "\n",
        "**Calculating AUC-PR:** The evaluate() method calculates the AUC-PR score using the model's predictions.\n",
        "\n",
        "This step quantifies the model's ability to distinguish between the two classes (e.g., purchase/no purchase).\n",
        "\n",
        "\n",
        "Further we will use NLP2SQL code generation to visualize the output\n",
        "\n",
        "**Prompt 1:** Generate code to plot the Precision-Recall (PR) curve. Calculate precision and recall from the model's predictions and display the PR curve using a suitable plotting library.\n",
        "\n",
        "**Prompt 2:** Generate code to create a confusion matrix visualization. Calculate the confusion matrix from the model's predictions and display it as a heatmap or a table with counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)."
      ],
      "metadata": {
        "id": "7z9R-zu7nWtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "eva = BinaryClassificationEvaluator(metricName=\"areaUnderPR\")\n",
        "aucPR = eva.evaluate(transformed_dataset)\n",
        "print(f\"AUC PR: {aucPR}\")"
      ],
      "metadata": {
        "id": "1PIlkb-Unatt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 7: Visualization**\n",
        "\n",
        "Let's visualize the results to see how our model performs, and how it has predicted.\n",
        "\n",
        "**Prompt 1:** Generate code to plot the Precision-Recall (PR) curve. Calculate precision and recall from the model's predictions and display the PR curve using a suitable plotting library.\n"
      ],
      "metadata": {
        "id": "m5wsbhsAncHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt 2**: Generate code to create a confusion matrix visualization. Calculate the confusion matrix from the model's predictions and display it as a heat map or a table with counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN)."
      ],
      "metadata": {
        "id": "GYjKJugrIWSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 8: Write Predictions to BigQuery**\n",
        "\n",
        "Use Gemini to write predictions to BigQuery.\n",
        "\n",
        "**Prompt:** Using Spark, write the transformed dataset to BigQuery."
      ],
      "metadata": {
        "id": "ncZ55Z0sxF7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 9: Save model to Cloud Storage**\n",
        "\n",
        "Save your MLlib model to Cloud Storage."
      ],
      "metadata": {
        "id": "hPB3lOTxIocf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"models/prediction_model\"\n",
        "pipeline_model.write().overwrite().save(f\"gs://{BUCKET_NAME}/{MODEL_PATH}\")"
      ],
      "metadata": {
        "id": "pl8p3ZEDI5Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 10: Create an inference server**\n",
        "\n",
        "Use [Cloud Run](https://cloud.google.com/run) to create an inference server for your MLlib model.\n",
        "\n",
        "Clone the Github Repository [devrel-demos](https://github.com/GoogleCloudPlatform/devrel-demos)."
      ],
      "metadata": {
        "id": "PH9_E97mBxQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/GoogleCloudPlatform/devrel-demos.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KLEqjT81ZMR",
        "outputId": "9b81c4d4-e1c7-45c5-f365-2d7116bb0192"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy the Cloud Run server."
      ],
      "metadata": {
        "id": "P_tXfvW2EZ9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "command = [\n",
        "    \"gcloud\",\n",
        "    \"run\",\n",
        "    \"deploy\",\n",
        "    \"inference-server\",\n",
        "    \"--source\",\n",
        "    \"/content/devrel-demos/data-analytics/dataproc-webinar/data-science/inference-server\",\n",
        "    \"--region\",\n",
        "    f\"{REGION}\",\n",
        "    \"--port\",\n",
        "    \"8080\",\n",
        "    \"--memory\",\n",
        "    \"2Gi\",\n",
        "    \"--allow-unauthenticated\",\n",
        "    \"--set-env-vars\",\n",
        "    f\"GCS_BUCKET={BUCKET_NAME},GCS_MODEL_PATH={MODEL_PATH}\"\n",
        "    \"--startup-probe\",\n"
        "    \"tcpSocket.port=8080,initialDelaySeconds=240,failureThreshold=3,timeoutSeconds=240,periodSeconds=240\"\n",
        "]\n",
        "\n",
        "result = subprocess.run(command, capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "print(result.stderr)"
      ],
      "metadata": {
        "id": "juz4WbCNDKTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the Service URL from the output and paste it below."
      ],
      "metadata": {
        "id": "YS9ixgI8VAT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INFERENCE_SERVER_URL = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "h8nXHEZrVM-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your inference server."
      ],
      "metadata": {
        "id": "e-_DkStLVhgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "age = \"25.0\"\n",
        "country = \"United States\"\n",
        "traffic_source = \"Search\"\n",
        "gender = \"F\"\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{INFERENCE_SERVER_URL}/predict\",\n",
        "    json=[{\"age\": age, \"country\": country, \"traffic_source\": traffic_source, \"gender\": gender}],\n",
        "    headers={\"Content-Type\": \"application/json\"}\n",
        ")\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "gnxhANfVVg9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 11: Create a remote agent**\n",
        "\n",
        "[Vertex AI Agent Engine](https://docs.cloud.google.com/agent-builder/agent-engine/overview) is a service for deploying, managing, and scaling AI agents in production.\n",
        "\n",
        "The [Agent Development Kit (ADK)](https://google.github.io/adk-docs/) is a model-agnostic framework for developing and deploying AI Agents.\n",
        "\n",
        "Use these two to build an agent that can call the deployed SparkML model\n"
      ],
      "metadata": {
        "id": "Sst1TWO_UE43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize a Vertex AI Client."
      ],
      "metadata": {
        "id": "EtaIK773UwX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai import agent_engines # For the prebuilt templates\n",
        "\n",
        "client = vertexai.Client(  # For service interactions via client.agent_engines\n",
        "    project=f\"{PROJECT_ID}\",\n",
        "    location=f\"{REGION}\",\n",
        ")"
      ],
      "metadata": {
        "id": "bR3dE0w8UvCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function for querying the deployed model."
      ],
      "metadata": {
        "id": "OFsIHknQU2vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_purchase(\n",
        "    age: str = \"25.0\",\n",
        "    country: str = \"United States\",\n",
        "    traffic_source: str = \"Search\",\n",
        "    gender: str = \"M\",\n",
        "):\n",
        "    \"\"\"Predicts whether or not a user will purchase a product.\n",
        "\n",
        "    Args:\n",
        "        age: The age of the user.\n",
        "        country: The country of the user. One of: \"China\", \"Poland\", \"Germany\", \"United States\", \"Spain\", \"United Kingdom\", \"Espa√±a\", \"Japan\", \"Brasil\", \"Colombia\", \"Belgium\", \"South Korea\", \"Austria\", \"France\", \"Australia\".\n",
        "        Traffic_source: The source of the user's traffic. One of: \"Display\", \"Email\", \"Search\", \"Organic\", \"Facebook\".\n",
        "        gender: The gender of the user. One of: \"M\" or \"F\".\n",
        "\n",
        "    Returns:\n",
        "        True if the model output is 1.0, False otherwise.\n",
        "    \"\"\"\n",
        "    import requests\n",
        "    response = requests.post(\n",
        "        f\"{INFERENCE_SERVER_URL}/predict\",\n",
        "        json=[{\"age\": age, \"country\": country, \"traffic_source\": traffic_source, \"gender\": gender}],\n",
        "        headers={\"Content-Type\": \"application/json\"}\n",
        "    )\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "erh69tIxIbB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the function by passing in sample parameters."
      ],
      "metadata": {
        "id": "6Ey95ugmU_Fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_purchase(age=25.0, country=\"United States\", traffic_source=\"Search\", gender=\"M\")"
      ],
      "metadata": {
        "id": "M74zWCJTLDZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the ADK, define an agent below and provide the `predict_purchase` function as a tool."
      ],
      "metadata": {
        "id": "pqzoNsHpVI4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.adk.agents import Agent\n",
        "from vertexai import agent_engines\n",
        "\n",
        "agent = Agent(\n",
        "   model=\"gemini-2.5-flash\",\n",
        "   name='purchase_prediction_agent',\n",
        "   tools=[predict_purchase]\n",
        ")"
      ],
      "metadata": {
        "id": "x1ucFlhELBet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the agent locally by passing in a query."
      ],
      "metadata": {
        "id": "muUks3GCVWdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = agent_engines.AdkApp(agent=agent)\n",
        "async for event in app.async_stream_query(\n",
        "    user_id=\"123\",\n",
        "    message=\"Will a 25 yo male from the United States who came from Search make a purchase? Strictly output 'yes' or 'no'.\",\n",
        "):\n",
        "    try:\n",
        "        print(event['content']['parts'][0]['text'])\n",
        "    except:\n",
        "      continue"
      ],
      "metadata": {
        "id": "XPB5DVvoNugs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy the model to Agent Engine."
      ],
      "metadata": {
        "id": "YOVKFvVrVdz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remote_agent = client.agent_engines.create(\n",
        "    agent=app,\n",
        "    config={\n",
        "        \"requirements\": [\"google-cloud-aiplatform[agent_engines,adk]\"],\n",
        "        \"staging_bucket\": f\"gs://{BUCKET_NAME}\",\n",
        "        \"display_name\": \"purchase-predictor\",\n",
        "        \"description\": \"Agent that predicts whether or not a user will purchase a product.\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "DfvJC4niPkCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once done, view the deployed model in the [Cloud Console](https://console.cloud.google.com/vertex-ai/agents/agent-engines).\n",
        "\n",
        "Query the model again. This now point to the deployed agent instead of the local version."
      ],
      "metadata": {
        "id": "d8qlvnxdVjJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async for event in remote_agent.async_stream_query(\n",
        "    user_id=\"123\",\n",
        "    message=\"Will a 25 yo male from the United States who came from Search make a purchase? Strictly output 'yes' or 'no'.\",\n",
        "):\n",
        "    try:\n",
        "        print(event['content']['parts'][0]['text'])\n",
        "    except:\n",
        "      continue"
      ],
      "metadata": {
        "id": "WoXg1H8pRGu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 12: Clean up**"
      ],
      "metadata": {
        "id": "MUzqL7XAV7rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete the deployed agent."
      ],
      "metadata": {
        "id": "d5bw1UNqWHfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remote_agent.delete(force=True)"
      ],
      "metadata": {
        "id": "IemmGqqkWFzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete the inference server."
      ],
      "metadata": {
        "id": "bWGCEKqfWK1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "command = [\n",
        "    \"gcloud\",\n",
        "    \"run\",\n",
        "    \"services\",\n",
        "    \"delete\",\n",
        "    \"inference-server\",\n",
        "    \"--region\",\n",
        "    f\"{REGION}\",\n",
        "]\n",
        "\n",
        "subprocess.run(command, capture_output=True, text=True)"
      ],
      "metadata": {
        "id": "JzgkZp_1WL8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete the BigQuery dataset."
      ],
      "metadata": {
        "id": "AEt6SYmXWja1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigquery_client = bigquery.Client()\n",
        "\n",
        "bigquery_client.delete_dataset(\n",
        "    f\"{PROJECT_ID}.demo\", delete_contents=True, not_found_ok=True\n",
        ")"
      ],
      "metadata": {
        "id": "cLuSOAwPWwxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete the Storage bucket."
      ],
      "metadata": {
        "id": "Ns5uGPQuXBxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "storage_client = storage.Client()\n",
        "\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "bucket.delete()"
      ],
      "metadata": {
        "id": "o8MMPeMPXEUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
